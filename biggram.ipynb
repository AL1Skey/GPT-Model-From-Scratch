{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library and Requirement Listing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # Neural Network \n",
    "from torch.nn import functional as F # Torch Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirement Listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('requirements.txt','w') as file:\n",
    "    import sys\n",
    "    import matplotlib as mpl\n",
    "    import numpy as np\n",
    "    import pylzma\n",
    "    import ipykernel\n",
    "    import jupyter\n",
    "    import torch\n",
    "    \n",
    "    file.write(f\"\"\"\n",
    "            #python{sys.version}\n",
    "            # Using Virtual environment\n",
    "            matplotlib=={mpl.__version__}\n",
    "            numpy=={np.__version__}\n",
    "            pylzma=={pylzma.__version__}\n",
    "            ipykernel=={ipykernel.__version__}\n",
    "            jupyter\n",
    "            torch=={torch.__version__}\n",
    "            \"\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "block_size = 10 # Length of integer\n",
    "batch_size = 5 # How many those model run on pararel\n",
    "epochs = 2\n",
    "max_iters = 10000\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 250 # How many iteration those model to evaluate\n",
    "dropout = 0.2 # Create dropout metadata that drop neuron on deep learning pytorch so ther's no overfit\n",
    "n_embd = 384 # Create length of generated section each values\n",
    "n_layer = 4 # Create layer for gpt decoder because gpt-model need 4 decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# See the device\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "with open('wizard_of_ox.txt','r',encoding='utf-8') as f:\n",
    "    text =  f.read()\n",
    "# Get Unique Character from 2000 last index\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "# Get Vocab_Size\n",
    "vocab_size = len(chars)\n",
    "print(len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tokenizers by Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strtoint:{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '&': 4, \"'\": 5, '(': 6, ')': 7, '*': 8, ',': 9, '-': 10, '.': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '?': 24, 'A': 25, 'B': 26, 'C': 27, 'D': 28, 'E': 29, 'F': 30, 'G': 31, 'H': 32, 'I': 33, 'J': 34, 'K': 35, 'L': 36, 'M': 37, 'N': 38, 'O': 39, 'P': 40, 'Q': 41, 'R': 42, 'S': 43, 'T': 44, 'U': 45, 'V': 46, 'W': 47, 'X': 48, 'Y': 49, 'Z': 50, '[': 51, ']': 52, '_': 53, 'a': 54, 'b': 55, 'c': 56, 'd': 57, 'e': 58, 'f': 59, 'g': 60, 'h': 61, 'i': 62, 'j': 63, 'k': 64, 'l': 65, 'm': 66, 'n': 67, 'o': 68, 'p': 69, 'q': 70, 'r': 71, 's': 72, 't': 73, 'u': 74, 'v': 75, 'w': 76, 'x': 77, 'y': 78, 'z': 79, '\\ufeff': 80}\n",
      "inttostr:{0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '&', 5: \"'\", 6: '(', 7: ')', 8: '*', 9: ',', 10: '-', 11: '.', 12: '0', 13: '1', 14: '2', 15: '3', 16: '4', 17: '5', 18: '6', 19: '7', 20: '8', 21: '9', 22: ':', 23: ';', 24: '?', 25: 'A', 26: 'B', 27: 'C', 28: 'D', 29: 'E', 30: 'F', 31: 'G', 32: 'H', 33: 'I', 34: 'J', 35: 'K', 36: 'L', 37: 'M', 38: 'N', 39: 'O', 40: 'P', 41: 'Q', 42: 'R', 43: 'S', 44: 'T', 45: 'U', 46: 'V', 47: 'W', 48: 'X', 49: 'Y', 50: 'Z', 51: '[', 52: ']', 53: '_', 54: 'a', 55: 'b', 56: 'c', 57: 'd', 58: 'e', 59: 'f', 60: 'g', 61: 'h', 62: 'i', 63: 'j', 64: 'k', 65: 'l', 66: 'm', 67: 'n', 68: 'o', 69: 'p', 70: 'q', 71: 'r', 72: 's', 73: 't', 74: 'u', 75: 'v', 76: 'w', 77: 'x', 78: 'y', 79: 'z', 80: '\\ufeff'}\n"
     ]
    }
   ],
   "source": [
    "strtoint = {ch:i for i,ch in enumerate(chars)} # Encoding Purpose\n",
    "inttostr = {i:ch for i,ch in enumerate(chars)} # Decoding Purpose\n",
    "\n",
    "print(f\"strtoint:{strtoint}\")\n",
    "print(f\"inttostr:{inttostr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda x:[strtoint[c] for c in x]# Tranform data to string\n",
    "decode = lambda y: ''.join([inttostr[c] for c in y])# Convert array to string to decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = text[:200]\n",
    "len(encode(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([80,  1,  1, 28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,\n",
      "         1, 47, 33, 50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0,  1,  1, 26,\n",
      "        49,  0,  0,  1,  1, 36, 11,  1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,\n",
      "         0,  0,  1,  1, 25, 45, 44, 32, 39, 42,  1, 39, 30,  1, 44, 32, 29,  1,\n",
      "        47, 33, 50, 25, 42, 28,  1, 39, 30,  1, 39, 50,  9,  1, 44, 32, 29,  1,\n",
      "        36, 25, 38, 28,  1, 39, 30,  1, 39, 50])\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor(encode(text),dtype=torch.long)[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Validation Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert encoded text into tensor and Get 50% from the evaluation\n",
    "data = torch.tensor(encode(text),dtype=torch.long)[:int(0.5*len(text))]\n",
    "# Get 80% of data for training and 20% for test\n",
    "n = int(0.8*len(data))\n",
    "# Split into train and test\n",
    "train_data = data[:n]\n",
    "validation_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split to Feature and Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview:\n",
    "\n",
    "BigGram Model are predicting next words based on previous words, so the feature are previous words and target are next words after previous words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function\n",
    "def get_batch(split='validation'):\n",
    "    # Get Data\n",
    "    data = train_data if split.lower() == 'train' else validation_data\n",
    "    # Generate Index\n",
    "    rng_index = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # print(rng_index)\n",
    "    # Get Feature and Target\n",
    "    x = torch.stack([data[i:i+block_size] for i in rng_index])# Get data from iteration to iteration + block size\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in rng_index])# Get data from iteration to iteration + block size\n",
    "    # Use Device\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Train Validation feature and Target\n",
    "X_train, y_train = get_batch('train')\n",
    "X_val, y_val = get_batch('else')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([80]) target is 80\n",
      "when input is tensor([80,  1]) target is 1\n",
      "when input is tensor([80,  1,  1]) target is 1\n",
      "when input is tensor([80,  1,  1, 28]) target is 28\n",
      "when input is tensor([80,  1,  1, 28, 39]) target is 39\n",
      "when input is tensor([80,  1,  1, 28, 39, 42]) target is 42\n",
      "when input is tensor([80,  1,  1, 28, 39, 42, 39]) target is 39\n",
      "when input is tensor([80,  1,  1, 28, 39, 42, 39, 44]) target is 44\n",
      "when input is tensor([80,  1,  1, 28, 39, 42, 39, 44, 32]) target is 32\n",
      "when input is tensor([80,  1,  1, 28, 39, 42, 39, 44, 32, 49]) target is 49\n"
     ]
    }
   ],
   "source": [
    "for i in range(block_size):\n",
    "    context = data[:i+1]\n",
    "    target = data[i]\n",
    "    print(f'when input is {context} target is {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Big Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BigGram Model relies on probability and occurence of character each next steps. in nutshell:\n",
    "- Suppose 'hello world'\n",
    "- the steps is h-e-l-l-o-' '-w-o-r-l-d\n",
    "- the question is 'how much occurrence that h meet e, e meet l, l meet l etc.'\n",
    "- the chance will be calculated and updated every train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Estimator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()# Disable GradientDescent\n",
    "# def estimate_loss(eval_iters):\n",
    "#     out = {}\n",
    "#     # Set Model to Evaluation Mode\n",
    "#     model.eval()\n",
    "#     for split in ['train','val']:\n",
    "#         # Create Tensor with length of 'eval_iters'\n",
    "#         losses = torch.zeros(eval_iters)\n",
    "#         for k in range(eval_iters):\n",
    "#             # Create dataset\n",
    "#             X, y = get_batch(split)\n",
    "#             # Extract logits and loss\n",
    "#             logits, loss = model.foward(X, y)\n",
    "#             # Stored loss on losses tensor\n",
    "#             losses[k] = loss.item()\n",
    "#         # Calculate the mean from the losses and stored to output\n",
    "#         out[split] = losses.mean()\n",
    "#     # Set Model to Train Mode\n",
    "#     model.train()\n",
    "#     return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition(Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BigGramLanguageModel(nn.Module):# Create Neural Network with Gradient Descent for loss optimization\n",
    "#     def __init__(self, vocab_size):\n",
    "#         #Get Parent properties\n",
    "#         super().__init__()\n",
    "#         # Create Embbeded Layer \n",
    "#         '''\n",
    "#         Embedded Layer being used for translating the character, text, etc by using vector to see how near or far the feature to target. the vector in Embedding torch are already being created on torch, so this function will only referencing the vector from pre-built embedding table.\n",
    "        \n",
    "#         Embedding Table contain probability of character combination, like how 'bi' has 91039 occurence in million of data\n",
    "#         '''\n",
    "#         self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
    "    \n",
    "#     # Foward Function\n",
    "#     def foward(self,index,target=None):\n",
    "#         logits = self.token_embedding_table(index)\n",
    "        \n",
    "#         if target is None:\n",
    "#             loss = None\n",
    "#         else:\n",
    "#             batch_dim,time_dim,channel_dim = logits.shape\n",
    "#             # Reshape for cross entropy evaluation\n",
    "#             logits = logits.view(batch_dim*time_dim,channel_dim)\n",
    "#             target = target.view(batch_dim*time_dim)\n",
    "#             loss = F.cross_entropy(logits,target)\n",
    "        \n",
    "#         return logits, loss\n",
    "    \n",
    "#     # Generate Function\n",
    "#     def generate(self,index,max_new_tokens):\n",
    "#         # index is (batch_dim, time_dim) array of indices in current context\n",
    "#         for _ in range(max_new_tokens):# Loop without retreive any value from range\n",
    "#             # get prediction\n",
    "#             logits, loss = self.foward(index)\n",
    "#             # focus only on last time step\n",
    "#             logits = logits[:, -1, :] # becomes (batch_dim, channel_dim)\n",
    "#             # apply softmax to get probabilities\n",
    "#             probs = F.softmax(logits, dim=-1) # (batch_dim, channel_dim)\n",
    "#             #sample from distribution\n",
    "#             index_next = torch.multinomial(probs, num_samples=1) # (batch_dim, 1)\n",
    "#             # append sampled index to running sequence\n",
    "#             index = torch.cat((index, index_next), dim=1) #(batch_dim, time_dim + 1)\n",
    "#         return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition(Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigGramLanguageModel(vocab_size=vocab_size)\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.8122, -0.4277, -0.4958,  ...,  1.3422,  0.2597,  0.2347],\n",
      "        [-0.0547, -0.6150, -0.0898,  ..., -0.3147,  1.2274, -0.1767],\n",
      "        [ 0.9265,  1.3961,  1.0511,  ..., -1.1287, -0.2557, -0.6895],\n",
      "        ...,\n",
      "        [ 0.8764,  0.1055, -0.1903,  ...,  0.6797, -0.3065, -0.2258],\n",
      "        [-0.2725, -0.3512, -0.1652,  ...,  1.1548,  0.1308,  0.5053],\n",
      "        [ 0.0677,  1.5344,  1.5184,  ...,  1.9445,  1.0363, -0.2777]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i in model.parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: .parameters() are callback function that present if class inherit nn.Module. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ycSYq8I3*?SpL[DOqV5NfMCnlVzOet*3 cJRGbfej\n",
      "wK:1:d﻿Q'\n",
      "q10-W5R:fMMTb7OKv_EN\"9Whp9Tnx1Uh\"WN3﻿-*5IabX]b\n",
      "GTnH?2 ch﻿-C5P8-Vjk&u)-B1vq9i8t5;0o?2f1 NRdL\"YzEPX(vwJ)_VjKR51)P\n",
      "Jt\n",
      "JIl)H.T3:?-l:1ijzA,Nf!?2BX&-O:PuwKx,adY1M!P8EtPlvD8c[﻿HKN;VHJZXEbu2F,GO)0H,;\n",
      "w3D-s[CLm;x9 cJ XwomXCRT:4\n",
      ":dNJ8YRfMVgo)(d:wJRl(VpC,sYQ&EYT(&a9lcl7anStTYL\"R9IqM*AHXlDfV- c;!'dQv!RJGd4W5fM9k9,m.(S85iHnBw\" 9yjek49;lU'2 N5yiw)_&3pfs(*3'0QwtD-sS1mcV07jI32 i132fGQ)cJ,;_0Vj3'qNIll)C_N﻿n3p;Q5jq!ZBnSYTn\n",
      "7d:4d﻿zn5A! chanBGb9*rvP5fQyj]e3'X&,jkD\n"
     ]
    }
   ],
   "source": [
    "# context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "# generated_chars = decode(m.generate(context,max_new_tokens=500)[0].tolist())\n",
    "# print(generated_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:0\n",
      "step:0, loss train:4.9846, loss val:4.9599\n",
      "step:250, loss train:4.9073, loss val:4.9134\n",
      "step:500, loss train:4.8408, loss val:4.8157\n",
      "step:750, loss train:4.7691, loss val:4.7683\n",
      "step:1000, loss train:4.7202, loss val:4.6988\n",
      "step:1250, loss train:4.6365, loss val:4.6052\n",
      "step:1500, loss train:4.5632, loss val:4.5729\n",
      "step:1750, loss train:4.5033, loss val:4.4992\n",
      "step:2000, loss train:4.4544, loss val:4.4202\n",
      "step:2250, loss train:4.3790, loss val:4.3656\n",
      "step:2500, loss train:4.3370, loss val:4.3203\n",
      "step:2750, loss train:4.2794, loss val:4.2702\n",
      "step:3000, loss train:4.2097, loss val:4.1976\n",
      "step:3250, loss train:4.1528, loss val:4.1429\n",
      "step:3500, loss train:4.0873, loss val:4.0762\n",
      "step:3750, loss train:4.0419, loss val:4.0250\n",
      "step:4000, loss train:4.0054, loss val:3.9731\n",
      "step:4250, loss train:3.9444, loss val:3.9325\n",
      "step:4500, loss train:3.8876, loss val:3.8774\n",
      "step:4750, loss train:3.8458, loss val:3.8201\n",
      "step:5000, loss train:3.8009, loss val:3.7790\n",
      "step:5250, loss train:3.7563, loss val:3.7354\n",
      "step:5500, loss train:3.6988, loss val:3.6946\n",
      "step:5750, loss train:3.6709, loss val:3.6415\n",
      "step:6000, loss train:3.6245, loss val:3.6100\n",
      "step:6250, loss train:3.5676, loss val:3.5707\n",
      "step:6500, loss train:3.5450, loss val:3.5258\n",
      "step:6750, loss train:3.4998, loss val:3.4708\n",
      "step:7000, loss train:3.4693, loss val:3.4399\n",
      "step:7250, loss train:3.4172, loss val:3.3996\n",
      "step:7500, loss train:3.3840, loss val:3.3825\n",
      "step:7750, loss train:3.3642, loss val:3.3301\n",
      "step:8000, loss train:3.3176, loss val:3.3052\n",
      "step:8250, loss train:3.2850, loss val:3.2663\n",
      "step:8500, loss train:3.2591, loss val:3.2401\n",
      "step:8750, loss train:3.2130, loss val:3.2128\n",
      "step:9000, loss train:3.2011, loss val:3.1891\n",
      "step:9250, loss train:3.1614, loss val:3.1590\n",
      "step:9500, loss train:3.1328, loss val:3.1195\n",
      "step:9750, loss train:3.1130, loss val:3.1048\n",
      "batch:1\n",
      "step:0, loss train:3.0851, loss val:3.0867\n",
      "step:250, loss train:3.0663, loss val:3.0403\n",
      "step:500, loss train:3.0422, loss val:3.0286\n",
      "step:750, loss train:3.0234, loss val:3.0054\n",
      "step:1000, loss train:2.9977, loss val:2.9888\n",
      "step:1250, loss train:2.9811, loss val:2.9496\n",
      "step:1500, loss train:2.9358, loss val:2.9329\n",
      "step:1750, loss train:2.9392, loss val:2.9356\n",
      "step:2000, loss train:2.9221, loss val:2.9214\n",
      "step:2250, loss train:2.8900, loss val:2.8804\n",
      "step:2500, loss train:2.8886, loss val:2.8660\n",
      "step:2750, loss train:2.8672, loss val:2.8509\n",
      "step:3000, loss train:2.8600, loss val:2.8193\n",
      "step:3250, loss train:2.8264, loss val:2.8054\n",
      "step:3500, loss train:2.8156, loss val:2.8094\n",
      "step:3750, loss train:2.8022, loss val:2.7984\n",
      "step:4000, loss train:2.7976, loss val:2.7810\n",
      "step:4250, loss train:2.7689, loss val:2.7486\n",
      "step:4500, loss train:2.7730, loss val:2.7502\n",
      "step:4750, loss train:2.7495, loss val:2.7378\n",
      "step:5000, loss train:2.7451, loss val:2.7317\n",
      "step:5250, loss train:2.7506, loss val:2.7249\n",
      "step:5500, loss train:2.7202, loss val:2.7071\n",
      "step:5750, loss train:2.7285, loss val:2.7026\n",
      "step:6000, loss train:2.7013, loss val:2.6769\n",
      "step:6250, loss train:2.7024, loss val:2.6646\n",
      "step:6500, loss train:2.6789, loss val:2.6794\n",
      "step:6750, loss train:2.6839, loss val:2.6619\n",
      "step:7000, loss train:2.6612, loss val:2.6505\n",
      "step:7250, loss train:2.6479, loss val:2.6516\n",
      "step:7500, loss train:2.6520, loss val:2.6375\n",
      "step:7750, loss train:2.6317, loss val:2.6398\n",
      "step:8000, loss train:2.6426, loss val:2.6170\n",
      "step:8250, loss train:2.6171, loss val:2.6140\n",
      "step:8500, loss train:2.6333, loss val:2.5923\n",
      "step:8750, loss train:2.6159, loss val:2.6015\n",
      "step:9000, loss train:2.6030, loss val:2.6180\n",
      "step:9250, loss train:2.6042, loss val:2.5816\n",
      "step:9500, loss train:2.6037, loss val:2.5944\n",
      "step:9750, loss train:2.5933, loss val:2.5667\n",
      "batch:2\n",
      "step:0, loss train:2.5842, loss val:2.5556\n",
      "step:250, loss train:2.5957, loss val:2.5646\n",
      "step:500, loss train:2.5906, loss val:2.5759\n",
      "step:750, loss train:2.5779, loss val:2.5481\n",
      "step:1000, loss train:2.5704, loss val:2.5376\n",
      "step:1250, loss train:2.5563, loss val:2.5723\n",
      "step:1500, loss train:2.5479, loss val:2.5516\n",
      "step:1750, loss train:2.5598, loss val:2.5265\n",
      "step:2000, loss train:2.5355, loss val:2.5595\n",
      "step:2250, loss train:2.5515, loss val:2.5310\n",
      "step:2500, loss train:2.5370, loss val:2.5357\n",
      "step:2750, loss train:2.5470, loss val:2.5403\n",
      "step:3000, loss train:2.5444, loss val:2.5245\n",
      "step:3250, loss train:2.5319, loss val:2.5494\n",
      "step:3500, loss train:2.5292, loss val:2.5243\n",
      "step:3750, loss train:2.5329, loss val:2.5361\n",
      "step:4000, loss train:2.5211, loss val:2.5134\n",
      "step:4250, loss train:2.5102, loss val:2.5142\n",
      "step:4500, loss train:2.5173, loss val:2.5113\n",
      "step:4750, loss train:2.5116, loss val:2.5325\n",
      "step:5000, loss train:2.5305, loss val:2.5110\n",
      "step:5250, loss train:2.5216, loss val:2.5167\n",
      "step:5500, loss train:2.5005, loss val:2.4846\n",
      "step:5750, loss train:2.4957, loss val:2.5014\n",
      "step:6000, loss train:2.5185, loss val:2.5103\n",
      "step:6250, loss train:2.5070, loss val:2.5039\n",
      "step:6500, loss train:2.5028, loss val:2.4954\n",
      "step:6750, loss train:2.5196, loss val:2.5069\n",
      "step:7000, loss train:2.4819, loss val:2.4913\n",
      "step:7250, loss train:2.4869, loss val:2.4913\n",
      "step:7500, loss train:2.5112, loss val:2.4842\n",
      "step:7750, loss train:2.4851, loss val:2.5072\n",
      "step:8000, loss train:2.4716, loss val:2.4770\n",
      "step:8250, loss train:2.4893, loss val:2.4954\n",
      "step:8500, loss train:2.4871, loss val:2.5004\n",
      "step:8750, loss train:2.4782, loss val:2.4888\n",
      "step:9000, loss train:2.4730, loss val:2.4835\n",
      "step:9250, loss train:2.4957, loss val:2.4885\n",
      "step:9500, loss train:2.5050, loss val:2.5002\n",
      "step:9750, loss train:2.4953, loss val:2.4686\n",
      "batch:3\n",
      "step:0, loss train:2.4769, loss val:2.4835\n",
      "step:250, loss train:2.4737, loss val:2.4799\n",
      "step:500, loss train:2.4925, loss val:2.4721\n",
      "step:750, loss train:2.4775, loss val:2.4648\n",
      "step:1000, loss train:2.4771, loss val:2.4782\n",
      "step:1250, loss train:2.4686, loss val:2.4732\n",
      "step:1500, loss train:2.4624, loss val:2.4646\n",
      "step:1750, loss train:2.4716, loss val:2.4445\n",
      "step:2000, loss train:2.4775, loss val:2.4906\n",
      "step:2250, loss train:2.4666, loss val:2.4631\n",
      "step:2500, loss train:2.4707, loss val:2.4631\n",
      "step:2750, loss train:2.4687, loss val:2.4575\n",
      "step:3000, loss train:2.4792, loss val:2.4631\n",
      "step:3250, loss train:2.4752, loss val:2.4712\n",
      "step:3500, loss train:2.4850, loss val:2.4667\n",
      "step:3750, loss train:2.4623, loss val:2.4954\n",
      "step:4000, loss train:2.4484, loss val:2.4747\n",
      "step:4250, loss train:2.4638, loss val:2.4783\n",
      "step:4500, loss train:2.4532, loss val:2.4761\n",
      "step:4750, loss train:2.4492, loss val:2.4581\n",
      "step:5000, loss train:2.4523, loss val:2.4431\n",
      "step:5250, loss train:2.4379, loss val:2.4628\n",
      "step:5500, loss train:2.4429, loss val:2.4792\n",
      "step:5750, loss train:2.4516, loss val:2.4484\n",
      "step:6000, loss train:2.4611, loss val:2.4642\n",
      "step:6250, loss train:2.4739, loss val:2.4657\n",
      "step:6500, loss train:2.4580, loss val:2.4441\n",
      "step:6750, loss train:2.4783, loss val:2.4688\n",
      "step:7000, loss train:2.4548, loss val:2.4524\n",
      "step:7250, loss train:2.4551, loss val:2.4607\n",
      "step:7500, loss train:2.4581, loss val:2.4576\n",
      "step:7750, loss train:2.4550, loss val:2.4598\n",
      "step:8000, loss train:2.4578, loss val:2.4445\n",
      "step:8250, loss train:2.4586, loss val:2.4814\n",
      "step:8500, loss train:2.4622, loss val:2.4509\n",
      "step:8750, loss train:2.4609, loss val:2.4442\n",
      "step:9000, loss train:2.4574, loss val:2.4196\n",
      "step:9250, loss train:2.4352, loss val:2.4258\n",
      "step:9500, loss train:2.4559, loss val:2.4687\n",
      "step:9750, loss train:2.4744, loss val:2.4578\n",
      "batch:4\n",
      "step:0, loss train:2.4714, loss val:2.4699\n",
      "step:250, loss train:2.4515, loss val:2.4613\n",
      "step:500, loss train:2.4395, loss val:2.4414\n",
      "step:750, loss train:2.4419, loss val:2.4404\n",
      "step:1000, loss train:2.4546, loss val:2.4612\n",
      "step:1250, loss train:2.4609, loss val:2.4680\n",
      "step:1500, loss train:2.4501, loss val:2.4469\n",
      "step:1750, loss train:2.4543, loss val:2.4460\n",
      "step:2000, loss train:2.4456, loss val:2.4462\n",
      "step:2250, loss train:2.4621, loss val:2.4674\n",
      "step:2500, loss train:2.4480, loss val:2.4546\n",
      "step:2750, loss train:2.4529, loss val:2.4277\n",
      "step:3000, loss train:2.4528, loss val:2.4548\n",
      "step:3250, loss train:2.4309, loss val:2.4406\n",
      "step:3500, loss train:2.4529, loss val:2.4581\n",
      "step:3750, loss train:2.4622, loss val:2.4597\n",
      "step:4000, loss train:2.4388, loss val:2.4262\n",
      "step:4250, loss train:2.4173, loss val:2.4602\n",
      "step:4500, loss train:2.4460, loss val:2.4547\n",
      "step:4750, loss train:2.4513, loss val:2.4568\n",
      "step:5000, loss train:2.4352, loss val:2.4573\n",
      "step:5250, loss train:2.4543, loss val:2.4488\n",
      "step:5500, loss train:2.4560, loss val:2.4241\n",
      "step:5750, loss train:2.4243, loss val:2.4750\n",
      "step:6000, loss train:2.4264, loss val:2.4167\n",
      "step:6250, loss train:2.4150, loss val:2.4623\n",
      "step:6500, loss train:2.4426, loss val:2.4642\n",
      "step:6750, loss train:2.4223, loss val:2.4638\n",
      "step:7000, loss train:2.4440, loss val:2.4585\n",
      "step:7250, loss train:2.4337, loss val:2.4553\n",
      "step:7500, loss train:2.4133, loss val:2.4431\n",
      "step:7750, loss train:2.4282, loss val:2.4536\n",
      "step:8000, loss train:2.4309, loss val:2.4515\n",
      "step:8250, loss train:2.4349, loss val:2.4321\n",
      "step:8500, loss train:2.4391, loss val:2.4407\n",
      "step:8750, loss train:2.4340, loss val:2.4492\n",
      "step:9000, loss train:2.4447, loss val:2.4686\n",
      "step:9250, loss train:2.4505, loss val:2.4496\n",
      "step:9500, loss train:2.4498, loss val:2.4471\n",
      "step:9750, loss train:2.4274, loss val:2.4293\n",
      "batch:5\n",
      "step:0, loss train:2.4357, loss val:2.4383\n",
      "step:250, loss train:2.4252, loss val:2.4465\n",
      "step:500, loss train:2.4225, loss val:2.4391\n",
      "step:750, loss train:2.4249, loss val:2.4561\n",
      "step:1000, loss train:2.4459, loss val:2.4583\n",
      "step:1250, loss train:2.4336, loss val:2.4373\n",
      "step:1500, loss train:2.4358, loss val:2.4375\n",
      "step:1750, loss train:2.4337, loss val:2.4404\n",
      "step:2000, loss train:2.4398, loss val:2.4375\n",
      "step:2250, loss train:2.4407, loss val:2.4496\n",
      "step:2500, loss train:2.4377, loss val:2.4424\n",
      "step:2750, loss train:2.4492, loss val:2.4251\n",
      "step:3000, loss train:2.4238, loss val:2.4434\n",
      "step:3250, loss train:2.4285, loss val:2.4289\n",
      "step:3500, loss train:2.4360, loss val:2.4425\n",
      "step:3750, loss train:2.4293, loss val:2.4578\n",
      "step:4000, loss train:2.4141, loss val:2.4543\n",
      "step:4250, loss train:2.4301, loss val:2.4426\n",
      "step:4500, loss train:2.4396, loss val:2.4410\n",
      "step:4750, loss train:2.4324, loss val:2.4538\n",
      "step:5000, loss train:2.4291, loss val:2.4583\n",
      "step:5250, loss train:2.4296, loss val:2.4417\n",
      "step:5500, loss train:2.4339, loss val:2.4694\n",
      "step:5750, loss train:2.4299, loss val:2.4531\n",
      "step:6000, loss train:2.4323, loss val:2.4416\n",
      "step:6250, loss train:2.4417, loss val:2.4287\n",
      "step:6500, loss train:2.4241, loss val:2.4431\n",
      "step:6750, loss train:2.4188, loss val:2.4368\n",
      "step:7000, loss train:2.4305, loss val:2.4451\n",
      "step:7250, loss train:2.4212, loss val:2.4279\n",
      "step:7500, loss train:2.4226, loss val:2.4468\n",
      "step:7750, loss train:2.4479, loss val:2.4710\n",
      "step:8000, loss train:2.4200, loss val:2.4337\n",
      "step:8250, loss train:2.4396, loss val:2.4434\n",
      "step:8500, loss train:2.4178, loss val:2.4359\n",
      "step:8750, loss train:2.4249, loss val:2.4269\n",
      "step:9000, loss train:2.4472, loss val:2.4454\n",
      "step:9250, loss train:2.4168, loss val:2.4378\n",
      "step:9500, loss train:2.4215, loss val:2.4562\n",
      "step:9750, loss train:2.4137, loss val:2.4386\n",
      "batch:6\n",
      "step:0, loss train:2.4357, loss val:2.4293\n",
      "step:250, loss train:2.4265, loss val:2.4592\n",
      "step:500, loss train:2.4140, loss val:2.4470\n",
      "step:750, loss train:2.4429, loss val:2.4459\n",
      "step:1000, loss train:2.4155, loss val:2.4187\n",
      "step:1250, loss train:2.4440, loss val:2.4361\n",
      "step:1500, loss train:2.4226, loss val:2.4243\n",
      "step:1750, loss train:2.4231, loss val:2.4287\n",
      "step:2000, loss train:2.4096, loss val:2.4634\n",
      "step:2250, loss train:2.4165, loss val:2.4536\n",
      "step:2500, loss train:2.4296, loss val:2.4431\n",
      "step:2750, loss train:2.4234, loss val:2.4494\n",
      "step:3000, loss train:2.4083, loss val:2.4066\n",
      "step:3250, loss train:2.4224, loss val:2.4401\n",
      "step:3500, loss train:2.4193, loss val:2.4599\n",
      "step:3750, loss train:2.4031, loss val:2.4656\n",
      "step:4000, loss train:2.4281, loss val:2.4523\n",
      "step:4250, loss train:2.4292, loss val:2.4268\n",
      "step:4500, loss train:2.4254, loss val:2.4387\n",
      "step:4750, loss train:2.4406, loss val:2.4347\n",
      "step:5000, loss train:2.4218, loss val:2.4310\n",
      "step:5250, loss train:2.4214, loss val:2.4486\n",
      "step:5500, loss train:2.4331, loss val:2.4223\n",
      "step:5750, loss train:2.4251, loss val:2.4478\n",
      "step:6000, loss train:2.4098, loss val:2.4429\n",
      "step:6250, loss train:2.4219, loss val:2.4397\n",
      "step:6500, loss train:2.4026, loss val:2.4552\n",
      "step:6750, loss train:2.4293, loss val:2.4434\n",
      "step:7000, loss train:2.4190, loss val:2.4247\n",
      "step:7250, loss train:2.4311, loss val:2.4333\n",
      "step:7500, loss train:2.4218, loss val:2.4542\n",
      "step:7750, loss train:2.4231, loss val:2.4623\n",
      "step:8000, loss train:2.4104, loss val:2.4172\n",
      "step:8250, loss train:2.4360, loss val:2.4427\n",
      "step:8500, loss train:2.4117, loss val:2.4273\n",
      "step:8750, loss train:2.4293, loss val:2.4454\n",
      "step:9000, loss train:2.4186, loss val:2.4523\n",
      "step:9250, loss train:2.4145, loss val:2.4526\n",
      "step:9500, loss train:2.4278, loss val:2.4373\n",
      "step:9750, loss train:2.4170, loss val:2.4497\n",
      "batch:7\n",
      "step:0, loss train:2.4178, loss val:2.4516\n",
      "step:250, loss train:2.4081, loss val:2.4495\n",
      "step:500, loss train:2.4192, loss val:2.4473\n",
      "step:750, loss train:2.4087, loss val:2.4594\n",
      "step:1000, loss train:2.4244, loss val:2.4420\n",
      "step:1250, loss train:2.3997, loss val:2.4370\n",
      "step:1500, loss train:2.4285, loss val:2.4482\n",
      "step:1750, loss train:2.4222, loss val:2.4618\n",
      "step:2000, loss train:2.4262, loss val:2.4416\n",
      "step:2250, loss train:2.4244, loss val:2.4356\n",
      "step:2500, loss train:2.4149, loss val:2.4434\n",
      "step:2750, loss train:2.4273, loss val:2.4356\n",
      "step:3000, loss train:2.4228, loss val:2.4590\n",
      "step:3250, loss train:2.4228, loss val:2.4395\n",
      "step:3500, loss train:2.4181, loss val:2.4347\n",
      "step:3750, loss train:2.4443, loss val:2.4171\n",
      "step:4000, loss train:2.4394, loss val:2.4625\n",
      "step:4250, loss train:2.4392, loss val:2.4297\n",
      "step:4500, loss train:2.4159, loss val:2.4429\n",
      "step:4750, loss train:2.4211, loss val:2.4372\n",
      "step:5000, loss train:2.4232, loss val:2.4525\n",
      "step:5250, loss train:2.4123, loss val:2.4388\n",
      "step:5500, loss train:2.4249, loss val:2.4596\n",
      "step:5750, loss train:2.4285, loss val:2.4619\n",
      "step:6000, loss train:2.4354, loss val:2.4558\n",
      "step:6250, loss train:2.4213, loss val:2.4559\n",
      "step:6500, loss train:2.4366, loss val:2.4480\n",
      "step:6750, loss train:2.4387, loss val:2.4382\n",
      "step:7000, loss train:2.4323, loss val:2.4424\n",
      "step:7250, loss train:2.4183, loss val:2.4373\n",
      "step:7500, loss train:2.4311, loss val:2.4589\n",
      "step:7750, loss train:2.4439, loss val:2.4448\n",
      "step:8000, loss train:2.4114, loss val:2.4216\n",
      "step:8250, loss train:2.4273, loss val:2.4366\n",
      "step:8500, loss train:2.4035, loss val:2.4436\n",
      "step:8750, loss train:2.4304, loss val:2.4552\n",
      "step:9000, loss train:2.4370, loss val:2.4420\n",
      "step:9250, loss train:2.4059, loss val:2.4504\n",
      "step:9500, loss train:2.4139, loss val:2.4329\n",
      "step:9750, loss train:2.4023, loss val:2.4475\n",
      "batch:8\n",
      "step:0, loss train:2.4331, loss val:2.4296\n",
      "step:250, loss train:2.4040, loss val:2.4326\n",
      "step:500, loss train:2.4107, loss val:2.4558\n",
      "step:750, loss train:2.4052, loss val:2.4357\n",
      "step:1000, loss train:2.4135, loss val:2.4541\n",
      "step:1250, loss train:2.4139, loss val:2.4301\n",
      "step:1500, loss train:2.4184, loss val:2.4355\n",
      "step:1750, loss train:2.4271, loss val:2.4208\n",
      "step:2000, loss train:2.4069, loss val:2.4514\n",
      "step:2250, loss train:2.3946, loss val:2.4323\n",
      "step:2500, loss train:2.4368, loss val:2.4507\n",
      "step:2750, loss train:2.4062, loss val:2.4159\n",
      "step:3000, loss train:2.4226, loss val:2.4615\n",
      "step:3250, loss train:2.4257, loss val:2.4560\n",
      "step:3500, loss train:2.4254, loss val:2.4259\n",
      "step:3750, loss train:2.4226, loss val:2.4360\n",
      "step:4000, loss train:2.4158, loss val:2.4422\n",
      "step:4250, loss train:2.4281, loss val:2.4546\n",
      "step:4500, loss train:2.4383, loss val:2.4316\n",
      "step:4750, loss train:2.4345, loss val:2.4384\n",
      "step:5000, loss train:2.4330, loss val:2.4501\n",
      "step:5250, loss train:2.4261, loss val:2.4643\n",
      "step:5500, loss train:2.4094, loss val:2.4513\n",
      "step:5750, loss train:2.4063, loss val:2.4195\n",
      "step:6000, loss train:2.4070, loss val:2.4264\n",
      "step:6250, loss train:2.4162, loss val:2.4226\n",
      "step:6500, loss train:2.4016, loss val:2.4351\n",
      "step:6750, loss train:2.4114, loss val:2.4279\n",
      "step:7000, loss train:2.4388, loss val:2.4390\n",
      "step:7250, loss train:2.4062, loss val:2.4597\n",
      "step:7500, loss train:2.4095, loss val:2.4524\n",
      "step:7750, loss train:2.4113, loss val:2.4435\n",
      "step:8000, loss train:2.4236, loss val:2.4615\n",
      "step:8250, loss train:2.4149, loss val:2.4382\n",
      "step:8500, loss train:2.4066, loss val:2.4555\n",
      "step:8750, loss train:2.4103, loss val:2.4274\n",
      "step:9000, loss train:2.4023, loss val:2.4463\n",
      "step:9250, loss train:2.4284, loss val:2.4365\n",
      "step:9500, loss train:2.4337, loss val:2.4411\n",
      "step:9750, loss train:2.4222, loss val:2.4548\n",
      "batch:9\n",
      "step:0, loss train:2.4231, loss val:2.4382\n",
      "step:250, loss train:2.4310, loss val:2.4419\n",
      "step:500, loss train:2.4138, loss val:2.4635\n",
      "step:750, loss train:2.4295, loss val:2.4268\n",
      "step:1000, loss train:2.4137, loss val:2.4267\n",
      "step:1250, loss train:2.4145, loss val:2.4148\n",
      "step:1500, loss train:2.4320, loss val:2.4335\n",
      "step:1750, loss train:2.4076, loss val:2.4169\n",
      "step:2000, loss train:2.4023, loss val:2.4501\n",
      "step:2250, loss train:2.4207, loss val:2.4069\n",
      "step:2500, loss train:2.4117, loss val:2.4344\n",
      "step:2750, loss train:2.4005, loss val:2.4091\n",
      "step:3000, loss train:2.4159, loss val:2.4244\n",
      "step:3250, loss train:2.4062, loss val:2.4329\n",
      "step:3500, loss train:2.4077, loss val:2.4376\n",
      "step:3750, loss train:2.4330, loss val:2.4632\n",
      "step:4000, loss train:2.4075, loss val:2.4590\n",
      "step:4250, loss train:2.4251, loss val:2.4432\n",
      "step:4500, loss train:2.4247, loss val:2.4461\n",
      "step:4750, loss train:2.4139, loss val:2.4443\n",
      "step:5000, loss train:2.4458, loss val:2.4774\n",
      "step:5250, loss train:2.4219, loss val:2.4188\n",
      "step:5500, loss train:2.4213, loss val:2.4300\n",
      "step:5750, loss train:2.4373, loss val:2.4543\n",
      "step:6000, loss train:2.4104, loss val:2.4341\n",
      "step:6250, loss train:2.4169, loss val:2.4194\n",
      "step:6500, loss train:2.4083, loss val:2.4357\n",
      "step:6750, loss train:2.4092, loss val:2.4420\n",
      "step:7000, loss train:2.4090, loss val:2.4534\n",
      "step:7250, loss train:2.4255, loss val:2.4409\n",
      "step:7500, loss train:2.4223, loss val:2.4316\n",
      "step:7750, loss train:2.4154, loss val:2.4370\n",
      "step:8000, loss train:2.4229, loss val:2.4591\n",
      "step:8250, loss train:2.4253, loss val:2.4554\n",
      "step:8500, loss train:2.4003, loss val:2.4209\n",
      "step:8750, loss train:2.4379, loss val:2.4320\n",
      "step:9000, loss train:2.4240, loss val:2.4172\n",
      "step:9250, loss train:2.4124, loss val:2.4401\n",
      "step:9500, loss train:2.4426, loss val:2.4363\n",
      "step:9750, loss train:2.4208, loss val:2.4499\n",
      "batch:10\n",
      "step:0, loss train:2.4180, loss val:2.4364\n",
      "step:250, loss train:2.4270, loss val:2.4235\n",
      "step:500, loss train:2.4156, loss val:2.4374\n",
      "step:750, loss train:2.4000, loss val:2.4270\n",
      "step:1000, loss train:2.4239, loss val:2.4404\n",
      "step:1250, loss train:2.4178, loss val:2.4442\n",
      "step:1500, loss train:2.4253, loss val:2.4512\n",
      "step:1750, loss train:2.4037, loss val:2.4574\n",
      "step:2000, loss train:2.4221, loss val:2.4392\n",
      "step:2250, loss train:2.4305, loss val:2.4167\n",
      "step:2500, loss train:2.4166, loss val:2.4365\n",
      "step:2750, loss train:2.4232, loss val:2.4473\n",
      "step:3000, loss train:2.4207, loss val:2.4443\n",
      "step:3250, loss train:2.4153, loss val:2.4270\n",
      "step:3500, loss train:2.4110, loss val:2.4403\n",
      "step:3750, loss train:2.4160, loss val:2.4265\n",
      "step:4000, loss train:2.4180, loss val:2.4366\n",
      "step:4250, loss train:2.4161, loss val:2.4382\n",
      "step:4500, loss train:2.4122, loss val:2.4319\n",
      "step:4750, loss train:2.4359, loss val:2.4522\n",
      "step:5000, loss train:2.4048, loss val:2.4321\n",
      "step:5250, loss train:2.4294, loss val:2.4523\n",
      "step:5500, loss train:2.4247, loss val:2.4505\n",
      "step:5750, loss train:2.4238, loss val:2.4436\n",
      "step:6000, loss train:2.4365, loss val:2.4436\n",
      "step:6250, loss train:2.4182, loss val:2.4515\n",
      "step:6500, loss train:2.4034, loss val:2.4513\n",
      "step:6750, loss train:2.4136, loss val:2.4468\n",
      "step:7000, loss train:2.4229, loss val:2.4516\n",
      "step:7250, loss train:2.4409, loss val:2.4564\n",
      "step:7500, loss train:2.4096, loss val:2.4392\n",
      "step:7750, loss train:2.4213, loss val:2.4381\n",
      "step:8000, loss train:2.4303, loss val:2.4468\n",
      "step:8250, loss train:2.4134, loss val:2.4561\n",
      "step:8500, loss train:2.4167, loss val:2.4510\n",
      "step:8750, loss train:2.3999, loss val:2.4322\n",
      "step:9000, loss train:2.4122, loss val:2.4408\n",
      "step:9250, loss train:2.4107, loss val:2.4357\n",
      "step:9500, loss train:2.4111, loss val:2.4450\n",
      "step:9750, loss train:2.4234, loss val:2.4191\n",
      "batch:11\n",
      "step:0, loss train:2.3968, loss val:2.4405\n",
      "step:250, loss train:2.4228, loss val:2.4481\n",
      "step:500, loss train:2.4065, loss val:2.4170\n",
      "step:750, loss train:2.4188, loss val:2.4899\n",
      "step:1000, loss train:2.4329, loss val:2.4195\n",
      "step:1250, loss train:2.4114, loss val:2.4252\n",
      "step:1500, loss train:2.4180, loss val:2.4343\n",
      "step:1750, loss train:2.4247, loss val:2.4309\n",
      "step:2000, loss train:2.3990, loss val:2.4414\n",
      "step:2250, loss train:2.4066, loss val:2.4456\n",
      "step:2500, loss train:2.4338, loss val:2.4475\n",
      "step:2750, loss train:2.4105, loss val:2.4653\n",
      "step:3000, loss train:2.4135, loss val:2.4228\n",
      "step:3250, loss train:2.4235, loss val:2.4377\n",
      "step:3500, loss train:2.4256, loss val:2.4273\n",
      "step:3750, loss train:2.4007, loss val:2.4324\n",
      "step:4000, loss train:2.4170, loss val:2.4359\n",
      "step:4250, loss train:2.4128, loss val:2.4312\n",
      "step:4500, loss train:2.4188, loss val:2.4272\n",
      "step:4750, loss train:2.4176, loss val:2.4289\n",
      "step:5000, loss train:2.3887, loss val:2.4228\n",
      "step:5250, loss train:2.4139, loss val:2.4176\n",
      "step:5500, loss train:2.4152, loss val:2.4425\n",
      "step:5750, loss train:2.4156, loss val:2.4511\n",
      "step:6000, loss train:2.4139, loss val:2.4482\n",
      "step:6250, loss train:2.4159, loss val:2.4419\n",
      "step:6500, loss train:2.4039, loss val:2.4391\n",
      "step:6750, loss train:2.4286, loss val:2.4141\n",
      "step:7000, loss train:2.4144, loss val:2.4393\n",
      "step:7250, loss train:2.4182, loss val:2.4248\n",
      "step:7500, loss train:2.4031, loss val:2.4564\n",
      "step:7750, loss train:2.4070, loss val:2.4397\n",
      "step:8000, loss train:2.4185, loss val:2.4430\n",
      "step:8250, loss train:2.3970, loss val:2.4410\n",
      "step:8500, loss train:2.4262, loss val:2.4543\n",
      "step:8750, loss train:2.4028, loss val:2.4424\n",
      "step:9000, loss train:2.4129, loss val:2.4363\n",
      "step:9250, loss train:2.4044, loss val:2.4529\n",
      "step:9500, loss train:2.4210, loss val:2.4344\n",
      "step:9750, loss train:2.4177, loss val:2.4526\n",
      "batch:12\n",
      "step:0, loss train:2.4149, loss val:2.4507\n",
      "step:250, loss train:2.4200, loss val:2.4376\n",
      "step:500, loss train:2.4226, loss val:2.4508\n",
      "step:750, loss train:2.4127, loss val:2.4793\n",
      "step:1000, loss train:2.4145, loss val:2.4484\n",
      "step:1250, loss train:2.4169, loss val:2.4655\n",
      "step:1500, loss train:2.4153, loss val:2.4403\n",
      "step:1750, loss train:2.4158, loss val:2.4358\n",
      "step:2000, loss train:2.4153, loss val:2.4392\n",
      "step:2250, loss train:2.3990, loss val:2.4437\n",
      "step:2500, loss train:2.4093, loss val:2.4321\n",
      "step:2750, loss train:2.4123, loss val:2.4525\n",
      "step:3000, loss train:2.4126, loss val:2.4819\n",
      "step:3250, loss train:2.4197, loss val:2.4516\n",
      "step:3500, loss train:2.4257, loss val:2.4399\n",
      "step:3750, loss train:2.4103, loss val:2.4501\n",
      "step:4000, loss train:2.4160, loss val:2.4492\n",
      "step:4250, loss train:2.4333, loss val:2.4255\n",
      "step:4500, loss train:2.4097, loss val:2.4453\n",
      "step:4750, loss train:2.4221, loss val:2.4376\n",
      "step:5000, loss train:2.3960, loss val:2.4529\n",
      "step:5250, loss train:2.4218, loss val:2.4344\n",
      "step:5500, loss train:2.4235, loss val:2.4359\n",
      "step:5750, loss train:2.4312, loss val:2.4215\n",
      "step:6000, loss train:2.4201, loss val:2.4235\n",
      "step:6250, loss train:2.4096, loss val:2.4562\n",
      "step:6500, loss train:2.4190, loss val:2.4202\n",
      "step:6750, loss train:2.4147, loss val:2.4512\n",
      "step:7000, loss train:2.4095, loss val:2.4464\n",
      "step:7250, loss train:2.3945, loss val:2.4397\n",
      "step:7500, loss train:2.4064, loss val:2.4374\n",
      "step:7750, loss train:2.4061, loss val:2.4674\n",
      "step:8000, loss train:2.4117, loss val:2.4429\n",
      "step:8250, loss train:2.4247, loss val:2.4472\n",
      "step:8500, loss train:2.4085, loss val:2.4420\n",
      "step:8750, loss train:2.3934, loss val:2.4355\n",
      "step:9000, loss train:2.4072, loss val:2.4514\n",
      "step:9250, loss train:2.4305, loss val:2.4202\n",
      "step:9500, loss train:2.4173, loss val:2.4467\n",
      "step:9750, loss train:2.4124, loss val:2.4262\n",
      "batch:13\n",
      "step:0, loss train:2.4171, loss val:2.4121\n",
      "step:250, loss train:2.4156, loss val:2.4436\n",
      "step:500, loss train:2.4267, loss val:2.4411\n",
      "step:750, loss train:2.4132, loss val:2.4503\n",
      "step:1000, loss train:2.4083, loss val:2.4336\n",
      "step:1250, loss train:2.4194, loss val:2.4536\n",
      "step:1500, loss train:2.4085, loss val:2.4435\n",
      "step:1750, loss train:2.3956, loss val:2.4454\n",
      "step:2000, loss train:2.4106, loss val:2.4342\n",
      "step:2250, loss train:2.4090, loss val:2.4429\n",
      "step:2500, loss train:2.3922, loss val:2.4471\n",
      "step:2750, loss train:2.4210, loss val:2.4209\n",
      "step:3000, loss train:2.4245, loss val:2.4517\n",
      "step:3250, loss train:2.4041, loss val:2.4515\n",
      "step:3500, loss train:2.4172, loss val:2.4492\n",
      "step:3750, loss train:2.4091, loss val:2.4386\n",
      "step:4000, loss train:2.4069, loss val:2.4339\n",
      "step:4250, loss train:2.4163, loss val:2.4581\n",
      "step:4500, loss train:2.4232, loss val:2.4313\n",
      "step:4750, loss train:2.4136, loss val:2.4254\n",
      "step:5000, loss train:2.4172, loss val:2.4488\n",
      "step:5250, loss train:2.4340, loss val:2.4392\n",
      "step:5500, loss train:2.4215, loss val:2.4477\n",
      "step:5750, loss train:2.4112, loss val:2.4582\n",
      "step:6000, loss train:2.4223, loss val:2.4449\n",
      "step:6250, loss train:2.4196, loss val:2.4513\n",
      "step:6500, loss train:2.4178, loss val:2.4507\n",
      "step:6750, loss train:2.4043, loss val:2.4160\n",
      "step:7000, loss train:2.4171, loss val:2.4603\n",
      "step:7250, loss train:2.4125, loss val:2.4469\n",
      "step:7500, loss train:2.4185, loss val:2.4450\n",
      "step:7750, loss train:2.4265, loss val:2.4562\n",
      "step:8000, loss train:2.4120, loss val:2.4597\n",
      "step:8250, loss train:2.4231, loss val:2.4482\n",
      "step:8500, loss train:2.4348, loss val:2.4445\n",
      "step:8750, loss train:2.4264, loss val:2.4231\n",
      "step:9000, loss train:2.4234, loss val:2.4199\n",
      "step:9250, loss train:2.4077, loss val:2.4256\n",
      "step:9500, loss train:2.4172, loss val:2.4384\n",
      "step:9750, loss train:2.3995, loss val:2.4414\n",
      "batch:14\n",
      "step:0, loss train:2.4101, loss val:2.4415\n",
      "step:250, loss train:2.4141, loss val:2.4398\n",
      "step:500, loss train:2.4061, loss val:2.4684\n",
      "step:750, loss train:2.4130, loss val:2.4533\n",
      "step:1000, loss train:2.4073, loss val:2.4619\n",
      "step:1250, loss train:2.4147, loss val:2.4377\n",
      "step:1500, loss train:2.4069, loss val:2.4350\n",
      "step:1750, loss train:2.4104, loss val:2.4431\n",
      "step:2000, loss train:2.4223, loss val:2.4501\n",
      "step:2250, loss train:2.4362, loss val:2.4368\n",
      "step:2500, loss train:2.3939, loss val:2.4327\n",
      "step:2750, loss train:2.4323, loss val:2.4125\n",
      "step:3000, loss train:2.4048, loss val:2.4570\n",
      "step:3250, loss train:2.4040, loss val:2.4587\n",
      "step:3500, loss train:2.4187, loss val:2.4462\n",
      "step:3750, loss train:2.4145, loss val:2.4367\n",
      "step:4000, loss train:2.4108, loss val:2.4478\n",
      "step:4250, loss train:2.4154, loss val:2.4714\n",
      "step:4500, loss train:2.4160, loss val:2.4457\n",
      "step:4750, loss train:2.4144, loss val:2.4219\n",
      "step:5000, loss train:2.4059, loss val:2.4460\n",
      "step:5250, loss train:2.4326, loss val:2.4157\n",
      "step:5500, loss train:2.4070, loss val:2.4479\n",
      "step:5750, loss train:2.4208, loss val:2.4389\n",
      "step:6000, loss train:2.4055, loss val:2.4599\n",
      "step:6250, loss train:2.4265, loss val:2.4250\n",
      "step:6500, loss train:2.4282, loss val:2.4344\n",
      "step:6750, loss train:2.4205, loss val:2.4429\n",
      "step:7000, loss train:2.4047, loss val:2.4306\n",
      "step:7250, loss train:2.4255, loss val:2.4542\n",
      "step:7500, loss train:2.4220, loss val:2.4494\n",
      "step:7750, loss train:2.4162, loss val:2.4319\n",
      "step:8000, loss train:2.4091, loss val:2.4656\n",
      "step:8250, loss train:2.4006, loss val:2.4472\n",
      "step:8500, loss train:2.4157, loss val:2.4452\n",
      "step:8750, loss train:2.4112, loss val:2.4285\n",
      "step:9000, loss train:2.4202, loss val:2.4246\n",
      "step:9250, loss train:2.4039, loss val:2.4391\n",
      "step:9500, loss train:2.4196, loss val:2.4446\n",
      "step:9750, loss train:2.4242, loss val:2.4528\n",
      "batch:15\n",
      "step:0, loss train:2.4273, loss val:2.4647\n",
      "step:250, loss train:2.4127, loss val:2.4337\n",
      "step:500, loss train:2.4042, loss val:2.4525\n",
      "step:750, loss train:2.4247, loss val:2.4682\n",
      "step:1000, loss train:2.3872, loss val:2.4707\n",
      "step:1250, loss train:2.4254, loss val:2.4404\n",
      "step:1500, loss train:2.4086, loss val:2.4428\n",
      "step:1750, loss train:2.4098, loss val:2.4564\n",
      "step:2000, loss train:2.4044, loss val:2.4509\n",
      "step:2250, loss train:2.4137, loss val:2.4587\n",
      "step:2500, loss train:2.4156, loss val:2.4561\n",
      "step:2750, loss train:2.4212, loss val:2.4513\n",
      "step:3000, loss train:2.4018, loss val:2.4434\n",
      "step:3250, loss train:2.4243, loss val:2.4329\n",
      "step:3500, loss train:2.4037, loss val:2.4467\n",
      "step:3750, loss train:2.4115, loss val:2.4409\n",
      "step:4000, loss train:2.4171, loss val:2.4410\n",
      "step:4250, loss train:2.4159, loss val:2.4441\n",
      "step:4500, loss train:2.4106, loss val:2.4492\n",
      "step:4750, loss train:2.4086, loss val:2.4511\n",
      "step:5000, loss train:2.4131, loss val:2.4427\n",
      "step:5250, loss train:2.4298, loss val:2.4300\n",
      "step:5500, loss train:2.4263, loss val:2.4360\n",
      "step:5750, loss train:2.4051, loss val:2.4580\n",
      "step:6000, loss train:2.4122, loss val:2.4533\n",
      "step:6250, loss train:2.4164, loss val:2.4366\n",
      "step:6500, loss train:2.4109, loss val:2.4639\n",
      "step:6750, loss train:2.4039, loss val:2.4577\n",
      "step:7000, loss train:2.4126, loss val:2.4408\n",
      "step:7250, loss train:2.4086, loss val:2.4364\n",
      "step:7500, loss train:2.4044, loss val:2.4162\n",
      "step:7750, loss train:2.4135, loss val:2.4404\n",
      "step:8000, loss train:2.4246, loss val:2.4378\n",
      "step:8250, loss train:2.4173, loss val:2.4372\n",
      "step:8500, loss train:2.4093, loss val:2.4407\n",
      "step:8750, loss train:2.3891, loss val:2.4608\n",
      "step:9000, loss train:2.4095, loss val:2.4579\n",
      "step:9250, loss train:2.4382, loss val:2.4527\n",
      "step:9500, loss train:2.4148, loss val:2.4438\n",
      "step:9750, loss train:2.4217, loss val:2.4307\n",
      "batch:16\n",
      "step:0, loss train:2.4125, loss val:2.4790\n",
      "step:250, loss train:2.4171, loss val:2.4254\n",
      "step:500, loss train:2.4135, loss val:2.4325\n",
      "step:750, loss train:2.3995, loss val:2.4508\n",
      "step:1000, loss train:2.4090, loss val:2.4366\n",
      "step:1250, loss train:2.4147, loss val:2.4740\n",
      "step:1500, loss train:2.4107, loss val:2.4528\n",
      "step:1750, loss train:2.4191, loss val:2.4426\n",
      "step:2000, loss train:2.4041, loss val:2.4287\n",
      "step:2250, loss train:2.3968, loss val:2.4460\n",
      "step:2500, loss train:2.3919, loss val:2.4475\n",
      "step:2750, loss train:2.4304, loss val:2.4195\n",
      "step:3000, loss train:2.4217, loss val:2.4302\n",
      "step:3250, loss train:2.4056, loss val:2.4333\n",
      "step:3500, loss train:2.4119, loss val:2.4455\n",
      "step:3750, loss train:2.4082, loss val:2.4300\n",
      "step:4000, loss train:2.4108, loss val:2.4375\n",
      "step:4250, loss train:2.4091, loss val:2.4532\n",
      "step:4500, loss train:2.4146, loss val:2.4330\n",
      "step:4750, loss train:2.4104, loss val:2.4543\n",
      "step:5000, loss train:2.4137, loss val:2.4259\n",
      "step:5250, loss train:2.4368, loss val:2.4587\n",
      "step:5500, loss train:2.3976, loss val:2.4404\n",
      "step:5750, loss train:2.4172, loss val:2.4508\n",
      "step:6000, loss train:2.4043, loss val:2.4385\n",
      "step:6250, loss train:2.4144, loss val:2.4477\n",
      "step:6500, loss train:2.4055, loss val:2.4413\n",
      "step:6750, loss train:2.4335, loss val:2.4340\n",
      "step:7000, loss train:2.3995, loss val:2.4400\n",
      "step:7250, loss train:2.3939, loss val:2.4460\n",
      "step:7500, loss train:2.3838, loss val:2.4318\n",
      "step:7750, loss train:2.4210, loss val:2.4485\n",
      "step:8000, loss train:2.4045, loss val:2.4457\n",
      "step:8250, loss train:2.4138, loss val:2.4579\n",
      "step:8500, loss train:2.4184, loss val:2.4631\n",
      "step:8750, loss train:2.4175, loss val:2.4687\n",
      "step:9000, loss train:2.4098, loss val:2.4592\n",
      "step:9250, loss train:2.3963, loss val:2.4333\n",
      "step:9500, loss train:2.4236, loss val:2.4643\n",
      "step:9750, loss train:2.4109, loss val:2.4412\n",
      "batch:17\n",
      "step:0, loss train:2.4226, loss val:2.4444\n",
      "step:250, loss train:2.4065, loss val:2.4310\n",
      "step:500, loss train:2.4090, loss val:2.4570\n",
      "step:750, loss train:2.4016, loss val:2.4474\n",
      "step:1000, loss train:2.4101, loss val:2.4544\n",
      "step:1250, loss train:2.4179, loss val:2.4293\n",
      "step:1500, loss train:2.4071, loss val:2.4410\n",
      "step:1750, loss train:2.3957, loss val:2.4602\n",
      "step:2000, loss train:2.4163, loss val:2.4480\n",
      "step:2250, loss train:2.4053, loss val:2.4407\n",
      "step:2500, loss train:2.4075, loss val:2.4477\n",
      "step:2750, loss train:2.3997, loss val:2.4483\n",
      "step:3000, loss train:2.4102, loss val:2.4433\n",
      "step:3250, loss train:2.4178, loss val:2.4443\n",
      "step:3500, loss train:2.4264, loss val:2.4400\n",
      "step:3750, loss train:2.4363, loss val:2.4378\n",
      "step:4000, loss train:2.4008, loss val:2.4674\n",
      "step:4250, loss train:2.3980, loss val:2.4365\n",
      "step:4500, loss train:2.4266, loss val:2.4279\n",
      "step:4750, loss train:2.4107, loss val:2.4476\n",
      "step:5000, loss train:2.4247, loss val:2.4604\n",
      "step:5250, loss train:2.4110, loss val:2.4175\n",
      "step:5500, loss train:2.4138, loss val:2.4413\n",
      "step:5750, loss train:2.4134, loss val:2.4372\n",
      "step:6000, loss train:2.3936, loss val:2.4515\n",
      "step:6250, loss train:2.4214, loss val:2.4468\n",
      "step:6500, loss train:2.4233, loss val:2.4433\n",
      "step:6750, loss train:2.4229, loss val:2.4395\n",
      "step:7000, loss train:2.4156, loss val:2.4538\n",
      "step:7250, loss train:2.4072, loss val:2.4343\n",
      "step:7500, loss train:2.3994, loss val:2.4625\n",
      "step:7750, loss train:2.4011, loss val:2.4253\n",
      "step:8000, loss train:2.4143, loss val:2.4451\n",
      "step:8250, loss train:2.4084, loss val:2.4478\n",
      "step:8500, loss train:2.4237, loss val:2.4259\n",
      "step:8750, loss train:2.3990, loss val:2.4492\n",
      "step:9000, loss train:2.4172, loss val:2.4362\n",
      "step:9250, loss train:2.4070, loss val:2.4300\n",
      "step:9500, loss train:2.4163, loss val:2.4245\n",
      "step:9750, loss train:2.4139, loss val:2.4212\n",
      "batch:18\n",
      "step:0, loss train:2.4215, loss val:2.4421\n",
      "step:250, loss train:2.3976, loss val:2.4632\n",
      "step:500, loss train:2.3987, loss val:2.4432\n",
      "step:750, loss train:2.4003, loss val:2.4516\n",
      "step:1000, loss train:2.4101, loss val:2.4494\n",
      "step:1250, loss train:2.4108, loss val:2.4523\n",
      "step:1500, loss train:2.4016, loss val:2.4281\n",
      "step:1750, loss train:2.4213, loss val:2.4373\n",
      "step:2000, loss train:2.4176, loss val:2.4334\n",
      "step:2250, loss train:2.4129, loss val:2.4226\n",
      "step:2500, loss train:2.4295, loss val:2.4500\n",
      "step:2750, loss train:2.4303, loss val:2.4424\n",
      "step:3000, loss train:2.4167, loss val:2.4205\n",
      "step:3250, loss train:2.4059, loss val:2.4491\n",
      "step:3500, loss train:2.4085, loss val:2.4539\n",
      "step:3750, loss train:2.4467, loss val:2.4307\n",
      "step:4000, loss train:2.4105, loss val:2.4392\n",
      "step:4250, loss train:2.4212, loss val:2.4343\n",
      "step:4500, loss train:2.4074, loss val:2.4392\n",
      "step:4750, loss train:2.4238, loss val:2.4400\n",
      "step:5000, loss train:2.3940, loss val:2.4476\n",
      "step:5250, loss train:2.3942, loss val:2.4315\n",
      "step:5500, loss train:2.4146, loss val:2.4547\n",
      "step:5750, loss train:2.4122, loss val:2.4619\n",
      "step:6000, loss train:2.4186, loss val:2.4565\n",
      "step:6250, loss train:2.3960, loss val:2.4487\n",
      "step:6500, loss train:2.4231, loss val:2.4432\n",
      "step:6750, loss train:2.4136, loss val:2.4561\n",
      "step:7000, loss train:2.4237, loss val:2.4342\n",
      "step:7250, loss train:2.4176, loss val:2.4349\n",
      "step:7500, loss train:2.4190, loss val:2.4509\n",
      "step:7750, loss train:2.4146, loss val:2.4442\n",
      "step:8000, loss train:2.4234, loss val:2.4523\n",
      "step:8250, loss train:2.4335, loss val:2.4239\n",
      "step:8500, loss train:2.4060, loss val:2.4426\n",
      "step:8750, loss train:2.4141, loss val:2.4436\n",
      "step:9000, loss train:2.4209, loss val:2.4434\n",
      "step:9250, loss train:2.4084, loss val:2.4657\n",
      "step:9500, loss train:2.4016, loss val:2.4322\n",
      "step:9750, loss train:2.3998, loss val:2.4265\n",
      "batch:19\n",
      "step:0, loss train:2.4111, loss val:2.4401\n",
      "step:250, loss train:2.4073, loss val:2.4498\n",
      "step:500, loss train:2.3937, loss val:2.4579\n",
      "step:750, loss train:2.3958, loss val:2.4371\n",
      "step:1000, loss train:2.4254, loss val:2.4502\n",
      "step:1250, loss train:2.4316, loss val:2.4457\n",
      "step:1500, loss train:2.4186, loss val:2.4661\n",
      "step:1750, loss train:2.4134, loss val:2.4548\n",
      "step:2000, loss train:2.3990, loss val:2.4586\n",
      "step:2250, loss train:2.4116, loss val:2.4848\n",
      "step:2500, loss train:2.4126, loss val:2.4417\n",
      "step:2750, loss train:2.4112, loss val:2.4546\n",
      "step:3000, loss train:2.3939, loss val:2.4669\n",
      "step:3250, loss train:2.4022, loss val:2.4378\n",
      "step:3500, loss train:2.4087, loss val:2.4488\n",
      "step:3750, loss train:2.4294, loss val:2.4347\n",
      "step:4000, loss train:2.3979, loss val:2.4493\n",
      "step:4250, loss train:2.4294, loss val:2.4402\n",
      "step:4500, loss train:2.4109, loss val:2.4579\n",
      "step:4750, loss train:2.4137, loss val:2.4474\n",
      "step:5000, loss train:2.4112, loss val:2.4479\n",
      "step:5250, loss train:2.3960, loss val:2.4678\n",
      "step:5500, loss train:2.4178, loss val:2.4663\n",
      "step:5750, loss train:2.3986, loss val:2.4395\n",
      "step:6000, loss train:2.4144, loss val:2.4506\n",
      "step:6250, loss train:2.4128, loss val:2.4183\n",
      "step:6500, loss train:2.4108, loss val:2.4407\n",
      "step:6750, loss train:2.4301, loss val:2.4606\n",
      "step:7000, loss train:2.4302, loss val:2.4366\n",
      "step:7250, loss train:2.4047, loss val:2.4443\n",
      "step:7500, loss train:2.4074, loss val:2.4403\n",
      "step:7750, loss train:2.3856, loss val:2.4196\n",
      "step:8000, loss train:2.4167, loss val:2.4322\n",
      "step:8250, loss train:2.4081, loss val:2.4359\n",
      "step:8500, loss train:2.4224, loss val:2.4405\n",
      "step:8750, loss train:2.4006, loss val:2.4309\n",
      "step:9000, loss train:2.4287, loss val:2.4519\n",
      "step:9250, loss train:2.4191, loss val:2.4396\n",
      "step:9500, loss train:2.4229, loss val:2.4706\n",
      "step:9750, loss train:2.4237, loss val:2.4305\n",
      "batch:20\n",
      "step:0, loss train:2.4121, loss val:2.4331\n",
      "step:250, loss train:2.4168, loss val:2.4280\n",
      "step:500, loss train:2.4248, loss val:2.4442\n",
      "step:750, loss train:2.4217, loss val:2.4481\n",
      "step:1000, loss train:2.4294, loss val:2.4345\n",
      "step:1250, loss train:2.4259, loss val:2.4439\n",
      "step:1500, loss train:2.3899, loss val:2.4327\n",
      "step:1750, loss train:2.4216, loss val:2.4297\n",
      "step:2000, loss train:2.4234, loss val:2.4499\n",
      "step:2250, loss train:2.3975, loss val:2.4393\n",
      "step:2500, loss train:2.4173, loss val:2.4646\n",
      "step:2750, loss train:2.4220, loss val:2.4510\n",
      "step:3000, loss train:2.4110, loss val:2.4421\n",
      "step:3250, loss train:2.4297, loss val:2.4500\n",
      "step:3500, loss train:2.4157, loss val:2.4576\n",
      "step:3750, loss train:2.3964, loss val:2.4525\n",
      "step:4000, loss train:2.4252, loss val:2.4435\n",
      "step:4250, loss train:2.4243, loss val:2.4583\n",
      "step:4500, loss train:2.4205, loss val:2.4499\n",
      "step:4750, loss train:2.3991, loss val:2.4348\n",
      "step:5000, loss train:2.4449, loss val:2.4437\n",
      "step:5250, loss train:2.4232, loss val:2.4407\n",
      "step:5500, loss train:2.4124, loss val:2.4226\n",
      "step:5750, loss train:2.3989, loss val:2.4498\n",
      "step:6000, loss train:2.4073, loss val:2.4539\n",
      "step:6250, loss train:2.4160, loss val:2.4380\n",
      "step:6500, loss train:2.4256, loss val:2.4406\n",
      "step:6750, loss train:2.4267, loss val:2.4529\n",
      "step:7000, loss train:2.3880, loss val:2.4408\n",
      "step:7250, loss train:2.4330, loss val:2.4473\n",
      "step:7500, loss train:2.4072, loss val:2.4319\n",
      "step:7750, loss train:2.4206, loss val:2.4672\n",
      "step:8000, loss train:2.4145, loss val:2.4499\n",
      "step:8250, loss train:2.4197, loss val:2.4514\n",
      "step:8500, loss train:2.4103, loss val:2.4320\n",
      "step:8750, loss train:2.4193, loss val:2.4441\n",
      "step:9000, loss train:2.4222, loss val:2.4480\n",
      "step:9250, loss train:2.4061, loss val:2.4465\n",
      "step:9500, loss train:2.4180, loss val:2.4502\n",
      "step:9750, loss train:2.4178, loss val:2.4623\n",
      "batch:21\n",
      "step:0, loss train:2.4105, loss val:2.4634\n",
      "step:250, loss train:2.4057, loss val:2.4415\n",
      "step:500, loss train:2.4296, loss val:2.4396\n",
      "step:750, loss train:2.4011, loss val:2.4468\n",
      "step:1000, loss train:2.4075, loss val:2.4522\n",
      "step:1250, loss train:2.4235, loss val:2.4331\n",
      "step:1500, loss train:2.4316, loss val:2.4361\n",
      "step:1750, loss train:2.4005, loss val:2.4653\n",
      "step:2000, loss train:2.4087, loss val:2.4441\n",
      "step:2250, loss train:2.4260, loss val:2.4298\n",
      "step:2500, loss train:2.4190, loss val:2.4509\n",
      "step:2750, loss train:2.4352, loss val:2.4493\n",
      "step:3000, loss train:2.4142, loss val:2.4528\n",
      "step:3250, loss train:2.4085, loss val:2.4353\n",
      "step:3500, loss train:2.4258, loss val:2.4383\n",
      "step:3750, loss train:2.4050, loss val:2.4480\n",
      "step:4000, loss train:2.4031, loss val:2.4342\n",
      "step:4250, loss train:2.4052, loss val:2.4551\n",
      "step:4500, loss train:2.4065, loss val:2.4407\n",
      "step:4750, loss train:2.4215, loss val:2.4443\n",
      "step:5000, loss train:2.4199, loss val:2.4480\n",
      "step:5250, loss train:2.4133, loss val:2.4287\n",
      "step:5500, loss train:2.4135, loss val:2.4386\n",
      "step:5750, loss train:2.4022, loss val:2.4339\n",
      "step:6000, loss train:2.4267, loss val:2.4581\n",
      "step:6250, loss train:2.3919, loss val:2.4698\n",
      "step:6500, loss train:2.4148, loss val:2.4440\n",
      "step:6750, loss train:2.4045, loss val:2.4385\n",
      "step:7000, loss train:2.4165, loss val:2.4620\n",
      "step:7250, loss train:2.4272, loss val:2.4471\n",
      "step:7500, loss train:2.4052, loss val:2.4417\n",
      "step:7750, loss train:2.4112, loss val:2.4688\n",
      "step:8000, loss train:2.4116, loss val:2.4280\n",
      "step:8250, loss train:2.4187, loss val:2.4367\n",
      "step:8500, loss train:2.4073, loss val:2.4460\n",
      "step:8750, loss train:2.4136, loss val:2.4509\n",
      "step:9000, loss train:2.4152, loss val:2.4478\n",
      "step:9250, loss train:2.4032, loss val:2.4396\n",
      "step:9500, loss train:2.4187, loss val:2.4454\n",
      "step:9750, loss train:2.4102, loss val:2.4445\n",
      "batch:22\n",
      "step:0, loss train:2.3988, loss val:2.4618\n",
      "step:250, loss train:2.4013, loss val:2.4425\n",
      "step:500, loss train:2.4341, loss val:2.4275\n",
      "step:750, loss train:2.3957, loss val:2.4458\n",
      "step:1000, loss train:2.4118, loss val:2.4521\n",
      "step:1250, loss train:2.4184, loss val:2.4188\n",
      "step:1500, loss train:2.4233, loss val:2.4493\n",
      "step:1750, loss train:2.4090, loss val:2.4373\n",
      "step:2000, loss train:2.4204, loss val:2.4566\n",
      "step:2250, loss train:2.4141, loss val:2.4418\n",
      "step:2500, loss train:2.3972, loss val:2.4428\n",
      "step:2750, loss train:2.4247, loss val:2.4577\n",
      "step:3000, loss train:2.4162, loss val:2.4333\n",
      "step:3250, loss train:2.4173, loss val:2.4610\n",
      "step:3500, loss train:2.4179, loss val:2.4386\n",
      "step:3750, loss train:2.4187, loss val:2.4386\n",
      "step:4000, loss train:2.4184, loss val:2.4284\n",
      "step:4250, loss train:2.4193, loss val:2.4799\n",
      "step:4500, loss train:2.4148, loss val:2.4290\n",
      "step:4750, loss train:2.4164, loss val:2.4527\n",
      "step:5000, loss train:2.4124, loss val:2.4498\n",
      "step:5250, loss train:2.4206, loss val:2.4433\n",
      "step:5500, loss train:2.4129, loss val:2.4359\n",
      "step:5750, loss train:2.4188, loss val:2.4686\n",
      "step:6000, loss train:2.4162, loss val:2.4450\n",
      "step:6250, loss train:2.4260, loss val:2.4135\n",
      "step:6500, loss train:2.4067, loss val:2.4287\n",
      "step:6750, loss train:2.4253, loss val:2.4501\n",
      "step:7000, loss train:2.4208, loss val:2.4527\n",
      "step:7250, loss train:2.3994, loss val:2.4602\n",
      "step:7500, loss train:2.4239, loss val:2.4370\n",
      "step:7750, loss train:2.3989, loss val:2.4387\n",
      "step:8000, loss train:2.4115, loss val:2.4695\n",
      "step:8250, loss train:2.4095, loss val:2.4211\n",
      "step:8500, loss train:2.4247, loss val:2.4486\n",
      "step:8750, loss train:2.3950, loss val:2.4525\n",
      "step:9000, loss train:2.4080, loss val:2.4450\n",
      "step:9250, loss train:2.4288, loss val:2.4139\n",
      "step:9500, loss train:2.4094, loss val:2.4485\n",
      "step:9750, loss train:2.3922, loss val:2.4339\n",
      "batch:23\n",
      "step:0, loss train:2.3997, loss val:2.4386\n",
      "step:250, loss train:2.4088, loss val:2.4346\n",
      "step:500, loss train:2.4189, loss val:2.4528\n",
      "step:750, loss train:2.4036, loss val:2.4442\n",
      "step:1000, loss train:2.4217, loss val:2.4365\n",
      "step:1250, loss train:2.4194, loss val:2.4361\n",
      "step:1500, loss train:2.4206, loss val:2.4654\n",
      "step:1750, loss train:2.4021, loss val:2.4603\n",
      "step:2000, loss train:2.4215, loss val:2.4617\n",
      "step:2250, loss train:2.4239, loss val:2.4168\n",
      "step:2500, loss train:2.4082, loss val:2.4413\n",
      "step:2750, loss train:2.4321, loss val:2.4147\n",
      "step:3000, loss train:2.4151, loss val:2.4466\n",
      "step:3250, loss train:2.4029, loss val:2.4512\n",
      "step:3500, loss train:2.4193, loss val:2.4354\n",
      "step:3750, loss train:2.4179, loss val:2.4737\n",
      "step:4000, loss train:2.4229, loss val:2.4307\n",
      "step:4250, loss train:2.3974, loss val:2.4515\n",
      "step:4500, loss train:2.4122, loss val:2.4220\n",
      "step:4750, loss train:2.4060, loss val:2.4535\n",
      "step:5000, loss train:2.3995, loss val:2.4343\n",
      "step:5250, loss train:2.4116, loss val:2.4307\n",
      "step:5500, loss train:2.4147, loss val:2.4411\n",
      "step:5750, loss train:2.4219, loss val:2.4537\n",
      "step:6000, loss train:2.4107, loss val:2.4231\n",
      "step:6250, loss train:2.4272, loss val:2.4440\n",
      "step:6500, loss train:2.4077, loss val:2.4479\n",
      "step:6750, loss train:2.4104, loss val:2.4429\n",
      "step:7000, loss train:2.4009, loss val:2.4392\n",
      "step:7250, loss train:2.4205, loss val:2.4479\n",
      "step:7500, loss train:2.4212, loss val:2.4689\n",
      "step:7750, loss train:2.4292, loss val:2.4376\n",
      "step:8000, loss train:2.4006, loss val:2.4487\n",
      "step:8250, loss train:2.4242, loss val:2.4424\n",
      "step:8500, loss train:2.4324, loss val:2.4574\n",
      "step:8750, loss train:2.4127, loss val:2.4645\n",
      "step:9000, loss train:2.4086, loss val:2.4595\n",
      "step:9250, loss train:2.4178, loss val:2.4320\n",
      "step:9500, loss train:2.4107, loss val:2.4235\n",
      "step:9750, loss train:2.4009, loss val:2.4518\n",
      "batch:24\n",
      "step:0, loss train:2.4271, loss val:2.4351\n",
      "step:250, loss train:2.4127, loss val:2.4561\n",
      "step:500, loss train:2.4399, loss val:2.4260\n",
      "step:750, loss train:2.4174, loss val:2.4473\n",
      "step:1000, loss train:2.4106, loss val:2.4483\n",
      "step:1250, loss train:2.3948, loss val:2.4451\n",
      "step:1500, loss train:2.4267, loss val:2.4496\n",
      "step:1750, loss train:2.4177, loss val:2.4564\n",
      "step:2000, loss train:2.4288, loss val:2.4390\n",
      "step:2250, loss train:2.4139, loss val:2.4382\n",
      "step:2500, loss train:2.4169, loss val:2.4417\n",
      "step:2750, loss train:2.4185, loss val:2.4605\n",
      "step:3000, loss train:2.3869, loss val:2.4421\n",
      "step:3250, loss train:2.3930, loss val:2.4551\n",
      "step:3500, loss train:2.4043, loss val:2.4456\n",
      "step:3750, loss train:2.4224, loss val:2.4303\n",
      "step:4000, loss train:2.4131, loss val:2.4772\n",
      "step:4250, loss train:2.4191, loss val:2.4612\n",
      "step:4500, loss train:2.4204, loss val:2.4518\n",
      "step:4750, loss train:2.4107, loss val:2.4728\n",
      "step:5000, loss train:2.4073, loss val:2.4476\n",
      "step:5250, loss train:2.4103, loss val:2.4599\n",
      "step:5500, loss train:2.4107, loss val:2.4348\n",
      "step:5750, loss train:2.4119, loss val:2.4286\n",
      "step:6000, loss train:2.4102, loss val:2.4328\n",
      "step:6250, loss train:2.4153, loss val:2.4181\n",
      "step:6500, loss train:2.4110, loss val:2.4723\n",
      "step:6750, loss train:2.4206, loss val:2.4410\n",
      "step:7000, loss train:2.4133, loss val:2.4429\n",
      "step:7250, loss train:2.4112, loss val:2.4284\n",
      "step:7500, loss train:2.4182, loss val:2.4558\n",
      "step:7750, loss train:2.4097, loss val:2.4381\n",
      "step:8000, loss train:2.4003, loss val:2.4143\n",
      "step:8250, loss train:2.4224, loss val:2.4559\n",
      "step:8500, loss train:2.4366, loss val:2.4388\n",
      "step:8750, loss train:2.4178, loss val:2.4522\n",
      "step:9000, loss train:2.4108, loss val:2.4539\n",
      "step:9250, loss train:2.4274, loss val:2.4363\n",
      "step:9500, loss train:2.4059, loss val:2.4611\n",
      "step:9750, loss train:2.4024, loss val:2.4606\n",
      "batch:25\n",
      "step:0, loss train:2.4058, loss val:2.4528\n",
      "step:250, loss train:2.3969, loss val:2.4459\n",
      "step:500, loss train:2.4160, loss val:2.4411\n",
      "step:750, loss train:2.4130, loss val:2.4523\n",
      "step:1000, loss train:2.4171, loss val:2.4320\n",
      "step:1250, loss train:2.4098, loss val:2.4524\n",
      "step:1500, loss train:2.4147, loss val:2.4519\n",
      "step:1750, loss train:2.4029, loss val:2.4422\n",
      "step:2000, loss train:2.4161, loss val:2.4547\n",
      "step:2250, loss train:2.4126, loss val:2.4798\n",
      "step:2500, loss train:2.4209, loss val:2.4426\n",
      "step:2750, loss train:2.3920, loss val:2.4232\n",
      "step:3000, loss train:2.4200, loss val:2.4124\n",
      "step:3250, loss train:2.4095, loss val:2.4319\n",
      "step:3500, loss train:2.4186, loss val:2.4433\n",
      "step:3750, loss train:2.3997, loss val:2.4617\n",
      "step:4000, loss train:2.4167, loss val:2.4428\n",
      "step:4250, loss train:2.4083, loss val:2.4503\n",
      "step:4500, loss train:2.4103, loss val:2.4311\n",
      "step:4750, loss train:2.4143, loss val:2.4559\n",
      "step:5000, loss train:2.4206, loss val:2.4390\n",
      "step:5250, loss train:2.4120, loss val:2.4367\n",
      "step:5500, loss train:2.4189, loss val:2.4423\n",
      "step:5750, loss train:2.4067, loss val:2.4566\n",
      "step:6000, loss train:2.4051, loss val:2.4634\n",
      "step:6250, loss train:2.4134, loss val:2.4451\n",
      "step:6500, loss train:2.4180, loss val:2.4353\n",
      "step:6750, loss train:2.4111, loss val:2.4503\n",
      "step:7000, loss train:2.4055, loss val:2.4572\n",
      "step:7250, loss train:2.4143, loss val:2.4382\n",
      "step:7500, loss train:2.4173, loss val:2.4219\n",
      "step:7750, loss train:2.4272, loss val:2.4207\n",
      "step:8000, loss train:2.4141, loss val:2.4176\n",
      "step:8250, loss train:2.4264, loss val:2.4326\n",
      "step:8500, loss train:2.4077, loss val:2.4648\n",
      "step:8750, loss train:2.4281, loss val:2.4464\n",
      "step:9000, loss train:2.4054, loss val:2.4539\n",
      "step:9250, loss train:2.4115, loss val:2.4577\n",
      "step:9500, loss train:2.4074, loss val:2.4424\n",
      "step:9750, loss train:2.4065, loss val:2.4282\n",
      "batch:26\n",
      "step:0, loss train:2.4066, loss val:2.4526\n",
      "step:250, loss train:2.4256, loss val:2.4372\n",
      "step:500, loss train:2.4192, loss val:2.4404\n",
      "step:750, loss train:2.4078, loss val:2.4403\n",
      "step:1000, loss train:2.4120, loss val:2.4351\n",
      "step:1250, loss train:2.4106, loss val:2.4337\n",
      "step:1500, loss train:2.4134, loss val:2.4506\n",
      "step:1750, loss train:2.3927, loss val:2.4522\n",
      "step:2000, loss train:2.4014, loss val:2.4640\n",
      "step:2250, loss train:2.4075, loss val:2.4433\n",
      "step:2500, loss train:2.3943, loss val:2.4446\n",
      "step:2750, loss train:2.4094, loss val:2.4499\n",
      "step:3000, loss train:2.4105, loss val:2.4227\n",
      "step:3250, loss train:2.4213, loss val:2.4525\n",
      "step:3500, loss train:2.4194, loss val:2.4301\n",
      "step:3750, loss train:2.4269, loss val:2.4288\n",
      "step:4000, loss train:2.4162, loss val:2.4731\n",
      "step:4250, loss train:2.4043, loss val:2.4378\n",
      "step:4500, loss train:2.4085, loss val:2.4609\n",
      "step:4750, loss train:2.4197, loss val:2.4316\n",
      "step:5000, loss train:2.4047, loss val:2.4605\n",
      "step:5250, loss train:2.4310, loss val:2.4526\n",
      "step:5500, loss train:2.4056, loss val:2.4476\n",
      "step:5750, loss train:2.4320, loss val:2.4273\n",
      "step:6000, loss train:2.4183, loss val:2.4469\n",
      "step:6250, loss train:2.4156, loss val:2.4191\n",
      "step:6500, loss train:2.4354, loss val:2.4564\n",
      "step:6750, loss train:2.4095, loss val:2.4297\n",
      "step:7000, loss train:2.3987, loss val:2.4547\n",
      "step:7250, loss train:2.4012, loss val:2.4542\n",
      "step:7500, loss train:2.4127, loss val:2.4488\n",
      "step:7750, loss train:2.4073, loss val:2.4330\n",
      "step:8000, loss train:2.3966, loss val:2.4497\n",
      "step:8250, loss train:2.4275, loss val:2.4301\n",
      "step:8500, loss train:2.4089, loss val:2.4344\n",
      "step:8750, loss train:2.4161, loss val:2.4408\n",
      "step:9000, loss train:2.4018, loss val:2.4335\n",
      "step:9250, loss train:2.4197, loss val:2.4342\n",
      "step:9500, loss train:2.4025, loss val:2.4508\n",
      "step:9750, loss train:2.4185, loss val:2.4321\n",
      "batch:27\n",
      "step:0, loss train:2.3955, loss val:2.4378\n",
      "step:250, loss train:2.4063, loss val:2.4356\n",
      "step:500, loss train:2.4218, loss val:2.4541\n",
      "step:750, loss train:2.4269, loss val:2.4401\n",
      "step:1000, loss train:2.4091, loss val:2.4450\n",
      "step:1250, loss train:2.4204, loss val:2.4351\n",
      "step:1500, loss train:2.4258, loss val:2.4661\n",
      "step:1750, loss train:2.4214, loss val:2.4705\n",
      "step:2000, loss train:2.4290, loss val:2.4508\n",
      "step:2250, loss train:2.4049, loss val:2.4592\n",
      "step:2500, loss train:2.4103, loss val:2.4538\n",
      "step:2750, loss train:2.4256, loss val:2.4292\n",
      "step:3000, loss train:2.4291, loss val:2.4499\n",
      "step:3250, loss train:2.4254, loss val:2.4537\n",
      "step:3500, loss train:2.3883, loss val:2.4553\n",
      "step:3750, loss train:2.4177, loss val:2.4241\n",
      "step:4000, loss train:2.4255, loss val:2.4416\n",
      "step:4250, loss train:2.4203, loss val:2.4154\n",
      "step:4500, loss train:2.3976, loss val:2.4621\n",
      "step:4750, loss train:2.4343, loss val:2.4422\n",
      "step:5000, loss train:2.4192, loss val:2.4651\n",
      "step:5250, loss train:2.4235, loss val:2.4131\n",
      "step:5500, loss train:2.4118, loss val:2.4505\n",
      "step:5750, loss train:2.4226, loss val:2.4428\n",
      "step:6000, loss train:2.4133, loss val:2.4379\n",
      "step:6250, loss train:2.4018, loss val:2.4331\n",
      "step:6500, loss train:2.4113, loss val:2.4667\n",
      "step:6750, loss train:2.4094, loss val:2.4561\n",
      "step:7000, loss train:2.4138, loss val:2.4424\n",
      "step:7250, loss train:2.3989, loss val:2.4560\n",
      "step:7500, loss train:2.4328, loss val:2.4361\n",
      "step:7750, loss train:2.4082, loss val:2.4470\n",
      "step:8000, loss train:2.4225, loss val:2.4307\n",
      "step:8250, loss train:2.4066, loss val:2.4505\n",
      "step:8500, loss train:2.4066, loss val:2.4498\n",
      "step:8750, loss train:2.4214, loss val:2.4355\n",
      "step:9000, loss train:2.4141, loss val:2.4527\n",
      "step:9250, loss train:2.4108, loss val:2.4500\n",
      "step:9500, loss train:2.4119, loss val:2.4550\n",
      "step:9750, loss train:2.4244, loss val:2.4516\n",
      "batch:28\n",
      "step:0, loss train:2.4082, loss val:2.4569\n",
      "step:250, loss train:2.4136, loss val:2.4500\n",
      "step:500, loss train:2.4343, loss val:2.4381\n",
      "step:750, loss train:2.3809, loss val:2.4448\n",
      "step:1000, loss train:2.4220, loss val:2.4450\n",
      "step:1250, loss train:2.4003, loss val:2.4477\n",
      "step:1500, loss train:2.3898, loss val:2.4396\n",
      "step:1750, loss train:2.4154, loss val:2.4471\n",
      "step:2000, loss train:2.4237, loss val:2.4520\n",
      "step:2250, loss train:2.4104, loss val:2.4345\n",
      "step:2500, loss train:2.4105, loss val:2.4434\n",
      "step:2750, loss train:2.3957, loss val:2.4631\n",
      "step:3000, loss train:2.4240, loss val:2.4385\n",
      "step:3250, loss train:2.4217, loss val:2.4610\n",
      "step:3500, loss train:2.4126, loss val:2.4473\n",
      "step:3750, loss train:2.4187, loss val:2.4333\n",
      "step:4000, loss train:2.3988, loss val:2.4287\n",
      "step:4250, loss train:2.4118, loss val:2.4434\n",
      "step:4500, loss train:2.4096, loss val:2.4428\n",
      "step:4750, loss train:2.4192, loss val:2.4346\n",
      "step:5000, loss train:2.4098, loss val:2.4313\n",
      "step:5250, loss train:2.3955, loss val:2.4357\n",
      "step:5500, loss train:2.4009, loss val:2.4517\n",
      "step:5750, loss train:2.4020, loss val:2.4444\n",
      "step:6000, loss train:2.4014, loss val:2.4431\n",
      "step:6250, loss train:2.4209, loss val:2.4343\n",
      "step:6500, loss train:2.4143, loss val:2.4529\n",
      "step:6750, loss train:2.4073, loss val:2.4582\n",
      "step:7000, loss train:2.4145, loss val:2.4430\n",
      "step:7250, loss train:2.4214, loss val:2.4536\n",
      "step:7500, loss train:2.4274, loss val:2.4555\n",
      "step:7750, loss train:2.4247, loss val:2.4548\n",
      "step:8000, loss train:2.4176, loss val:2.4317\n",
      "step:8250, loss train:2.4074, loss val:2.4521\n",
      "step:8500, loss train:2.4031, loss val:2.4639\n",
      "step:8750, loss train:2.4180, loss val:2.4248\n",
      "step:9000, loss train:2.4118, loss val:2.4431\n",
      "step:9250, loss train:2.4341, loss val:2.4333\n",
      "step:9500, loss train:2.4166, loss val:2.4311\n",
      "step:9750, loss train:2.4094, loss val:2.4587\n",
      "batch:29\n",
      "step:0, loss train:2.4168, loss val:2.4518\n",
      "step:250, loss train:2.4205, loss val:2.4660\n",
      "step:500, loss train:2.4028, loss val:2.4654\n",
      "step:750, loss train:2.4107, loss val:2.4624\n",
      "step:1000, loss train:2.4258, loss val:2.4745\n",
      "step:1250, loss train:2.4334, loss val:2.4428\n",
      "step:1500, loss train:2.4039, loss val:2.3959\n",
      "step:1750, loss train:2.4242, loss val:2.4291\n",
      "step:2000, loss train:2.4061, loss val:2.4534\n",
      "step:2250, loss train:2.4098, loss val:2.4332\n",
      "step:2500, loss train:2.4079, loss val:2.4396\n",
      "step:2750, loss train:2.4132, loss val:2.4416\n",
      "step:3000, loss train:2.4268, loss val:2.4414\n",
      "step:3250, loss train:2.4067, loss val:2.4658\n",
      "step:3500, loss train:2.4048, loss val:2.4406\n",
      "step:3750, loss train:2.4007, loss val:2.4397\n",
      "step:4000, loss train:2.4320, loss val:2.4460\n",
      "step:4250, loss train:2.4030, loss val:2.4304\n",
      "step:4500, loss train:2.4019, loss val:2.4388\n",
      "step:4750, loss train:2.4083, loss val:2.4292\n",
      "step:5000, loss train:2.4112, loss val:2.4492\n",
      "step:5250, loss train:2.4183, loss val:2.4508\n",
      "step:5500, loss train:2.4230, loss val:2.4619\n",
      "step:5750, loss train:2.4078, loss val:2.4418\n",
      "step:6000, loss train:2.3858, loss val:2.4396\n",
      "step:6250, loss train:2.4182, loss val:2.4521\n",
      "step:6500, loss train:2.4014, loss val:2.4457\n",
      "step:6750, loss train:2.3987, loss val:2.4359\n",
      "step:7000, loss train:2.4203, loss val:2.4821\n",
      "step:7250, loss train:2.4069, loss val:2.4471\n",
      "step:7500, loss train:2.4201, loss val:2.4552\n",
      "step:7750, loss train:2.4148, loss val:2.4469\n",
      "step:8000, loss train:2.4126, loss val:2.4601\n",
      "step:8250, loss train:2.3991, loss val:2.4620\n",
      "step:8500, loss train:2.4252, loss val:2.4281\n",
      "step:8750, loss train:2.4163, loss val:2.4613\n",
      "step:9000, loss train:2.4129, loss val:2.4520\n",
      "step:9250, loss train:2.4243, loss val:2.4542\n",
      "step:9500, loss train:2.4026, loss val:2.4536\n",
      "step:9750, loss train:2.3895, loss val:2.4429\n",
      "batch:30\n",
      "step:0, loss train:2.3997, loss val:2.4127\n",
      "step:250, loss train:2.4193, loss val:2.4447\n",
      "step:500, loss train:2.4359, loss val:2.4306\n",
      "step:750, loss train:2.3995, loss val:2.4504\n",
      "step:1000, loss train:2.4214, loss val:2.4597\n",
      "step:1250, loss train:2.3978, loss val:2.4246\n",
      "step:1500, loss train:2.4255, loss val:2.4400\n",
      "step:1750, loss train:2.4011, loss val:2.4351\n",
      "step:2000, loss train:2.4153, loss val:2.4577\n",
      "step:2250, loss train:2.4213, loss val:2.4760\n",
      "step:2500, loss train:2.4214, loss val:2.4273\n",
      "step:2750, loss train:2.4138, loss val:2.4445\n",
      "step:3000, loss train:2.3998, loss val:2.4315\n",
      "step:3250, loss train:2.4067, loss val:2.4468\n",
      "step:3500, loss train:2.4067, loss val:2.4510\n",
      "step:3750, loss train:2.4197, loss val:2.4319\n",
      "step:4000, loss train:2.4079, loss val:2.4366\n",
      "step:4250, loss train:2.3901, loss val:2.4582\n",
      "step:4500, loss train:2.3997, loss val:2.4602\n",
      "step:4750, loss train:2.4191, loss val:2.4450\n",
      "step:5000, loss train:2.4095, loss val:2.4636\n",
      "step:5250, loss train:2.4135, loss val:2.4570\n",
      "step:5500, loss train:2.4113, loss val:2.4421\n",
      "step:5750, loss train:2.4150, loss val:2.4675\n",
      "step:6000, loss train:2.4033, loss val:2.4460\n",
      "step:6250, loss train:2.3985, loss val:2.4671\n",
      "step:6500, loss train:2.3915, loss val:2.4267\n",
      "step:6750, loss train:2.4207, loss val:2.4439\n",
      "step:7000, loss train:2.4132, loss val:2.4529\n",
      "step:7250, loss train:2.4139, loss val:2.4460\n",
      "step:7500, loss train:2.4143, loss val:2.4489\n",
      "step:7750, loss train:2.4256, loss val:2.4315\n",
      "step:8000, loss train:2.4225, loss val:2.4475\n",
      "step:8250, loss train:2.4148, loss val:2.4530\n",
      "step:8500, loss train:2.4059, loss val:2.4438\n",
      "step:8750, loss train:2.4333, loss val:2.4465\n",
      "step:9000, loss train:2.4190, loss val:2.4623\n",
      "step:9250, loss train:2.4060, loss val:2.4401\n",
      "step:9500, loss train:2.4108, loss val:2.4655\n",
      "step:9750, loss train:2.4203, loss val:2.4420\n",
      "batch:31\n",
      "step:0, loss train:2.4174, loss val:2.4491\n",
      "step:250, loss train:2.3986, loss val:2.4143\n",
      "step:500, loss train:2.4139, loss val:2.4324\n",
      "step:750, loss train:2.4270, loss val:2.4353\n",
      "step:1000, loss train:2.4095, loss val:2.4464\n",
      "step:1250, loss train:2.4035, loss val:2.4717\n",
      "step:1500, loss train:2.4199, loss val:2.4431\n",
      "step:1750, loss train:2.4323, loss val:2.4465\n",
      "step:2000, loss train:2.4532, loss val:2.4354\n",
      "step:2250, loss train:2.4018, loss val:2.4411\n",
      "step:2500, loss train:2.4094, loss val:2.4477\n",
      "step:2750, loss train:2.4103, loss val:2.4544\n",
      "step:3000, loss train:2.4137, loss val:2.4318\n",
      "step:3250, loss train:2.4225, loss val:2.4564\n",
      "step:3500, loss train:2.4074, loss val:2.4624\n",
      "step:3750, loss train:2.4195, loss val:2.4525\n",
      "step:4000, loss train:2.4146, loss val:2.4580\n",
      "step:4250, loss train:2.4120, loss val:2.4441\n",
      "step:4500, loss train:2.4139, loss val:2.4572\n",
      "step:4750, loss train:2.4118, loss val:2.4280\n",
      "step:5000, loss train:2.4075, loss val:2.4687\n",
      "step:5250, loss train:2.4045, loss val:2.4540\n",
      "step:5500, loss train:2.4251, loss val:2.4522\n",
      "step:5750, loss train:2.4090, loss val:2.4224\n",
      "step:6000, loss train:2.4013, loss val:2.4459\n",
      "step:6250, loss train:2.4057, loss val:2.4581\n",
      "step:6500, loss train:2.4033, loss val:2.4634\n",
      "step:6750, loss train:2.4232, loss val:2.4290\n",
      "step:7000, loss train:2.4070, loss val:2.4405\n",
      "step:7250, loss train:2.4189, loss val:2.4466\n",
      "step:7500, loss train:2.4024, loss val:2.4300\n",
      "step:7750, loss train:2.4165, loss val:2.4299\n",
      "step:8000, loss train:2.4138, loss val:2.4627\n",
      "step:8250, loss train:2.4191, loss val:2.4483\n",
      "step:8500, loss train:2.3959, loss val:2.4546\n",
      "step:8750, loss train:2.4069, loss val:2.4411\n",
      "step:9000, loss train:2.4160, loss val:2.4563\n",
      "step:9250, loss train:2.3963, loss val:2.4487\n",
      "step:9500, loss train:2.4216, loss val:2.4723\n",
      "step:9750, loss train:2.4317, loss val:2.4496\n",
      "batch:32\n",
      "step:0, loss train:2.4140, loss val:2.4543\n",
      "step:250, loss train:2.4049, loss val:2.4481\n",
      "step:500, loss train:2.4279, loss val:2.4381\n",
      "step:750, loss train:2.4163, loss val:2.4429\n",
      "step:1000, loss train:2.4177, loss val:2.4519\n",
      "step:1250, loss train:2.4129, loss val:2.4385\n",
      "step:1500, loss train:2.4125, loss val:2.4514\n",
      "step:1750, loss train:2.4165, loss val:2.4394\n",
      "step:2000, loss train:2.4112, loss val:2.4274\n",
      "step:2250, loss train:2.4168, loss val:2.4339\n",
      "step:2500, loss train:2.4067, loss val:2.4403\n",
      "step:2750, loss train:2.4138, loss val:2.4358\n",
      "step:3000, loss train:2.4197, loss val:2.4355\n",
      "step:3250, loss train:2.4147, loss val:2.4417\n",
      "step:3500, loss train:2.3901, loss val:2.4166\n",
      "step:3750, loss train:2.4073, loss val:2.4566\n",
      "step:4000, loss train:2.4050, loss val:2.4442\n",
      "step:4250, loss train:2.3997, loss val:2.4467\n",
      "step:4500, loss train:2.4113, loss val:2.4411\n",
      "step:4750, loss train:2.4209, loss val:2.4153\n",
      "step:5000, loss train:2.4027, loss val:2.4382\n",
      "step:5250, loss train:2.4027, loss val:2.4378\n",
      "step:5500, loss train:2.4261, loss val:2.4455\n",
      "step:5750, loss train:2.4175, loss val:2.4558\n",
      "step:6000, loss train:2.4150, loss val:2.4565\n",
      "step:6250, loss train:2.4209, loss val:2.4473\n",
      "step:6500, loss train:2.4054, loss val:2.4552\n",
      "step:6750, loss train:2.4234, loss val:2.4680\n",
      "step:7000, loss train:2.4110, loss val:2.4666\n",
      "step:7250, loss train:2.4100, loss val:2.4494\n",
      "step:7500, loss train:2.4085, loss val:2.4630\n",
      "step:7750, loss train:2.4082, loss val:2.4388\n",
      "step:8000, loss train:2.4071, loss val:2.4548\n",
      "step:8250, loss train:2.4210, loss val:2.4513\n",
      "step:8500, loss train:2.4160, loss val:2.4501\n",
      "step:8750, loss train:2.4184, loss val:2.4510\n",
      "step:9000, loss train:2.4091, loss val:2.4594\n",
      "step:9250, loss train:2.4261, loss val:2.4409\n",
      "step:9500, loss train:2.4099, loss val:2.4539\n",
      "step:9750, loss train:2.3996, loss val:2.4395\n",
      "batch:33\n",
      "step:0, loss train:2.4163, loss val:2.4471\n",
      "step:250, loss train:2.4225, loss val:2.4298\n",
      "step:500, loss train:2.4100, loss val:2.4355\n",
      "step:750, loss train:2.4134, loss val:2.4652\n",
      "step:1000, loss train:2.4095, loss val:2.4517\n",
      "step:1250, loss train:2.4091, loss val:2.4246\n",
      "step:1500, loss train:2.4194, loss val:2.4474\n",
      "step:1750, loss train:2.4130, loss val:2.4434\n",
      "step:2000, loss train:2.4242, loss val:2.4386\n",
      "step:2250, loss train:2.4077, loss val:2.4553\n",
      "step:2500, loss train:2.4105, loss val:2.4474\n",
      "step:2750, loss train:2.3965, loss val:2.4352\n",
      "step:3000, loss train:2.4009, loss val:2.4605\n",
      "step:3250, loss train:2.4040, loss val:2.4697\n",
      "step:3500, loss train:2.3952, loss val:2.4409\n",
      "step:3750, loss train:2.4298, loss val:2.4688\n",
      "step:4000, loss train:2.4255, loss val:2.4609\n",
      "step:4250, loss train:2.4200, loss val:2.4377\n",
      "step:4500, loss train:2.4104, loss val:2.4748\n",
      "step:4750, loss train:2.4236, loss val:2.4324\n",
      "step:5000, loss train:2.4255, loss val:2.4663\n",
      "step:5250, loss train:2.4155, loss val:2.4356\n",
      "step:5500, loss train:2.4160, loss val:2.4371\n",
      "step:5750, loss train:2.4196, loss val:2.4434\n",
      "step:6000, loss train:2.4057, loss val:2.4380\n",
      "step:6250, loss train:2.4007, loss val:2.4449\n",
      "step:6500, loss train:2.4233, loss val:2.4591\n",
      "step:6750, loss train:2.4230, loss val:2.4488\n",
      "step:7000, loss train:2.4203, loss val:2.4593\n",
      "step:7250, loss train:2.4218, loss val:2.4286\n",
      "step:7500, loss train:2.4187, loss val:2.4593\n",
      "step:7750, loss train:2.4127, loss val:2.4486\n",
      "step:8000, loss train:2.4034, loss val:2.4504\n",
      "step:8250, loss train:2.4377, loss val:2.4399\n",
      "step:8500, loss train:2.4180, loss val:2.4473\n",
      "step:8750, loss train:2.4184, loss val:2.4489\n",
      "step:9000, loss train:2.3995, loss val:2.4565\n",
      "step:9250, loss train:2.4209, loss val:2.4404\n",
      "step:9500, loss train:2.4152, loss val:2.4756\n",
      "step:9750, loss train:2.4110, loss val:2.4434\n",
      "batch:34\n",
      "step:0, loss train:2.4203, loss val:2.4546\n",
      "step:250, loss train:2.3932, loss val:2.4325\n",
      "step:500, loss train:2.4313, loss val:2.4347\n",
      "step:750, loss train:2.4205, loss val:2.4390\n",
      "step:1000, loss train:2.4159, loss val:2.4428\n",
      "step:1250, loss train:2.3883, loss val:2.4336\n",
      "step:1500, loss train:2.4188, loss val:2.4339\n",
      "step:1750, loss train:2.4047, loss val:2.4321\n",
      "step:2000, loss train:2.4090, loss val:2.4337\n",
      "step:2250, loss train:2.4020, loss val:2.4486\n",
      "step:2500, loss train:2.4053, loss val:2.4562\n",
      "step:2750, loss train:2.4283, loss val:2.4446\n",
      "step:3000, loss train:2.4023, loss val:2.4546\n",
      "step:3250, loss train:2.4151, loss val:2.4223\n",
      "step:3500, loss train:2.3954, loss val:2.4574\n",
      "step:3750, loss train:2.4119, loss val:2.4557\n",
      "step:4000, loss train:2.4083, loss val:2.4609\n",
      "step:4250, loss train:2.4170, loss val:2.4488\n",
      "step:4500, loss train:2.3995, loss val:2.4514\n",
      "step:4750, loss train:2.4059, loss val:2.4758\n",
      "step:5000, loss train:2.4041, loss val:2.4284\n",
      "step:5250, loss train:2.4046, loss val:2.4309\n",
      "step:5500, loss train:2.4138, loss val:2.4432\n",
      "step:5750, loss train:2.4236, loss val:2.4366\n",
      "step:6000, loss train:2.4262, loss val:2.4249\n",
      "step:6250, loss train:2.4024, loss val:2.4751\n",
      "step:6500, loss train:2.4138, loss val:2.4398\n",
      "step:6750, loss train:2.3952, loss val:2.4246\n",
      "step:7000, loss train:2.4249, loss val:2.4635\n",
      "step:7250, loss train:2.3928, loss val:2.4329\n",
      "step:7500, loss train:2.4039, loss val:2.4393\n",
      "step:7750, loss train:2.3903, loss val:2.4574\n",
      "step:8000, loss train:2.4081, loss val:2.4547\n",
      "step:8250, loss train:2.4163, loss val:2.4348\n",
      "step:8500, loss train:2.4274, loss val:2.4393\n",
      "step:8750, loss train:2.4127, loss val:2.4391\n",
      "step:9000, loss train:2.4151, loss val:2.4619\n",
      "step:9250, loss train:2.4197, loss val:2.4446\n",
      "step:9500, loss train:2.4203, loss val:2.4357\n",
      "step:9750, loss train:2.4001, loss val:2.4583\n",
      "batch:35\n",
      "step:0, loss train:2.4225, loss val:2.4446\n",
      "step:250, loss train:2.4055, loss val:2.4721\n",
      "step:500, loss train:2.4395, loss val:2.4413\n",
      "step:750, loss train:2.4121, loss val:2.4431\n",
      "step:1000, loss train:2.3990, loss val:2.4268\n",
      "step:1250, loss train:2.4162, loss val:2.4403\n",
      "step:1500, loss train:2.4094, loss val:2.4248\n",
      "step:1750, loss train:2.3996, loss val:2.4428\n",
      "step:2000, loss train:2.4079, loss val:2.4557\n",
      "step:2250, loss train:2.4073, loss val:2.4386\n",
      "step:2500, loss train:2.4143, loss val:2.4365\n",
      "step:2750, loss train:2.4427, loss val:2.4277\n",
      "step:3000, loss train:2.4195, loss val:2.4352\n",
      "step:3250, loss train:2.4355, loss val:2.4458\n",
      "step:3500, loss train:2.4071, loss val:2.4420\n",
      "step:3750, loss train:2.4419, loss val:2.4534\n",
      "step:4000, loss train:2.4025, loss val:2.4339\n",
      "step:4250, loss train:2.4233, loss val:2.4035\n",
      "step:4500, loss train:2.4165, loss val:2.4281\n",
      "step:4750, loss train:2.4110, loss val:2.4599\n",
      "step:5000, loss train:2.4005, loss val:2.4498\n",
      "step:5250, loss train:2.4255, loss val:2.4189\n",
      "step:5500, loss train:2.4146, loss val:2.4356\n",
      "step:5750, loss train:2.4218, loss val:2.4382\n",
      "step:6000, loss train:2.4209, loss val:2.4387\n",
      "step:6250, loss train:2.3960, loss val:2.4538\n",
      "step:6500, loss train:2.4089, loss val:2.4510\n",
      "step:6750, loss train:2.4233, loss val:2.4418\n",
      "step:7000, loss train:2.4209, loss val:2.4694\n",
      "step:7250, loss train:2.3899, loss val:2.4293\n",
      "step:7500, loss train:2.4148, loss val:2.4514\n",
      "step:7750, loss train:2.4045, loss val:2.4128\n",
      "step:8000, loss train:2.4072, loss val:2.4479\n",
      "step:8250, loss train:2.4215, loss val:2.4280\n",
      "step:8500, loss train:2.3976, loss val:2.4393\n",
      "step:8750, loss train:2.4122, loss val:2.4457\n",
      "step:9000, loss train:2.4092, loss val:2.4438\n",
      "step:9250, loss train:2.4064, loss val:2.4539\n",
      "step:9500, loss train:2.4135, loss val:2.4470\n",
      "step:9750, loss train:2.4246, loss val:2.4582\n",
      "batch:36\n",
      "step:0, loss train:2.4143, loss val:2.4510\n",
      "step:250, loss train:2.4210, loss val:2.4192\n",
      "step:500, loss train:2.4196, loss val:2.4228\n",
      "step:750, loss train:2.4014, loss val:2.4457\n",
      "step:1000, loss train:2.4172, loss val:2.4512\n",
      "step:1250, loss train:2.4121, loss val:2.4345\n",
      "step:1500, loss train:2.4262, loss val:2.4683\n",
      "step:1750, loss train:2.4061, loss val:2.4488\n",
      "step:2000, loss train:2.4085, loss val:2.4439\n",
      "step:2250, loss train:2.4197, loss val:2.4291\n",
      "step:2500, loss train:2.3920, loss val:2.4284\n",
      "step:2750, loss train:2.4290, loss val:2.4433\n",
      "step:3000, loss train:2.4144, loss val:2.4510\n",
      "step:3250, loss train:2.4114, loss val:2.4437\n",
      "step:3500, loss train:2.4203, loss val:2.4354\n",
      "step:3750, loss train:2.4048, loss val:2.4388\n",
      "step:4000, loss train:2.4060, loss val:2.4515\n",
      "step:4250, loss train:2.4109, loss val:2.4515\n",
      "step:4500, loss train:2.4115, loss val:2.4426\n",
      "step:4750, loss train:2.4055, loss val:2.4415\n",
      "step:5000, loss train:2.3960, loss val:2.4552\n",
      "step:5250, loss train:2.4044, loss val:2.4384\n",
      "step:5500, loss train:2.4130, loss val:2.4435\n",
      "step:5750, loss train:2.4083, loss val:2.4399\n",
      "step:6000, loss train:2.4030, loss val:2.4264\n",
      "step:6250, loss train:2.4205, loss val:2.4459\n",
      "step:6500, loss train:2.4183, loss val:2.4533\n",
      "step:6750, loss train:2.4151, loss val:2.4469\n",
      "step:7000, loss train:2.4386, loss val:2.4376\n",
      "step:7250, loss train:2.4189, loss val:2.4646\n",
      "step:7500, loss train:2.4223, loss val:2.4579\n",
      "step:7750, loss train:2.4149, loss val:2.4548\n",
      "step:8000, loss train:2.4230, loss val:2.4563\n",
      "step:8250, loss train:2.4008, loss val:2.4489\n",
      "step:8500, loss train:2.4121, loss val:2.4526\n",
      "step:8750, loss train:2.3960, loss val:2.4282\n",
      "step:9000, loss train:2.4276, loss val:2.4404\n",
      "step:9250, loss train:2.4013, loss val:2.4366\n",
      "step:9500, loss train:2.4209, loss val:2.4302\n",
      "step:9750, loss train:2.4260, loss val:2.4342\n",
      "batch:37\n",
      "step:0, loss train:2.4254, loss val:2.4407\n",
      "step:250, loss train:2.4147, loss val:2.4440\n",
      "step:500, loss train:2.4140, loss val:2.4650\n",
      "step:750, loss train:2.4378, loss val:2.4633\n",
      "step:1000, loss train:2.3958, loss val:2.4519\n",
      "step:1250, loss train:2.4138, loss val:2.4395\n",
      "step:1500, loss train:2.3957, loss val:2.4500\n",
      "step:1750, loss train:2.3991, loss val:2.4695\n",
      "step:2000, loss train:2.4067, loss val:2.4323\n",
      "step:2250, loss train:2.4299, loss val:2.4309\n",
      "step:2500, loss train:2.4020, loss val:2.4511\n",
      "step:2750, loss train:2.4304, loss val:2.4291\n",
      "step:3000, loss train:2.4117, loss val:2.4210\n",
      "step:3250, loss train:2.4038, loss val:2.4451\n",
      "step:3500, loss train:2.4142, loss val:2.4552\n",
      "step:3750, loss train:2.3951, loss val:2.4435\n",
      "step:4000, loss train:2.4026, loss val:2.4507\n",
      "step:4250, loss train:2.4153, loss val:2.4460\n",
      "step:4500, loss train:2.4362, loss val:2.4503\n",
      "step:4750, loss train:2.4136, loss val:2.4434\n",
      "step:5000, loss train:2.4306, loss val:2.4308\n",
      "step:5250, loss train:2.4063, loss val:2.4357\n",
      "step:5500, loss train:2.4184, loss val:2.4606\n",
      "step:5750, loss train:2.4034, loss val:2.4252\n",
      "step:6000, loss train:2.3969, loss val:2.4357\n",
      "step:6250, loss train:2.4115, loss val:2.4624\n",
      "step:6500, loss train:2.4124, loss val:2.4105\n",
      "step:6750, loss train:2.4077, loss val:2.4487\n",
      "step:7000, loss train:2.4153, loss val:2.4561\n",
      "step:7250, loss train:2.4070, loss val:2.4426\n",
      "step:7500, loss train:2.4144, loss val:2.4622\n",
      "step:7750, loss train:2.4014, loss val:2.4550\n",
      "step:8000, loss train:2.4213, loss val:2.4499\n",
      "step:8250, loss train:2.4322, loss val:2.4541\n",
      "step:8500, loss train:2.4093, loss val:2.4698\n",
      "step:8750, loss train:2.3994, loss val:2.4606\n",
      "step:9000, loss train:2.4032, loss val:2.4700\n",
      "step:9250, loss train:2.3918, loss val:2.4415\n",
      "step:9500, loss train:2.4234, loss val:2.4294\n",
      "step:9750, loss train:2.4057, loss val:2.4323\n",
      "batch:38\n",
      "step:0, loss train:2.4157, loss val:2.4620\n",
      "step:250, loss train:2.3812, loss val:2.4456\n",
      "step:500, loss train:2.4092, loss val:2.4496\n",
      "step:750, loss train:2.3945, loss val:2.4536\n",
      "step:1000, loss train:2.4332, loss val:2.4496\n",
      "step:1250, loss train:2.4151, loss val:2.4334\n",
      "step:1500, loss train:2.4139, loss val:2.4551\n",
      "step:1750, loss train:2.3971, loss val:2.4396\n",
      "step:2000, loss train:2.4166, loss val:2.4450\n",
      "step:2250, loss train:2.4159, loss val:2.4615\n",
      "step:2500, loss train:2.4154, loss val:2.4591\n",
      "step:2750, loss train:2.4200, loss val:2.4513\n",
      "step:3000, loss train:2.4036, loss val:2.4743\n",
      "step:3250, loss train:2.4173, loss val:2.4442\n",
      "step:3500, loss train:2.4132, loss val:2.4219\n",
      "step:3750, loss train:2.3948, loss val:2.4508\n",
      "step:4000, loss train:2.3999, loss val:2.4510\n",
      "step:4250, loss train:2.4157, loss val:2.4698\n",
      "step:4500, loss train:2.4125, loss val:2.4453\n",
      "step:4750, loss train:2.4140, loss val:2.4584\n",
      "step:5000, loss train:2.4124, loss val:2.4339\n",
      "step:5250, loss train:2.4129, loss val:2.4459\n",
      "step:5500, loss train:2.4193, loss val:2.4479\n",
      "step:5750, loss train:2.3933, loss val:2.4309\n",
      "step:6000, loss train:2.3986, loss val:2.4271\n",
      "step:6250, loss train:2.3887, loss val:2.4439\n",
      "step:6500, loss train:2.3998, loss val:2.4421\n",
      "step:6750, loss train:2.4320, loss val:2.4481\n",
      "step:7000, loss train:2.4168, loss val:2.4570\n",
      "step:7250, loss train:2.4073, loss val:2.4425\n",
      "step:7500, loss train:2.4365, loss val:2.4384\n",
      "step:7750, loss train:2.3919, loss val:2.4444\n",
      "step:8000, loss train:2.4119, loss val:2.4126\n",
      "step:8250, loss train:2.4230, loss val:2.4351\n",
      "step:8500, loss train:2.4128, loss val:2.4517\n",
      "step:8750, loss train:2.4127, loss val:2.4311\n",
      "step:9000, loss train:2.4092, loss val:2.4251\n",
      "step:9250, loss train:2.4152, loss val:2.4497\n",
      "step:9500, loss train:2.4273, loss val:2.4397\n",
      "step:9750, loss train:2.4117, loss val:2.4715\n",
      "batch:39\n",
      "step:0, loss train:2.4177, loss val:2.4326\n",
      "step:250, loss train:2.4234, loss val:2.4579\n",
      "step:500, loss train:2.4044, loss val:2.4610\n",
      "step:750, loss train:2.3915, loss val:2.4506\n",
      "step:1000, loss train:2.3944, loss val:2.4406\n",
      "step:1250, loss train:2.4292, loss val:2.4245\n",
      "step:1500, loss train:2.4043, loss val:2.4305\n",
      "step:1750, loss train:2.4000, loss val:2.4373\n",
      "step:2000, loss train:2.4320, loss val:2.4439\n",
      "step:2250, loss train:2.3955, loss val:2.4541\n",
      "step:2500, loss train:2.4035, loss val:2.4431\n",
      "step:2750, loss train:2.4289, loss val:2.4565\n",
      "step:3000, loss train:2.3931, loss val:2.4381\n",
      "step:3250, loss train:2.4173, loss val:2.4505\n",
      "step:3500, loss train:2.4157, loss val:2.4677\n",
      "step:3750, loss train:2.4207, loss val:2.4341\n",
      "step:4000, loss train:2.4151, loss val:2.4310\n",
      "step:4250, loss train:2.4204, loss val:2.4284\n",
      "step:4500, loss train:2.3951, loss val:2.4431\n",
      "step:4750, loss train:2.4160, loss val:2.4587\n",
      "step:5000, loss train:2.4348, loss val:2.4407\n",
      "step:5250, loss train:2.4107, loss val:2.4583\n",
      "step:5500, loss train:2.4260, loss val:2.4441\n",
      "step:5750, loss train:2.4181, loss val:2.4321\n",
      "step:6000, loss train:2.4162, loss val:2.4464\n",
      "step:6250, loss train:2.4199, loss val:2.4256\n",
      "step:6500, loss train:2.4070, loss val:2.4292\n",
      "step:6750, loss train:2.4084, loss val:2.4429\n",
      "step:7000, loss train:2.4107, loss val:2.4356\n",
      "step:7250, loss train:2.4248, loss val:2.4335\n",
      "step:7500, loss train:2.4077, loss val:2.4490\n",
      "step:7750, loss train:2.4151, loss val:2.4491\n",
      "step:8000, loss train:2.4086, loss val:2.4324\n",
      "step:8250, loss train:2.4235, loss val:2.4513\n",
      "step:8500, loss train:2.4292, loss val:2.4477\n",
      "step:8750, loss train:2.4127, loss val:2.4390\n",
      "step:9000, loss train:2.4141, loss val:2.4402\n",
      "step:9250, loss train:2.4076, loss val:2.4605\n",
      "step:9500, loss train:2.3961, loss val:2.4682\n",
      "step:9750, loss train:2.4101, loss val:2.4399\n",
      "batch:40\n",
      "step:0, loss train:2.4372, loss val:2.4701\n",
      "step:250, loss train:2.4282, loss val:2.4366\n",
      "step:500, loss train:2.4085, loss val:2.4468\n",
      "step:750, loss train:2.4184, loss val:2.4547\n",
      "step:1000, loss train:2.4075, loss val:2.4568\n",
      "step:1250, loss train:2.4109, loss val:2.4466\n",
      "step:1500, loss train:2.4228, loss val:2.4621\n",
      "step:1750, loss train:2.4029, loss val:2.4515\n",
      "step:2000, loss train:2.4076, loss val:2.4373\n",
      "step:2250, loss train:2.4048, loss val:2.4520\n",
      "step:2500, loss train:2.4152, loss val:2.4454\n",
      "step:2750, loss train:2.3956, loss val:2.4350\n",
      "step:3000, loss train:2.4043, loss val:2.4516\n",
      "step:3250, loss train:2.4310, loss val:2.4270\n",
      "step:3500, loss train:2.4310, loss val:2.4639\n",
      "step:3750, loss train:2.3946, loss val:2.4507\n",
      "step:4000, loss train:2.4213, loss val:2.4779\n",
      "step:4250, loss train:2.4386, loss val:2.4259\n",
      "step:4500, loss train:2.4018, loss val:2.4357\n",
      "step:4750, loss train:2.4020, loss val:2.4258\n",
      "step:5000, loss train:2.4228, loss val:2.4489\n",
      "step:5250, loss train:2.4206, loss val:2.4311\n",
      "step:5500, loss train:2.4075, loss val:2.4305\n",
      "step:5750, loss train:2.4120, loss val:2.4349\n",
      "step:6000, loss train:2.4098, loss val:2.4618\n",
      "step:6250, loss train:2.4157, loss val:2.4628\n",
      "step:6500, loss train:2.4144, loss val:2.4292\n",
      "step:6750, loss train:2.4152, loss val:2.4387\n",
      "step:7000, loss train:2.4221, loss val:2.4643\n",
      "step:7250, loss train:2.4245, loss val:2.4500\n",
      "step:7500, loss train:2.4017, loss val:2.4411\n",
      "step:7750, loss train:2.3994, loss val:2.4468\n",
      "step:8000, loss train:2.4130, loss val:2.4468\n",
      "step:8250, loss train:2.4156, loss val:2.4607\n",
      "step:8500, loss train:2.4240, loss val:2.4235\n",
      "step:8750, loss train:2.4160, loss val:2.4529\n",
      "step:9000, loss train:2.3946, loss val:2.4287\n",
      "step:9250, loss train:2.4051, loss val:2.4617\n",
      "step:9500, loss train:2.4063, loss val:2.4339\n",
      "step:9750, loss train:2.3995, loss val:2.4409\n",
      "batch:41\n",
      "step:0, loss train:2.3968, loss val:2.4416\n",
      "step:250, loss train:2.4210, loss val:2.4427\n",
      "step:500, loss train:2.4151, loss val:2.4350\n",
      "step:750, loss train:2.3978, loss val:2.4264\n",
      "step:1000, loss train:2.4159, loss val:2.4220\n",
      "step:1250, loss train:2.4160, loss val:2.4385\n",
      "step:1500, loss train:2.4124, loss val:2.4442\n",
      "step:1750, loss train:2.4184, loss val:2.4296\n",
      "step:2000, loss train:2.4104, loss val:2.4557\n",
      "step:2250, loss train:2.4056, loss val:2.4349\n",
      "step:2500, loss train:2.4115, loss val:2.4368\n",
      "step:2750, loss train:2.4039, loss val:2.4718\n",
      "step:3000, loss train:2.4249, loss val:2.4263\n",
      "step:3250, loss train:2.3990, loss val:2.4554\n",
      "step:3500, loss train:2.4004, loss val:2.4377\n",
      "step:3750, loss train:2.4003, loss val:2.4400\n",
      "step:4000, loss train:2.4178, loss val:2.4415\n",
      "step:4250, loss train:2.4155, loss val:2.4380\n",
      "step:4500, loss train:2.3905, loss val:2.4493\n",
      "step:4750, loss train:2.4165, loss val:2.4539\n",
      "step:5000, loss train:2.4173, loss val:2.4547\n",
      "step:5250, loss train:2.4212, loss val:2.4542\n",
      "step:5500, loss train:2.4015, loss val:2.4547\n",
      "step:5750, loss train:2.4111, loss val:2.4568\n",
      "step:6000, loss train:2.4261, loss val:2.4318\n",
      "step:6250, loss train:2.4173, loss val:2.4481\n",
      "step:6500, loss train:2.4127, loss val:2.4535\n",
      "step:6750, loss train:2.4124, loss val:2.4428\n",
      "step:7000, loss train:2.4155, loss val:2.4520\n",
      "step:7250, loss train:2.4134, loss val:2.4575\n",
      "step:7500, loss train:2.4228, loss val:2.4646\n",
      "step:7750, loss train:2.4090, loss val:2.4323\n",
      "step:8000, loss train:2.3928, loss val:2.4675\n",
      "step:8250, loss train:2.4042, loss val:2.4360\n",
      "step:8500, loss train:2.3956, loss val:2.4286\n",
      "step:8750, loss train:2.4044, loss val:2.4666\n",
      "step:9000, loss train:2.4017, loss val:2.4549\n",
      "step:9250, loss train:2.4105, loss val:2.4303\n",
      "step:9500, loss train:2.4095, loss val:2.4373\n",
      "step:9750, loss train:2.4284, loss val:2.4652\n",
      "batch:42\n",
      "step:0, loss train:2.4294, loss val:2.4194\n",
      "step:250, loss train:2.4143, loss val:2.4496\n",
      "step:500, loss train:2.4197, loss val:2.4630\n",
      "step:750, loss train:2.4075, loss val:2.4613\n",
      "step:1000, loss train:2.3971, loss val:2.4362\n",
      "step:1250, loss train:2.4089, loss val:2.4594\n",
      "step:1500, loss train:2.4188, loss val:2.4471\n",
      "step:1750, loss train:2.4165, loss val:2.4169\n",
      "step:2000, loss train:2.4044, loss val:2.4573\n",
      "step:2250, loss train:2.4157, loss val:2.4501\n",
      "step:2500, loss train:2.4356, loss val:2.4433\n",
      "step:2750, loss train:2.3995, loss val:2.4382\n",
      "step:3000, loss train:2.4316, loss val:2.4338\n",
      "step:3250, loss train:2.4291, loss val:2.4355\n",
      "step:3500, loss train:2.4248, loss val:2.4579\n",
      "step:3750, loss train:2.4001, loss val:2.4401\n",
      "step:4000, loss train:2.4294, loss val:2.4517\n",
      "step:4250, loss train:2.4230, loss val:2.4443\n",
      "step:4500, loss train:2.4205, loss val:2.4451\n",
      "step:4750, loss train:2.4066, loss val:2.4510\n",
      "step:5000, loss train:2.4380, loss val:2.4386\n",
      "step:5250, loss train:2.4147, loss val:2.4275\n",
      "step:5500, loss train:2.4077, loss val:2.4448\n",
      "step:5750, loss train:2.4128, loss val:2.4405\n",
      "step:6000, loss train:2.4207, loss val:2.4596\n",
      "step:6250, loss train:2.4171, loss val:2.4693\n",
      "step:6500, loss train:2.4130, loss val:2.4490\n",
      "step:6750, loss train:2.4078, loss val:2.4587\n",
      "step:7000, loss train:2.4184, loss val:2.4482\n",
      "step:7250, loss train:2.4134, loss val:2.4312\n",
      "step:7500, loss train:2.4144, loss val:2.4394\n",
      "step:7750, loss train:2.4150, loss val:2.4503\n",
      "step:8000, loss train:2.4024, loss val:2.4734\n",
      "step:8250, loss train:2.4007, loss val:2.4258\n",
      "step:8500, loss train:2.4129, loss val:2.4305\n",
      "step:8750, loss train:2.4004, loss val:2.4479\n",
      "step:9000, loss train:2.4017, loss val:2.4493\n",
      "step:9250, loss train:2.4009, loss val:2.4637\n",
      "step:9500, loss train:2.4142, loss val:2.4223\n",
      "step:9750, loss train:2.4047, loss val:2.4527\n",
      "batch:43\n",
      "step:0, loss train:2.4135, loss val:2.4435\n",
      "step:250, loss train:2.3843, loss val:2.4377\n",
      "step:500, loss train:2.4155, loss val:2.4389\n",
      "step:750, loss train:2.3912, loss val:2.4363\n",
      "step:1000, loss train:2.4212, loss val:2.4489\n",
      "step:1250, loss train:2.4076, loss val:2.4149\n",
      "step:1500, loss train:2.4218, loss val:2.4544\n",
      "step:1750, loss train:2.4082, loss val:2.4406\n",
      "step:2000, loss train:2.4205, loss val:2.4483\n",
      "step:2250, loss train:2.4376, loss val:2.4252\n",
      "step:2500, loss train:2.4147, loss val:2.4463\n",
      "step:2750, loss train:2.4187, loss val:2.4468\n",
      "step:3000, loss train:2.4094, loss val:2.4365\n",
      "step:3250, loss train:2.4204, loss val:2.4421\n",
      "step:3500, loss train:2.4170, loss val:2.4537\n",
      "step:3750, loss train:2.4044, loss val:2.4589\n",
      "step:4000, loss train:2.4084, loss val:2.4509\n",
      "step:4250, loss train:2.4141, loss val:2.4562\n",
      "step:4500, loss train:2.4197, loss val:2.4493\n",
      "step:4750, loss train:2.4230, loss val:2.4427\n",
      "step:5000, loss train:2.4032, loss val:2.4476\n",
      "step:5250, loss train:2.3894, loss val:2.4584\n",
      "step:5500, loss train:2.4107, loss val:2.4675\n",
      "step:5750, loss train:2.4130, loss val:2.4387\n",
      "step:6000, loss train:2.4125, loss val:2.4395\n",
      "step:6250, loss train:2.4153, loss val:2.4355\n",
      "step:6500, loss train:2.4117, loss val:2.4528\n",
      "step:6750, loss train:2.4247, loss val:2.4478\n",
      "step:7000, loss train:2.4248, loss val:2.4776\n",
      "step:7250, loss train:2.4230, loss val:2.4324\n",
      "step:7500, loss train:2.4128, loss val:2.4625\n",
      "step:7750, loss train:2.3890, loss val:2.4518\n",
      "step:8000, loss train:2.4196, loss val:2.4591\n",
      "step:8250, loss train:2.4271, loss val:2.4253\n",
      "step:8500, loss train:2.4207, loss val:2.4550\n",
      "step:8750, loss train:2.4280, loss val:2.4523\n",
      "step:9000, loss train:2.4087, loss val:2.4592\n",
      "step:9250, loss train:2.4321, loss val:2.4637\n",
      "step:9500, loss train:2.4103, loss val:2.4400\n",
      "step:9750, loss train:2.4062, loss val:2.4572\n",
      "batch:44\n",
      "step:0, loss train:2.4187, loss val:2.4236\n",
      "step:250, loss train:2.4216, loss val:2.4408\n",
      "step:500, loss train:2.4035, loss val:2.4446\n",
      "step:750, loss train:2.4156, loss val:2.4569\n",
      "step:1000, loss train:2.4110, loss val:2.4577\n",
      "step:1250, loss train:2.4238, loss val:2.4397\n",
      "step:1500, loss train:2.4077, loss val:2.4510\n",
      "step:1750, loss train:2.3981, loss val:2.4317\n",
      "step:2000, loss train:2.4343, loss val:2.4632\n",
      "step:2250, loss train:2.4102, loss val:2.4530\n",
      "step:2500, loss train:2.4175, loss val:2.4659\n",
      "step:2750, loss train:2.4204, loss val:2.4423\n",
      "step:3000, loss train:2.4127, loss val:2.4325\n",
      "step:3250, loss train:2.4054, loss val:2.4556\n",
      "step:3500, loss train:2.4216, loss val:2.4543\n",
      "step:3750, loss train:2.4075, loss val:2.4492\n",
      "step:4000, loss train:2.3961, loss val:2.4356\n",
      "step:4250, loss train:2.4056, loss val:2.4384\n",
      "step:4500, loss train:2.4018, loss val:2.4631\n",
      "step:4750, loss train:2.4398, loss val:2.4661\n",
      "step:5000, loss train:2.4232, loss val:2.4141\n",
      "step:5250, loss train:2.4108, loss val:2.4430\n",
      "step:5500, loss train:2.4126, loss val:2.4437\n",
      "step:5750, loss train:2.4045, loss val:2.4272\n",
      "step:6000, loss train:2.3983, loss val:2.4413\n",
      "step:6250, loss train:2.4181, loss val:2.4515\n",
      "step:6500, loss train:2.4185, loss val:2.4581\n",
      "step:6750, loss train:2.3859, loss val:2.4530\n",
      "step:7000, loss train:2.4253, loss val:2.4616\n",
      "step:7250, loss train:2.4151, loss val:2.4354\n",
      "step:7500, loss train:2.4033, loss val:2.4405\n",
      "step:7750, loss train:2.4246, loss val:2.4609\n",
      "step:8000, loss train:2.3871, loss val:2.4487\n",
      "step:8250, loss train:2.4100, loss val:2.4267\n",
      "step:8500, loss train:2.4128, loss val:2.4576\n",
      "step:8750, loss train:2.4209, loss val:2.4337\n",
      "step:9000, loss train:2.4280, loss val:2.4173\n",
      "step:9250, loss train:2.3971, loss val:2.4186\n",
      "step:9500, loss train:2.4219, loss val:2.4525\n",
      "step:9750, loss train:2.4109, loss val:2.4336\n",
      "batch:45\n",
      "step:0, loss train:2.4064, loss val:2.4653\n",
      "step:250, loss train:2.4060, loss val:2.4525\n",
      "step:500, loss train:2.4123, loss val:2.4355\n",
      "step:750, loss train:2.4266, loss val:2.4388\n",
      "step:1000, loss train:2.4025, loss val:2.4590\n",
      "step:1250, loss train:2.4099, loss val:2.4631\n",
      "step:1500, loss train:2.4181, loss val:2.4331\n",
      "step:1750, loss train:2.4132, loss val:2.4499\n",
      "step:2000, loss train:2.4248, loss val:2.4589\n",
      "step:2250, loss train:2.4085, loss val:2.4413\n",
      "step:2500, loss train:2.4127, loss val:2.4549\n",
      "step:2750, loss train:2.3997, loss val:2.4377\n",
      "step:3000, loss train:2.4147, loss val:2.4420\n",
      "step:3250, loss train:2.4091, loss val:2.4685\n",
      "step:3500, loss train:2.4129, loss val:2.4513\n",
      "step:3750, loss train:2.4197, loss val:2.4475\n",
      "step:4000, loss train:2.4188, loss val:2.4656\n",
      "step:4250, loss train:2.4042, loss val:2.4555\n",
      "step:4500, loss train:2.4093, loss val:2.4364\n",
      "step:4750, loss train:2.3931, loss val:2.4622\n",
      "step:5000, loss train:2.4211, loss val:2.4358\n",
      "step:5250, loss train:2.4219, loss val:2.4329\n",
      "step:5500, loss train:2.4215, loss val:2.4574\n",
      "step:5750, loss train:2.4353, loss val:2.4594\n",
      "step:6000, loss train:2.4226, loss val:2.4285\n",
      "step:6250, loss train:2.4083, loss val:2.4451\n",
      "step:6500, loss train:2.4122, loss val:2.4520\n",
      "step:6750, loss train:2.4175, loss val:2.4435\n",
      "step:7000, loss train:2.4134, loss val:2.4523\n",
      "step:7250, loss train:2.4186, loss val:2.4359\n",
      "step:7500, loss train:2.4152, loss val:2.4667\n",
      "step:7750, loss train:2.4225, loss val:2.4471\n",
      "step:8000, loss train:2.4147, loss val:2.4363\n",
      "step:8250, loss train:2.4300, loss val:2.4432\n",
      "step:8500, loss train:2.3925, loss val:2.4380\n",
      "step:8750, loss train:2.4082, loss val:2.4672\n",
      "step:9000, loss train:2.4139, loss val:2.4561\n",
      "step:9250, loss train:2.4214, loss val:2.4370\n",
      "step:9500, loss train:2.4072, loss val:2.4653\n",
      "step:9750, loss train:2.4079, loss val:2.4669\n",
      "batch:46\n",
      "step:0, loss train:2.3995, loss val:2.4380\n",
      "step:250, loss train:2.4080, loss val:2.4600\n",
      "step:500, loss train:2.4069, loss val:2.4353\n",
      "step:750, loss train:2.4217, loss val:2.4461\n",
      "step:1000, loss train:2.4139, loss val:2.4341\n",
      "step:1250, loss train:2.4157, loss val:2.4537\n",
      "step:1500, loss train:2.4232, loss val:2.4564\n",
      "step:1750, loss train:2.4190, loss val:2.4413\n",
      "step:2000, loss train:2.4156, loss val:2.4436\n",
      "step:2250, loss train:2.4007, loss val:2.4523\n",
      "step:2500, loss train:2.4070, loss val:2.4402\n",
      "step:2750, loss train:2.4200, loss val:2.4449\n",
      "step:3000, loss train:2.4055, loss val:2.4408\n",
      "step:3250, loss train:2.4211, loss val:2.4537\n",
      "step:3500, loss train:2.4026, loss val:2.4457\n",
      "step:3750, loss train:2.4174, loss val:2.4518\n",
      "step:4000, loss train:2.3967, loss val:2.4486\n",
      "step:4250, loss train:2.4085, loss val:2.4493\n",
      "step:4500, loss train:2.4065, loss val:2.4518\n",
      "step:4750, loss train:2.4091, loss val:2.4574\n",
      "step:5000, loss train:2.4096, loss val:2.4401\n",
      "step:5250, loss train:2.4162, loss val:2.4708\n",
      "step:5500, loss train:2.4397, loss val:2.4392\n",
      "step:5750, loss train:2.4130, loss val:2.4361\n",
      "step:6000, loss train:2.4182, loss val:2.4433\n",
      "step:6250, loss train:2.4118, loss val:2.4450\n",
      "step:6500, loss train:2.4250, loss val:2.4435\n",
      "step:6750, loss train:2.3989, loss val:2.4670\n",
      "step:7000, loss train:2.4322, loss val:2.4392\n",
      "step:7250, loss train:2.3916, loss val:2.4401\n",
      "step:7500, loss train:2.4083, loss val:2.4268\n",
      "step:7750, loss train:2.4039, loss val:2.4427\n",
      "step:8000, loss train:2.4135, loss val:2.4499\n",
      "step:8250, loss train:2.4081, loss val:2.4431\n",
      "step:8500, loss train:2.4155, loss val:2.4407\n",
      "step:8750, loss train:2.4117, loss val:2.4305\n",
      "step:9000, loss train:2.4282, loss val:2.4423\n",
      "step:9250, loss train:2.4260, loss val:2.4350\n",
      "step:9500, loss train:2.4082, loss val:2.4542\n",
      "step:9750, loss train:2.4084, loss val:2.4661\n",
      "batch:47\n",
      "step:0, loss train:2.4165, loss val:2.4387\n",
      "step:250, loss train:2.4044, loss val:2.4550\n",
      "step:500, loss train:2.4181, loss val:2.4455\n",
      "step:750, loss train:2.4021, loss val:2.4419\n",
      "step:1000, loss train:2.4161, loss val:2.4450\n",
      "step:1250, loss train:2.4069, loss val:2.4457\n",
      "step:1500, loss train:2.4170, loss val:2.4398\n",
      "step:1750, loss train:2.4315, loss val:2.4556\n",
      "step:2000, loss train:2.4335, loss val:2.4723\n",
      "step:2250, loss train:2.4120, loss val:2.4580\n",
      "step:2500, loss train:2.4132, loss val:2.4430\n",
      "step:2750, loss train:2.3898, loss val:2.4470\n",
      "step:3000, loss train:2.4169, loss val:2.4636\n",
      "step:3250, loss train:2.3946, loss val:2.4423\n",
      "step:3500, loss train:2.4177, loss val:2.4412\n",
      "step:3750, loss train:2.4134, loss val:2.4540\n",
      "step:4000, loss train:2.4135, loss val:2.4455\n",
      "step:4250, loss train:2.4102, loss val:2.4577\n",
      "step:4500, loss train:2.4093, loss val:2.4578\n",
      "step:4750, loss train:2.3976, loss val:2.4494\n",
      "step:5000, loss train:2.4253, loss val:2.4606\n",
      "step:5250, loss train:2.4252, loss val:2.4676\n",
      "step:5500, loss train:2.4140, loss val:2.4348\n",
      "step:5750, loss train:2.4165, loss val:2.4494\n",
      "step:6000, loss train:2.4102, loss val:2.4204\n",
      "step:6250, loss train:2.4037, loss val:2.4608\n",
      "step:6500, loss train:2.4267, loss val:2.4432\n",
      "step:6750, loss train:2.4390, loss val:2.4571\n",
      "step:7000, loss train:2.4097, loss val:2.4650\n",
      "step:7250, loss train:2.4104, loss val:2.4559\n",
      "step:7500, loss train:2.3892, loss val:2.4418\n",
      "step:7750, loss train:2.4139, loss val:2.4324\n",
      "step:8000, loss train:2.4085, loss val:2.4549\n",
      "step:8250, loss train:2.4052, loss val:2.4593\n",
      "step:8500, loss train:2.4269, loss val:2.4447\n",
      "step:8750, loss train:2.4082, loss val:2.4627\n",
      "step:9000, loss train:2.4259, loss val:2.4613\n",
      "step:9250, loss train:2.3964, loss val:2.4400\n",
      "step:9500, loss train:2.4134, loss val:2.4288\n",
      "step:9750, loss train:2.4094, loss val:2.4314\n",
      "batch:48\n",
      "step:0, loss train:2.3954, loss val:2.4072\n",
      "step:250, loss train:2.4283, loss val:2.4332\n",
      "step:500, loss train:2.4093, loss val:2.4579\n",
      "step:750, loss train:2.4117, loss val:2.4474\n",
      "step:1000, loss train:2.4113, loss val:2.4464\n",
      "step:1250, loss train:2.3961, loss val:2.4456\n",
      "step:1500, loss train:2.4082, loss val:2.4472\n",
      "step:1750, loss train:2.4062, loss val:2.4286\n",
      "step:2000, loss train:2.4172, loss val:2.4539\n",
      "step:2250, loss train:2.4329, loss val:2.4269\n",
      "step:2500, loss train:2.4147, loss val:2.4273\n",
      "step:2750, loss train:2.4058, loss val:2.4662\n",
      "step:3000, loss train:2.4020, loss val:2.4491\n",
      "step:3250, loss train:2.4061, loss val:2.4564\n",
      "step:3500, loss train:2.4180, loss val:2.4680\n",
      "step:3750, loss train:2.4150, loss val:2.4383\n",
      "step:4000, loss train:2.4003, loss val:2.4559\n",
      "step:4250, loss train:2.4216, loss val:2.4419\n",
      "step:4500, loss train:2.4237, loss val:2.4554\n",
      "step:4750, loss train:2.3957, loss val:2.4573\n",
      "step:5000, loss train:2.4154, loss val:2.4333\n",
      "step:5250, loss train:2.4059, loss val:2.4433\n",
      "step:5500, loss train:2.4218, loss val:2.4511\n",
      "step:5750, loss train:2.4245, loss val:2.4334\n",
      "step:6000, loss train:2.4393, loss val:2.4519\n",
      "step:6250, loss train:2.4227, loss val:2.4542\n",
      "step:6500, loss train:2.4123, loss val:2.4664\n",
      "step:6750, loss train:2.4139, loss val:2.4624\n",
      "step:7000, loss train:2.4067, loss val:2.4382\n",
      "step:7250, loss train:2.4333, loss val:2.4228\n",
      "step:7500, loss train:2.4141, loss val:2.4474\n",
      "step:7750, loss train:2.4035, loss val:2.4310\n",
      "step:8000, loss train:2.4125, loss val:2.4492\n",
      "step:8250, loss train:2.4291, loss val:2.4505\n",
      "step:8500, loss train:2.4141, loss val:2.4467\n",
      "step:8750, loss train:2.4231, loss val:2.4343\n",
      "step:9000, loss train:2.4089, loss val:2.4471\n",
      "step:9250, loss train:2.4214, loss val:2.4214\n",
      "step:9500, loss train:2.3990, loss val:2.4556\n",
      "step:9750, loss train:2.4008, loss val:2.4396\n",
      "batch:49\n",
      "step:0, loss train:2.4110, loss val:2.4475\n",
      "step:250, loss train:2.4041, loss val:2.4519\n",
      "step:500, loss train:2.3845, loss val:2.4465\n",
      "step:750, loss train:2.4127, loss val:2.4283\n",
      "step:1000, loss train:2.4238, loss val:2.4394\n",
      "step:1250, loss train:2.4405, loss val:2.4357\n",
      "step:1500, loss train:2.4057, loss val:2.4484\n",
      "step:1750, loss train:2.4104, loss val:2.4598\n",
      "step:2000, loss train:2.4189, loss val:2.4328\n",
      "step:2250, loss train:2.3986, loss val:2.4354\n",
      "step:2500, loss train:2.4151, loss val:2.4295\n",
      "step:2750, loss train:2.4336, loss val:2.4497\n",
      "step:3000, loss train:2.3904, loss val:2.4513\n",
      "step:3250, loss train:2.4071, loss val:2.4539\n",
      "step:3500, loss train:2.4140, loss val:2.4597\n",
      "step:3750, loss train:2.4184, loss val:2.4650\n",
      "step:4000, loss train:2.4201, loss val:2.4568\n",
      "step:4250, loss train:2.4076, loss val:2.4613\n",
      "step:4500, loss train:2.4072, loss val:2.4540\n",
      "step:4750, loss train:2.4099, loss val:2.4603\n",
      "step:5000, loss train:2.4122, loss val:2.4364\n",
      "step:5250, loss train:2.3994, loss val:2.4479\n",
      "step:5500, loss train:2.4205, loss val:2.4448\n",
      "step:5750, loss train:2.4059, loss val:2.4577\n",
      "step:6000, loss train:2.4160, loss val:2.4476\n",
      "step:6250, loss train:2.3935, loss val:2.4288\n",
      "step:6500, loss train:2.4109, loss val:2.4488\n",
      "step:6750, loss train:2.4079, loss val:2.4223\n",
      "step:7000, loss train:2.4231, loss val:2.4360\n",
      "step:7250, loss train:2.4288, loss val:2.4494\n",
      "step:7500, loss train:2.4217, loss val:2.4426\n",
      "step:7750, loss train:2.4258, loss val:2.4370\n",
      "step:8000, loss train:2.4173, loss val:2.4609\n",
      "step:8250, loss train:2.3963, loss val:2.4480\n",
      "step:8500, loss train:2.4254, loss val:2.4304\n",
      "step:8750, loss train:2.4028, loss val:2.4516\n",
      "step:9000, loss train:2.4155, loss val:2.4383\n",
      "step:9250, loss train:2.4128, loss val:2.4472\n",
      "step:9500, loss train:2.4050, loss val:2.4294\n",
      "step:9750, loss train:2.4221, loss val:2.4460\n",
      "batch:50\n",
      "step:0, loss train:2.4097, loss val:2.4424\n",
      "step:250, loss train:2.4162, loss val:2.4684\n",
      "step:500, loss train:2.4097, loss val:2.4548\n",
      "step:750, loss train:2.4118, loss val:2.4423\n",
      "step:1000, loss train:2.4176, loss val:2.4436\n",
      "step:1250, loss train:2.4258, loss val:2.4370\n",
      "step:1500, loss train:2.4106, loss val:2.4514\n",
      "step:1750, loss train:2.4309, loss val:2.4538\n",
      "step:2000, loss train:2.4114, loss val:2.4681\n",
      "step:2250, loss train:2.4217, loss val:2.4634\n",
      "step:2500, loss train:2.4347, loss val:2.4659\n",
      "step:2750, loss train:2.4243, loss val:2.4464\n",
      "step:3000, loss train:2.4031, loss val:2.4508\n",
      "step:3250, loss train:2.4146, loss val:2.4527\n",
      "step:3500, loss train:2.4142, loss val:2.4396\n",
      "step:3750, loss train:2.4294, loss val:2.4444\n",
      "step:4000, loss train:2.4301, loss val:2.4459\n",
      "step:4250, loss train:2.4071, loss val:2.4375\n",
      "step:4500, loss train:2.4187, loss val:2.4558\n",
      "step:4750, loss train:2.4177, loss val:2.4556\n",
      "step:5000, loss train:2.4125, loss val:2.4639\n",
      "step:5250, loss train:2.4341, loss val:2.4624\n",
      "step:5500, loss train:2.4055, loss val:2.4605\n",
      "step:5750, loss train:2.4139, loss val:2.4553\n",
      "step:6000, loss train:2.4091, loss val:2.4547\n",
      "step:6250, loss train:2.4171, loss val:2.4628\n",
      "step:6500, loss train:2.4146, loss val:2.4376\n",
      "step:6750, loss train:2.4056, loss val:2.4383\n",
      "step:7000, loss train:2.4209, loss val:2.4511\n",
      "step:7250, loss train:2.4063, loss val:2.4424\n",
      "step:7500, loss train:2.4233, loss val:2.4344\n",
      "step:7750, loss train:2.4354, loss val:2.4597\n",
      "step:8000, loss train:2.4180, loss val:2.4446\n",
      "step:8250, loss train:2.4226, loss val:2.4633\n",
      "step:8500, loss train:2.4207, loss val:2.4462\n",
      "step:8750, loss train:2.4281, loss val:2.4617\n",
      "step:9000, loss train:2.4312, loss val:2.4440\n",
      "step:9250, loss train:2.4180, loss val:2.4437\n",
      "step:9500, loss train:2.4250, loss val:2.4306\n",
      "step:9750, loss train:2.3993, loss val:2.4470\n",
      "batch:51\n",
      "step:0, loss train:2.4089, loss val:2.4371\n",
      "step:250, loss train:2.4104, loss val:2.4322\n",
      "step:500, loss train:2.4053, loss val:2.4220\n",
      "step:750, loss train:2.4195, loss val:2.4347\n",
      "step:1000, loss train:2.4047, loss val:2.4351\n",
      "step:1250, loss train:2.4213, loss val:2.4402\n",
      "step:1500, loss train:2.4137, loss val:2.4593\n",
      "step:1750, loss train:2.4203, loss val:2.4660\n",
      "step:2000, loss train:2.4104, loss val:2.4140\n",
      "step:2250, loss train:2.4133, loss val:2.4340\n",
      "step:2500, loss train:2.4059, loss val:2.4565\n",
      "step:2750, loss train:2.4077, loss val:2.4546\n",
      "step:3000, loss train:2.4061, loss val:2.4448\n",
      "step:3250, loss train:2.4216, loss val:2.4601\n",
      "step:3500, loss train:2.4080, loss val:2.4500\n",
      "step:3750, loss train:2.3973, loss val:2.4260\n",
      "step:4000, loss train:2.4156, loss val:2.4641\n",
      "step:4250, loss train:2.4160, loss val:2.4428\n",
      "step:4500, loss train:2.4202, loss val:2.4591\n",
      "step:4750, loss train:2.4124, loss val:2.4820\n",
      "step:5000, loss train:2.3885, loss val:2.4684\n",
      "step:5250, loss train:2.4070, loss val:2.4284\n",
      "step:5500, loss train:2.4159, loss val:2.4414\n",
      "step:5750, loss train:2.4301, loss val:2.4268\n",
      "step:6000, loss train:2.3985, loss val:2.4297\n",
      "step:6250, loss train:2.4210, loss val:2.4597\n",
      "step:6500, loss train:2.4156, loss val:2.4521\n",
      "step:6750, loss train:2.4125, loss val:2.4473\n",
      "step:7000, loss train:2.3952, loss val:2.4537\n",
      "step:7250, loss train:2.4063, loss val:2.4448\n",
      "step:7500, loss train:2.4107, loss val:2.4578\n",
      "step:7750, loss train:2.4073, loss val:2.4428\n",
      "step:8000, loss train:2.4105, loss val:2.4420\n",
      "step:8250, loss train:2.4164, loss val:2.4537\n",
      "step:8500, loss train:2.4145, loss val:2.4468\n",
      "step:8750, loss train:2.4097, loss val:2.4368\n",
      "step:9000, loss train:2.4145, loss val:2.4050\n",
      "step:9250, loss train:2.4144, loss val:2.4404\n",
      "step:9500, loss train:2.4219, loss val:2.4409\n",
      "step:9750, loss train:2.4237, loss val:2.4341\n",
      "batch:52\n",
      "step:0, loss train:2.4057, loss val:2.4427\n",
      "step:250, loss train:2.4326, loss val:2.4506\n",
      "step:500, loss train:2.4061, loss val:2.4573\n",
      "step:750, loss train:2.4079, loss val:2.4491\n",
      "step:1000, loss train:2.4148, loss val:2.4228\n",
      "step:1250, loss train:2.4166, loss val:2.4188\n",
      "step:1500, loss train:2.3962, loss val:2.4433\n",
      "step:1750, loss train:2.4278, loss val:2.4501\n",
      "step:2000, loss train:2.4231, loss val:2.4457\n",
      "step:2250, loss train:2.4185, loss val:2.4249\n",
      "step:2500, loss train:2.4255, loss val:2.4566\n",
      "step:2750, loss train:2.4181, loss val:2.4685\n",
      "step:3000, loss train:2.4122, loss val:2.4597\n",
      "step:3250, loss train:2.4154, loss val:2.4329\n",
      "step:3500, loss train:2.4178, loss val:2.4768\n",
      "step:3750, loss train:2.4122, loss val:2.4482\n",
      "step:4000, loss train:2.4011, loss val:2.4438\n",
      "step:4250, loss train:2.3941, loss val:2.4458\n",
      "step:4500, loss train:2.4055, loss val:2.4459\n",
      "step:4750, loss train:2.3856, loss val:2.4580\n",
      "step:5000, loss train:2.4159, loss val:2.4407\n",
      "step:5250, loss train:2.3824, loss val:2.4693\n",
      "step:5500, loss train:2.4084, loss val:2.4404\n",
      "step:5750, loss train:2.4111, loss val:2.4570\n",
      "step:6000, loss train:2.4318, loss val:2.4181\n",
      "step:6250, loss train:2.4180, loss val:2.4654\n",
      "step:6500, loss train:2.4002, loss val:2.4450\n",
      "step:6750, loss train:2.3999, loss val:2.4516\n",
      "step:7000, loss train:2.4200, loss val:2.4508\n",
      "step:7250, loss train:2.4014, loss val:2.4439\n",
      "step:7500, loss train:2.4229, loss val:2.4513\n",
      "step:7750, loss train:2.4140, loss val:2.4298\n",
      "step:8000, loss train:2.4213, loss val:2.4356\n",
      "step:8250, loss train:2.4041, loss val:2.4644\n",
      "step:8500, loss train:2.4167, loss val:2.4454\n",
      "step:8750, loss train:2.4068, loss val:2.4346\n",
      "step:9000, loss train:2.4124, loss val:2.4650\n",
      "step:9250, loss train:2.4076, loss val:2.4717\n",
      "step:9500, loss train:2.4225, loss val:2.4577\n",
      "step:9750, loss train:2.4153, loss val:2.4395\n",
      "batch:53\n",
      "step:0, loss train:2.4261, loss val:2.4440\n",
      "step:250, loss train:2.4011, loss val:2.4487\n",
      "step:500, loss train:2.4185, loss val:2.4493\n",
      "step:750, loss train:2.4219, loss val:2.4557\n",
      "step:1000, loss train:2.4135, loss val:2.4502\n",
      "step:1250, loss train:2.4279, loss val:2.4613\n",
      "step:1500, loss train:2.3943, loss val:2.4420\n",
      "step:1750, loss train:2.4029, loss val:2.4413\n",
      "step:2000, loss train:2.3917, loss val:2.4426\n",
      "step:2250, loss train:2.4077, loss val:2.4412\n",
      "step:2500, loss train:2.4114, loss val:2.4454\n",
      "step:2750, loss train:2.4133, loss val:2.4464\n",
      "step:3000, loss train:2.4299, loss val:2.4460\n",
      "step:3250, loss train:2.4086, loss val:2.4680\n",
      "step:3500, loss train:2.4038, loss val:2.4472\n",
      "step:3750, loss train:2.4063, loss val:2.4473\n",
      "step:4000, loss train:2.4029, loss val:2.4418\n",
      "step:4250, loss train:2.3867, loss val:2.4664\n",
      "step:4500, loss train:2.4020, loss val:2.4482\n",
      "step:4750, loss train:2.4169, loss val:2.4505\n",
      "step:5000, loss train:2.3908, loss val:2.4352\n",
      "step:5250, loss train:2.4165, loss val:2.4352\n",
      "step:5500, loss train:2.4066, loss val:2.4420\n",
      "step:5750, loss train:2.4209, loss val:2.4659\n",
      "step:6000, loss train:2.3896, loss val:2.4320\n",
      "step:6250, loss train:2.4313, loss val:2.4458\n",
      "step:6500, loss train:2.4113, loss val:2.4366\n",
      "step:6750, loss train:2.4261, loss val:2.4429\n",
      "step:7000, loss train:2.4216, loss val:2.4216\n",
      "step:7250, loss train:2.4023, loss val:2.4329\n",
      "step:7500, loss train:2.3930, loss val:2.4424\n",
      "step:7750, loss train:2.4272, loss val:2.4311\n",
      "step:8000, loss train:2.4114, loss val:2.4460\n",
      "step:8250, loss train:2.4109, loss val:2.4525\n",
      "step:8500, loss train:2.4127, loss val:2.4512\n",
      "step:8750, loss train:2.4271, loss val:2.4390\n",
      "step:9000, loss train:2.4042, loss val:2.4474\n",
      "step:9250, loss train:2.4188, loss val:2.4351\n",
      "step:9500, loss train:2.4110, loss val:2.4481\n",
      "step:9750, loss train:2.3939, loss val:2.4477\n",
      "batch:54\n",
      "step:0, loss train:2.4203, loss val:2.4704\n",
      "step:250, loss train:2.4076, loss val:2.4178\n",
      "step:500, loss train:2.4097, loss val:2.4218\n",
      "step:750, loss train:2.4256, loss val:2.4598\n",
      "step:1000, loss train:2.4120, loss val:2.4587\n",
      "step:1250, loss train:2.4042, loss val:2.4423\n",
      "step:1500, loss train:2.4107, loss val:2.4333\n",
      "step:1750, loss train:2.4116, loss val:2.4563\n",
      "step:2000, loss train:2.4037, loss val:2.4467\n",
      "step:2250, loss train:2.4334, loss val:2.4430\n",
      "step:2500, loss train:2.4082, loss val:2.4771\n",
      "step:2750, loss train:2.4056, loss val:2.4333\n",
      "step:3000, loss train:2.4143, loss val:2.4480\n",
      "step:3250, loss train:2.4198, loss val:2.4497\n",
      "step:3500, loss train:2.4031, loss val:2.4528\n",
      "step:3750, loss train:2.4224, loss val:2.4473\n",
      "step:4000, loss train:2.4139, loss val:2.4515\n",
      "step:4250, loss train:2.3994, loss val:2.4453\n",
      "step:4500, loss train:2.4148, loss val:2.4401\n",
      "step:4750, loss train:2.4194, loss val:2.4438\n",
      "step:5000, loss train:2.4089, loss val:2.4601\n",
      "step:5250, loss train:2.4089, loss val:2.4270\n",
      "step:5500, loss train:2.4024, loss val:2.4438\n",
      "step:5750, loss train:2.4164, loss val:2.4207\n",
      "step:6000, loss train:2.4122, loss val:2.4218\n",
      "step:6250, loss train:2.4065, loss val:2.4283\n",
      "step:6500, loss train:2.3946, loss val:2.4674\n",
      "step:6750, loss train:2.4302, loss val:2.4423\n",
      "step:7000, loss train:2.4230, loss val:2.4559\n",
      "step:7250, loss train:2.4086, loss val:2.4328\n",
      "step:7500, loss train:2.4208, loss val:2.4498\n",
      "step:7750, loss train:2.4243, loss val:2.4432\n",
      "step:8000, loss train:2.4088, loss val:2.4492\n",
      "step:8250, loss train:2.4038, loss val:2.4388\n",
      "step:8500, loss train:2.3911, loss val:2.4615\n",
      "step:8750, loss train:2.4151, loss val:2.4395\n",
      "step:9000, loss train:2.4115, loss val:2.4441\n",
      "step:9250, loss train:2.4196, loss val:2.4733\n",
      "step:9500, loss train:2.4121, loss val:2.4456\n",
      "step:9750, loss train:2.4021, loss val:2.4488\n",
      "batch:55\n",
      "step:0, loss train:2.4076, loss val:2.4596\n",
      "step:250, loss train:2.4147, loss val:2.4349\n",
      "step:500, loss train:2.4026, loss val:2.4384\n",
      "step:750, loss train:2.4084, loss val:2.4479\n",
      "step:1000, loss train:2.4043, loss val:2.4570\n",
      "step:1250, loss train:2.4519, loss val:2.4455\n",
      "step:1500, loss train:2.4209, loss val:2.4524\n",
      "step:1750, loss train:2.4011, loss val:2.4501\n",
      "step:2000, loss train:2.4242, loss val:2.4505\n",
      "step:2250, loss train:2.3962, loss val:2.4312\n",
      "step:2500, loss train:2.4248, loss val:2.4466\n",
      "step:2750, loss train:2.4145, loss val:2.4331\n",
      "step:3000, loss train:2.4311, loss val:2.4226\n",
      "step:3250, loss train:2.4158, loss val:2.4394\n",
      "step:3500, loss train:2.4180, loss val:2.4496\n",
      "step:3750, loss train:2.4000, loss val:2.4386\n",
      "step:4000, loss train:2.4098, loss val:2.4318\n",
      "step:4250, loss train:2.4142, loss val:2.4699\n",
      "step:4500, loss train:2.3934, loss val:2.4417\n",
      "step:4750, loss train:2.4176, loss val:2.4463\n",
      "step:5000, loss train:2.3894, loss val:2.4805\n",
      "step:5250, loss train:2.4086, loss val:2.4418\n",
      "step:5500, loss train:2.4079, loss val:2.4490\n",
      "step:5750, loss train:2.4016, loss val:2.4313\n",
      "step:6000, loss train:2.4239, loss val:2.4724\n",
      "step:6250, loss train:2.4044, loss val:2.4446\n",
      "step:6500, loss train:2.3770, loss val:2.4608\n",
      "step:6750, loss train:2.4114, loss val:2.4380\n",
      "step:7000, loss train:2.4133, loss val:2.4529\n",
      "step:7250, loss train:2.4050, loss val:2.4312\n",
      "step:7500, loss train:2.4238, loss val:2.4362\n",
      "step:7750, loss train:2.3934, loss val:2.4390\n",
      "step:8000, loss train:2.4036, loss val:2.4466\n",
      "step:8250, loss train:2.4169, loss val:2.4334\n",
      "step:8500, loss train:2.4221, loss val:2.4626\n",
      "step:8750, loss train:2.4278, loss val:2.4478\n",
      "step:9000, loss train:2.4093, loss val:2.4380\n",
      "step:9250, loss train:2.4128, loss val:2.4261\n",
      "step:9500, loss train:2.3932, loss val:2.4648\n",
      "step:9750, loss train:2.4086, loss val:2.4478\n",
      "batch:56\n",
      "step:0, loss train:2.4097, loss val:2.4522\n",
      "step:250, loss train:2.4224, loss val:2.4375\n",
      "step:500, loss train:2.4045, loss val:2.4489\n",
      "step:750, loss train:2.4080, loss val:2.4472\n",
      "step:1000, loss train:2.4097, loss val:2.4508\n",
      "step:1250, loss train:2.4022, loss val:2.4659\n",
      "step:1500, loss train:2.4035, loss val:2.4681\n",
      "step:1750, loss train:2.4086, loss val:2.4304\n",
      "step:2000, loss train:2.4102, loss val:2.4466\n",
      "step:2250, loss train:2.4312, loss val:2.4437\n",
      "step:2500, loss train:2.4182, loss val:2.4373\n",
      "step:2750, loss train:2.4096, loss val:2.4406\n",
      "step:3000, loss train:2.4048, loss val:2.4566\n",
      "step:3250, loss train:2.4119, loss val:2.4539\n",
      "step:3500, loss train:2.4059, loss val:2.4654\n",
      "step:3750, loss train:2.4261, loss val:2.4307\n",
      "step:4000, loss train:2.4362, loss val:2.4565\n",
      "step:4250, loss train:2.3993, loss val:2.4252\n",
      "step:4500, loss train:2.4273, loss val:2.4498\n",
      "step:4750, loss train:2.4232, loss val:2.4509\n",
      "step:5000, loss train:2.3928, loss val:2.4345\n",
      "step:5250, loss train:2.4062, loss val:2.4503\n",
      "step:5500, loss train:2.4307, loss val:2.4511\n",
      "step:5750, loss train:2.4159, loss val:2.4581\n",
      "step:6000, loss train:2.4131, loss val:2.4480\n",
      "step:6250, loss train:2.4243, loss val:2.4427\n",
      "step:6500, loss train:2.4115, loss val:2.4548\n",
      "step:6750, loss train:2.4176, loss val:2.4315\n",
      "step:7000, loss train:2.4035, loss val:2.4565\n",
      "step:7250, loss train:2.4266, loss val:2.4239\n",
      "step:7500, loss train:2.4164, loss val:2.4433\n",
      "step:7750, loss train:2.4239, loss val:2.4453\n",
      "step:8000, loss train:2.4241, loss val:2.4388\n",
      "step:8250, loss train:2.4129, loss val:2.4395\n",
      "step:8500, loss train:2.4097, loss val:2.4519\n",
      "step:8750, loss train:2.3916, loss val:2.4517\n",
      "step:9000, loss train:2.3931, loss val:2.4722\n",
      "step:9250, loss train:2.3929, loss val:2.4671\n",
      "step:9500, loss train:2.4265, loss val:2.4460\n",
      "step:9750, loss train:2.3922, loss val:2.4479\n",
      "batch:57\n",
      "step:0, loss train:2.4132, loss val:2.4632\n",
      "step:250, loss train:2.4185, loss val:2.4538\n",
      "step:500, loss train:2.4101, loss val:2.4412\n",
      "step:750, loss train:2.4238, loss val:2.4309\n",
      "step:1000, loss train:2.4030, loss val:2.4167\n",
      "step:1250, loss train:2.4289, loss val:2.4503\n",
      "step:1500, loss train:2.4133, loss val:2.4585\n",
      "step:1750, loss train:2.4117, loss val:2.4546\n",
      "step:2000, loss train:2.4013, loss val:2.4535\n",
      "step:2250, loss train:2.4069, loss val:2.4514\n",
      "step:2500, loss train:2.4088, loss val:2.4492\n",
      "step:2750, loss train:2.4080, loss val:2.4315\n",
      "step:3000, loss train:2.4216, loss val:2.4379\n",
      "step:3250, loss train:2.4165, loss val:2.4407\n",
      "step:3500, loss train:2.3923, loss val:2.4521\n",
      "step:3750, loss train:2.4245, loss val:2.4643\n",
      "step:4000, loss train:2.4153, loss val:2.4613\n",
      "step:4250, loss train:2.4146, loss val:2.4362\n",
      "step:4500, loss train:2.4253, loss val:2.4474\n",
      "step:4750, loss train:2.4288, loss val:2.4745\n",
      "step:5000, loss train:2.4214, loss val:2.4421\n",
      "step:5250, loss train:2.4189, loss val:2.4480\n",
      "step:5500, loss train:2.4088, loss val:2.4294\n",
      "step:5750, loss train:2.4066, loss val:2.4406\n",
      "step:6000, loss train:2.4193, loss val:2.4619\n",
      "step:6250, loss train:2.4067, loss val:2.4440\n",
      "step:6500, loss train:2.3957, loss val:2.4395\n",
      "step:6750, loss train:2.4131, loss val:2.4415\n",
      "step:7000, loss train:2.4111, loss val:2.4233\n",
      "step:7250, loss train:2.4049, loss val:2.4448\n",
      "step:7500, loss train:2.4291, loss val:2.4348\n",
      "step:7750, loss train:2.4252, loss val:2.4293\n",
      "step:8000, loss train:2.4008, loss val:2.4648\n",
      "step:8250, loss train:2.4182, loss val:2.4624\n",
      "step:8500, loss train:2.4128, loss val:2.4492\n",
      "step:8750, loss train:2.4213, loss val:2.4315\n",
      "step:9000, loss train:2.4183, loss val:2.4630\n",
      "step:9250, loss train:2.4105, loss val:2.4472\n",
      "step:9500, loss train:2.4239, loss val:2.4534\n",
      "step:9750, loss train:2.4239, loss val:2.4351\n",
      "batch:58\n",
      "step:0, loss train:2.4028, loss val:2.4329\n",
      "step:250, loss train:2.3919, loss val:2.4333\n",
      "step:500, loss train:2.4254, loss val:2.4470\n",
      "step:750, loss train:2.4078, loss val:2.4424\n",
      "step:1000, loss train:2.4024, loss val:2.4421\n",
      "step:1250, loss train:2.4252, loss val:2.4421\n",
      "step:1500, loss train:2.4217, loss val:2.4280\n",
      "step:1750, loss train:2.4067, loss val:2.4370\n",
      "step:2000, loss train:2.4323, loss val:2.4262\n",
      "step:2250, loss train:2.4073, loss val:2.4228\n",
      "step:2500, loss train:2.4255, loss val:2.4233\n",
      "step:2750, loss train:2.4089, loss val:2.4345\n",
      "step:3000, loss train:2.4003, loss val:2.4359\n",
      "step:3250, loss train:2.3984, loss val:2.4525\n",
      "step:3500, loss train:2.4425, loss val:2.4411\n",
      "step:3750, loss train:2.4180, loss val:2.4525\n",
      "step:4000, loss train:2.4219, loss val:2.4640\n",
      "step:4250, loss train:2.4108, loss val:2.4549\n",
      "step:4500, loss train:2.4045, loss val:2.4543\n",
      "step:4750, loss train:2.4081, loss val:2.4591\n",
      "step:5000, loss train:2.4107, loss val:2.4686\n",
      "step:5250, loss train:2.4086, loss val:2.4370\n",
      "step:5500, loss train:2.4304, loss val:2.4464\n",
      "step:5750, loss train:2.4109, loss val:2.4349\n",
      "step:6000, loss train:2.4075, loss val:2.4483\n",
      "step:6250, loss train:2.4117, loss val:2.4494\n",
      "step:6500, loss train:2.4066, loss val:2.4679\n",
      "step:6750, loss train:2.4244, loss val:2.4547\n",
      "step:7000, loss train:2.4178, loss val:2.4537\n",
      "step:7250, loss train:2.4032, loss val:2.4549\n",
      "step:7500, loss train:2.4058, loss val:2.4506\n",
      "step:7750, loss train:2.3961, loss val:2.4643\n",
      "step:8000, loss train:2.4103, loss val:2.4421\n",
      "step:8250, loss train:2.4033, loss val:2.4323\n",
      "step:8500, loss train:2.4099, loss val:2.4117\n",
      "step:8750, loss train:2.4044, loss val:2.4514\n",
      "step:9000, loss train:2.4297, loss val:2.4226\n",
      "step:9250, loss train:2.4001, loss val:2.4403\n",
      "step:9500, loss train:2.4214, loss val:2.4311\n",
      "step:9750, loss train:2.3972, loss val:2.4735\n",
      "batch:59\n",
      "step:0, loss train:2.4162, loss val:2.4560\n",
      "step:250, loss train:2.4143, loss val:2.4327\n",
      "step:500, loss train:2.4049, loss val:2.4482\n",
      "step:750, loss train:2.4027, loss val:2.4387\n",
      "step:1000, loss train:2.4100, loss val:2.4489\n",
      "step:1250, loss train:2.4218, loss val:2.4516\n",
      "step:1500, loss train:2.4009, loss val:2.4439\n",
      "step:1750, loss train:2.4055, loss val:2.4392\n",
      "step:2000, loss train:2.4150, loss val:2.4450\n",
      "step:2250, loss train:2.4086, loss val:2.4381\n",
      "step:2500, loss train:2.4101, loss val:2.4512\n",
      "step:2750, loss train:2.4179, loss val:2.4662\n",
      "step:3000, loss train:2.4201, loss val:2.4569\n",
      "step:3250, loss train:2.4025, loss val:2.4492\n",
      "step:3500, loss train:2.4139, loss val:2.4504\n",
      "step:3750, loss train:2.4200, loss val:2.4447\n",
      "step:4000, loss train:2.4194, loss val:2.4450\n",
      "step:4250, loss train:2.4108, loss val:2.4581\n",
      "step:4500, loss train:2.4063, loss val:2.4364\n",
      "step:4750, loss train:2.3941, loss val:2.4556\n",
      "step:5000, loss train:2.4211, loss val:2.4495\n",
      "step:5250, loss train:2.4028, loss val:2.4487\n",
      "step:5500, loss train:2.4008, loss val:2.4313\n",
      "step:5750, loss train:2.4043, loss val:2.4600\n",
      "step:6000, loss train:2.4178, loss val:2.4349\n",
      "step:6250, loss train:2.3897, loss val:2.4219\n",
      "step:6500, loss train:2.4124, loss val:2.4513\n",
      "step:6750, loss train:2.4356, loss val:2.4629\n",
      "step:7000, loss train:2.4144, loss val:2.4512\n",
      "step:7250, loss train:2.4159, loss val:2.4842\n",
      "step:7500, loss train:2.4195, loss val:2.4421\n",
      "step:7750, loss train:2.4223, loss val:2.4481\n",
      "step:8000, loss train:2.4006, loss val:2.4309\n",
      "step:8250, loss train:2.4063, loss val:2.4658\n",
      "step:8500, loss train:2.4220, loss val:2.4425\n",
      "step:8750, loss train:2.4241, loss val:2.4535\n",
      "step:9000, loss train:2.4133, loss val:2.4417\n",
      "step:9250, loss train:2.4165, loss val:2.4625\n",
      "step:9500, loss train:2.4008, loss val:2.4413\n",
      "step:9750, loss train:2.4258, loss val:2.4584\n",
      "batch:60\n",
      "step:0, loss train:2.4324, loss val:2.4434\n",
      "step:250, loss train:2.3973, loss val:2.4364\n",
      "step:500, loss train:2.4095, loss val:2.4474\n",
      "step:750, loss train:2.3886, loss val:2.4461\n",
      "step:1000, loss train:2.4278, loss val:2.4549\n",
      "step:1250, loss train:2.4000, loss val:2.4373\n",
      "step:1500, loss train:2.4140, loss val:2.4526\n",
      "step:1750, loss train:2.4103, loss val:2.4277\n",
      "step:2000, loss train:2.4169, loss val:2.4499\n",
      "step:2250, loss train:2.3903, loss val:2.4597\n",
      "step:2500, loss train:2.3962, loss val:2.4487\n",
      "step:2750, loss train:2.4281, loss val:2.4356\n",
      "step:3000, loss train:2.4152, loss val:2.4195\n",
      "step:3250, loss train:2.4187, loss val:2.4547\n",
      "step:3500, loss train:2.4071, loss val:2.4354\n",
      "step:3750, loss train:2.4091, loss val:2.4712\n",
      "step:4000, loss train:2.4086, loss val:2.4543\n",
      "step:4250, loss train:2.4129, loss val:2.4617\n",
      "step:4500, loss train:2.4118, loss val:2.4403\n",
      "step:4750, loss train:2.3935, loss val:2.4283\n",
      "step:5000, loss train:2.4200, loss val:2.4668\n",
      "step:5250, loss train:2.4194, loss val:2.4548\n",
      "step:5500, loss train:2.3986, loss val:2.4394\n",
      "step:5750, loss train:2.4328, loss val:2.4448\n",
      "step:6000, loss train:2.4387, loss val:2.4204\n",
      "step:6250, loss train:2.4112, loss val:2.4411\n",
      "step:6500, loss train:2.4149, loss val:2.4284\n",
      "step:6750, loss train:2.3973, loss val:2.4412\n",
      "step:7000, loss train:2.4205, loss val:2.4609\n",
      "step:7250, loss train:2.4335, loss val:2.4478\n",
      "step:7500, loss train:2.4032, loss val:2.4214\n",
      "step:7750, loss train:2.4052, loss val:2.4599\n",
      "step:8000, loss train:2.3947, loss val:2.4220\n",
      "step:8250, loss train:2.4192, loss val:2.4367\n",
      "step:8500, loss train:2.4283, loss val:2.4357\n",
      "step:8750, loss train:2.4101, loss val:2.4364\n",
      "step:9000, loss train:2.4139, loss val:2.4310\n",
      "step:9250, loss train:2.4039, loss val:2.4452\n",
      "step:9500, loss train:2.4115, loss val:2.4454\n",
      "step:9750, loss train:2.4183, loss val:2.4336\n",
      "batch:61\n",
      "step:0, loss train:2.4319, loss val:2.4317\n",
      "step:250, loss train:2.4222, loss val:2.4578\n",
      "step:500, loss train:2.4199, loss val:2.4404\n",
      "step:750, loss train:2.4035, loss val:2.4629\n",
      "step:1000, loss train:2.4155, loss val:2.4473\n",
      "step:1250, loss train:2.4204, loss val:2.4389\n",
      "step:1500, loss train:2.4057, loss val:2.4367\n",
      "step:1750, loss train:2.4023, loss val:2.4350\n",
      "step:2000, loss train:2.3958, loss val:2.4311\n",
      "step:2250, loss train:2.4127, loss val:2.4650\n",
      "step:2500, loss train:2.4208, loss val:2.4482\n",
      "step:2750, loss train:2.4140, loss val:2.4318\n",
      "step:3000, loss train:2.4105, loss val:2.4564\n",
      "step:3250, loss train:2.4065, loss val:2.4718\n",
      "step:3500, loss train:2.4104, loss val:2.4462\n",
      "step:3750, loss train:2.4256, loss val:2.4267\n",
      "step:4000, loss train:2.3863, loss val:2.4571\n",
      "step:4250, loss train:2.4271, loss val:2.4501\n",
      "step:4500, loss train:2.4147, loss val:2.4555\n",
      "step:4750, loss train:2.4001, loss val:2.4600\n",
      "step:5000, loss train:2.4230, loss val:2.4543\n",
      "step:5250, loss train:2.4231, loss val:2.4544\n",
      "step:5500, loss train:2.4157, loss val:2.4184\n",
      "step:5750, loss train:2.4213, loss val:2.4479\n",
      "step:6000, loss train:2.4092, loss val:2.4545\n",
      "step:6250, loss train:2.4186, loss val:2.4195\n",
      "step:6500, loss train:2.4055, loss val:2.4533\n",
      "step:6750, loss train:2.4079, loss val:2.4482\n",
      "step:7000, loss train:2.3960, loss val:2.4430\n",
      "step:7250, loss train:2.4370, loss val:2.4335\n",
      "step:7500, loss train:2.4201, loss val:2.4393\n",
      "step:7750, loss train:2.4083, loss val:2.4519\n",
      "step:8000, loss train:2.4139, loss val:2.4650\n",
      "step:8250, loss train:2.4218, loss val:2.4360\n",
      "step:8500, loss train:2.4281, loss val:2.4565\n",
      "step:8750, loss train:2.4101, loss val:2.4386\n",
      "step:9000, loss train:2.4025, loss val:2.4463\n",
      "step:9250, loss train:2.4161, loss val:2.4287\n",
      "step:9500, loss train:2.4078, loss val:2.4393\n",
      "step:9750, loss train:2.4148, loss val:2.4311\n",
      "batch:62\n",
      "step:0, loss train:2.4193, loss val:2.4518\n",
      "step:250, loss train:2.4150, loss val:2.4557\n",
      "step:500, loss train:2.4256, loss val:2.4611\n",
      "step:750, loss train:2.4109, loss val:2.4476\n",
      "step:1000, loss train:2.3816, loss val:2.4504\n",
      "step:1250, loss train:2.3957, loss val:2.4417\n",
      "step:1500, loss train:2.4143, loss val:2.4362\n",
      "step:1750, loss train:2.3923, loss val:2.4461\n",
      "step:2000, loss train:2.4218, loss val:2.4348\n",
      "step:2250, loss train:2.4082, loss val:2.4418\n",
      "step:2500, loss train:2.4275, loss val:2.4504\n",
      "step:2750, loss train:2.4101, loss val:2.4316\n",
      "step:3000, loss train:2.4164, loss val:2.4574\n",
      "step:3250, loss train:2.4195, loss val:2.4362\n",
      "step:3500, loss train:2.4261, loss val:2.4748\n",
      "step:3750, loss train:2.4049, loss val:2.4469\n",
      "step:4000, loss train:2.4199, loss val:2.4444\n",
      "step:4250, loss train:2.4102, loss val:2.4387\n",
      "step:4500, loss train:2.4169, loss val:2.4470\n",
      "step:4750, loss train:2.4090, loss val:2.4526\n",
      "step:5000, loss train:2.4092, loss val:2.4316\n",
      "step:5250, loss train:2.4135, loss val:2.4362\n",
      "step:5500, loss train:2.4123, loss val:2.4655\n",
      "step:5750, loss train:2.4081, loss val:2.4563\n",
      "step:6000, loss train:2.4143, loss val:2.4498\n",
      "step:6250, loss train:2.4143, loss val:2.4507\n",
      "step:6500, loss train:2.4097, loss val:2.4354\n",
      "step:6750, loss train:2.4172, loss val:2.4431\n",
      "step:7000, loss train:2.4045, loss val:2.4682\n",
      "step:7250, loss train:2.4064, loss val:2.4266\n",
      "step:7500, loss train:2.4035, loss val:2.4500\n",
      "step:7750, loss train:2.4056, loss val:2.4500\n",
      "step:8000, loss train:2.4195, loss val:2.4425\n",
      "step:8250, loss train:2.4276, loss val:2.4544\n",
      "step:8500, loss train:2.4055, loss val:2.4520\n",
      "step:8750, loss train:2.4103, loss val:2.4453\n",
      "step:9000, loss train:2.4157, loss val:2.4472\n",
      "step:9250, loss train:2.4092, loss val:2.4308\n",
      "step:9500, loss train:2.3965, loss val:2.4376\n",
      "step:9750, loss train:2.4135, loss val:2.4429\n",
      "batch:63\n",
      "step:0, loss train:2.4100, loss val:2.4361\n",
      "step:250, loss train:2.3985, loss val:2.4559\n",
      "step:500, loss train:2.4263, loss val:2.4456\n",
      "step:750, loss train:2.4053, loss val:2.4420\n",
      "step:1000, loss train:2.4078, loss val:2.4186\n",
      "step:1250, loss train:2.4104, loss val:2.4569\n",
      "step:1500, loss train:2.4149, loss val:2.4530\n",
      "step:1750, loss train:2.4025, loss val:2.4451\n",
      "step:2000, loss train:2.4108, loss val:2.4465\n",
      "step:2250, loss train:2.4218, loss val:2.4497\n",
      "step:2500, loss train:2.3982, loss val:2.4498\n",
      "step:2750, loss train:2.4273, loss val:2.4409\n",
      "step:3000, loss train:2.4091, loss val:2.4447\n",
      "step:3250, loss train:2.3925, loss val:2.4531\n",
      "step:3500, loss train:2.4227, loss val:2.4569\n",
      "step:3750, loss train:2.3986, loss val:2.4383\n",
      "step:4000, loss train:2.4218, loss val:2.4215\n",
      "step:4250, loss train:2.4026, loss val:2.4547\n",
      "step:4500, loss train:2.4192, loss val:2.4531\n",
      "step:4750, loss train:2.3997, loss val:2.4269\n",
      "step:5000, loss train:2.4039, loss val:2.4420\n",
      "step:5250, loss train:2.4297, loss val:2.4641\n",
      "step:5500, loss train:2.4154, loss val:2.4555\n",
      "step:5750, loss train:2.4096, loss val:2.4296\n",
      "step:6000, loss train:2.4315, loss val:2.4488\n",
      "step:6250, loss train:2.4144, loss val:2.4616\n",
      "step:6500, loss train:2.4202, loss val:2.4412\n",
      "step:6750, loss train:2.4144, loss val:2.4327\n",
      "step:7000, loss train:2.4124, loss val:2.4503\n",
      "step:7250, loss train:2.4082, loss val:2.4453\n",
      "step:7500, loss train:2.4013, loss val:2.4414\n",
      "step:7750, loss train:2.4029, loss val:2.4472\n",
      "step:8000, loss train:2.4005, loss val:2.4706\n",
      "step:8250, loss train:2.4146, loss val:2.4280\n",
      "step:8500, loss train:2.4143, loss val:2.4405\n",
      "step:8750, loss train:2.4255, loss val:2.4403\n",
      "step:9000, loss train:2.4121, loss val:2.4766\n",
      "step:9250, loss train:2.4100, loss val:2.4694\n",
      "step:9500, loss train:2.4085, loss val:2.4491\n",
      "step:9750, loss train:2.4067, loss val:2.4299\n",
      "batch:64\n",
      "step:0, loss train:2.4208, loss val:2.4533\n",
      "step:250, loss train:2.4162, loss val:2.4498\n",
      "step:500, loss train:2.3999, loss val:2.4289\n",
      "step:750, loss train:2.4152, loss val:2.4670\n",
      "step:1000, loss train:2.4122, loss val:2.4373\n",
      "step:1250, loss train:2.4140, loss val:2.4497\n",
      "step:1500, loss train:2.3950, loss val:2.4497\n",
      "step:1750, loss train:2.4295, loss val:2.4554\n",
      "step:2000, loss train:2.4290, loss val:2.4827\n",
      "step:2250, loss train:2.4184, loss val:2.4615\n",
      "step:2500, loss train:2.4181, loss val:2.4480\n",
      "step:2750, loss train:2.3997, loss val:2.4548\n",
      "step:3000, loss train:2.3884, loss val:2.4401\n",
      "step:3250, loss train:2.4037, loss val:2.4571\n",
      "step:3500, loss train:2.4070, loss val:2.4371\n",
      "step:3750, loss train:2.4129, loss val:2.4382\n",
      "step:4000, loss train:2.4147, loss val:2.4485\n",
      "step:4250, loss train:2.4075, loss val:2.4650\n",
      "step:4500, loss train:2.4168, loss val:2.4290\n",
      "step:4750, loss train:2.4040, loss val:2.4523\n",
      "step:5000, loss train:2.4153, loss val:2.4504\n",
      "step:5250, loss train:2.4012, loss val:2.4550\n",
      "step:5500, loss train:2.4252, loss val:2.4443\n",
      "step:5750, loss train:2.4287, loss val:2.4269\n",
      "step:6000, loss train:2.4164, loss val:2.4594\n",
      "step:6250, loss train:2.4156, loss val:2.4290\n",
      "step:6500, loss train:2.4124, loss val:2.4286\n",
      "step:6750, loss train:2.3903, loss val:2.4704\n",
      "step:7000, loss train:2.4109, loss val:2.4574\n",
      "step:7250, loss train:2.4238, loss val:2.4450\n",
      "step:7500, loss train:2.4024, loss val:2.4385\n",
      "step:7750, loss train:2.4053, loss val:2.4480\n",
      "step:8000, loss train:2.4235, loss val:2.4259\n",
      "step:8250, loss train:2.4064, loss val:2.4548\n",
      "step:8500, loss train:2.4089, loss val:2.4553\n",
      "step:8750, loss train:2.4055, loss val:2.4512\n",
      "step:9000, loss train:2.4082, loss val:2.4427\n",
      "step:9250, loss train:2.4147, loss val:2.4434\n",
      "step:9500, loss train:2.4184, loss val:2.4555\n",
      "step:9750, loss train:2.4018, loss val:2.4583\n",
      "batch:65\n",
      "step:0, loss train:2.4279, loss val:2.4426\n",
      "step:250, loss train:2.4018, loss val:2.4546\n",
      "step:500, loss train:2.3989, loss val:2.4588\n",
      "step:750, loss train:2.4193, loss val:2.4315\n",
      "step:1000, loss train:2.4035, loss val:2.4484\n",
      "step:1250, loss train:2.4026, loss val:2.4345\n",
      "step:1500, loss train:2.4160, loss val:2.4391\n",
      "step:1750, loss train:2.4009, loss val:2.4523\n",
      "step:2000, loss train:2.4134, loss val:2.4424\n",
      "step:2250, loss train:2.4116, loss val:2.4122\n",
      "step:2500, loss train:2.4354, loss val:2.4541\n",
      "step:2750, loss train:2.4080, loss val:2.4479\n",
      "step:3000, loss train:2.4126, loss val:2.4338\n",
      "step:3250, loss train:2.4126, loss val:2.4458\n",
      "step:3500, loss train:2.4118, loss val:2.4602\n",
      "step:3750, loss train:2.4211, loss val:2.4550\n",
      "step:4000, loss train:2.4160, loss val:2.4510\n",
      "step:4250, loss train:2.4010, loss val:2.4371\n",
      "step:4500, loss train:2.3962, loss val:2.4461\n",
      "step:4750, loss train:2.4267, loss val:2.4418\n",
      "step:5000, loss train:2.4085, loss val:2.4611\n",
      "step:5250, loss train:2.3958, loss val:2.4329\n",
      "step:5500, loss train:2.3956, loss val:2.4660\n",
      "step:5750, loss train:2.4272, loss val:2.4502\n",
      "step:6000, loss train:2.4037, loss val:2.4675\n",
      "step:6250, loss train:2.4036, loss val:2.4327\n",
      "step:6500, loss train:2.4153, loss val:2.4448\n",
      "step:6750, loss train:2.4073, loss val:2.4499\n",
      "step:7000, loss train:2.4171, loss val:2.4511\n",
      "step:7250, loss train:2.3980, loss val:2.4485\n",
      "step:7500, loss train:2.4085, loss val:2.4553\n",
      "step:7750, loss train:2.3946, loss val:2.4699\n",
      "step:8000, loss train:2.4294, loss val:2.4429\n",
      "step:8250, loss train:2.3947, loss val:2.4384\n",
      "step:8500, loss train:2.4137, loss val:2.4550\n",
      "step:8750, loss train:2.3973, loss val:2.4313\n",
      "step:9000, loss train:2.4201, loss val:2.4337\n",
      "step:9250, loss train:2.4168, loss val:2.4365\n",
      "step:9500, loss train:2.4102, loss val:2.4371\n",
      "step:9750, loss train:2.4087, loss val:2.4484\n",
      "batch:66\n",
      "step:0, loss train:2.4166, loss val:2.4155\n",
      "step:250, loss train:2.4001, loss val:2.4522\n",
      "step:500, loss train:2.4142, loss val:2.4530\n",
      "step:750, loss train:2.4078, loss val:2.4278\n",
      "step:1000, loss train:2.4187, loss val:2.4517\n",
      "step:1250, loss train:2.4117, loss val:2.4350\n",
      "step:1500, loss train:2.4203, loss val:2.4484\n",
      "step:1750, loss train:2.4107, loss val:2.4126\n",
      "step:2000, loss train:2.4287, loss val:2.4339\n",
      "step:2250, loss train:2.4198, loss val:2.4523\n",
      "step:2500, loss train:2.4164, loss val:2.4690\n",
      "step:2750, loss train:2.4113, loss val:2.4432\n",
      "step:3000, loss train:2.4136, loss val:2.4431\n",
      "step:3250, loss train:2.4161, loss val:2.4332\n",
      "step:3500, loss train:2.4238, loss val:2.4418\n",
      "step:3750, loss train:2.4163, loss val:2.4429\n",
      "step:4000, loss train:2.4080, loss val:2.4608\n",
      "step:4250, loss train:2.4216, loss val:2.4385\n",
      "step:4500, loss train:2.4366, loss val:2.4460\n",
      "step:4750, loss train:2.4135, loss val:2.4356\n",
      "step:5000, loss train:2.4089, loss val:2.4612\n",
      "step:5250, loss train:2.4074, loss val:2.4399\n",
      "step:5500, loss train:2.4006, loss val:2.4434\n",
      "step:5750, loss train:2.4128, loss val:2.4293\n",
      "step:6000, loss train:2.4223, loss val:2.4365\n",
      "step:6250, loss train:2.4181, loss val:2.4690\n",
      "step:6500, loss train:2.4161, loss val:2.4652\n",
      "step:6750, loss train:2.4247, loss val:2.4267\n",
      "step:7000, loss train:2.4185, loss val:2.4534\n",
      "step:7250, loss train:2.4281, loss val:2.4300\n",
      "step:7500, loss train:2.4052, loss val:2.4397\n",
      "step:7750, loss train:2.4134, loss val:2.4412\n",
      "step:8000, loss train:2.4150, loss val:2.4589\n",
      "step:8250, loss train:2.4024, loss val:2.4559\n",
      "step:8500, loss train:2.3866, loss val:2.4643\n",
      "step:8750, loss train:2.4115, loss val:2.4542\n",
      "step:9000, loss train:2.4107, loss val:2.4698\n",
      "step:9250, loss train:2.4247, loss val:2.4360\n",
      "step:9500, loss train:2.4233, loss val:2.4472\n",
      "step:9750, loss train:2.4270, loss val:2.4415\n",
      "batch:67\n",
      "step:0, loss train:2.4137, loss val:2.4326\n",
      "step:250, loss train:2.3967, loss val:2.4303\n",
      "step:500, loss train:2.4085, loss val:2.4602\n",
      "step:750, loss train:2.4185, loss val:2.4449\n",
      "step:1000, loss train:2.4191, loss val:2.4649\n",
      "step:1250, loss train:2.4070, loss val:2.4478\n",
      "step:1500, loss train:2.4172, loss val:2.4448\n",
      "step:1750, loss train:2.3944, loss val:2.4437\n",
      "step:2000, loss train:2.4263, loss val:2.4591\n",
      "step:2250, loss train:2.3987, loss val:2.4596\n",
      "step:2500, loss train:2.4101, loss val:2.4600\n",
      "step:2750, loss train:2.4239, loss val:2.4406\n",
      "step:3000, loss train:2.4247, loss val:2.4188\n",
      "step:3250, loss train:2.4154, loss val:2.4597\n",
      "step:3500, loss train:2.4254, loss val:2.4569\n",
      "step:3750, loss train:2.4040, loss val:2.4614\n",
      "step:4000, loss train:2.4136, loss val:2.4440\n",
      "step:4250, loss train:2.3979, loss val:2.4511\n",
      "step:4500, loss train:2.4263, loss val:2.4431\n",
      "step:4750, loss train:2.3981, loss val:2.4347\n",
      "step:5000, loss train:2.4004, loss val:2.4634\n",
      "step:5250, loss train:2.4052, loss val:2.4502\n",
      "step:5500, loss train:2.4077, loss val:2.4497\n",
      "step:5750, loss train:2.4060, loss val:2.4651\n",
      "step:6000, loss train:2.4219, loss val:2.4545\n",
      "step:6250, loss train:2.4189, loss val:2.4430\n",
      "step:6500, loss train:2.4064, loss val:2.4589\n",
      "step:6750, loss train:2.4209, loss val:2.4513\n",
      "step:7000, loss train:2.4090, loss val:2.4637\n",
      "step:7250, loss train:2.4032, loss val:2.4447\n",
      "step:7500, loss train:2.4164, loss val:2.4253\n",
      "step:7750, loss train:2.4142, loss val:2.4746\n",
      "step:8000, loss train:2.4244, loss val:2.4412\n",
      "step:8250, loss train:2.4036, loss val:2.4469\n",
      "step:8500, loss train:2.4209, loss val:2.4439\n",
      "step:8750, loss train:2.4250, loss val:2.4497\n",
      "step:9000, loss train:2.4083, loss val:2.4538\n",
      "step:9250, loss train:2.4065, loss val:2.4552\n",
      "step:9500, loss train:2.4230, loss val:2.4738\n",
      "step:9750, loss train:2.4345, loss val:2.4455\n",
      "batch:68\n",
      "step:0, loss train:2.4023, loss val:2.4296\n",
      "step:250, loss train:2.3844, loss val:2.4449\n",
      "step:500, loss train:2.4103, loss val:2.4147\n",
      "step:750, loss train:2.4062, loss val:2.4471\n",
      "step:1000, loss train:2.4172, loss val:2.4486\n",
      "step:1250, loss train:2.4124, loss val:2.4460\n",
      "step:1500, loss train:2.3834, loss val:2.4311\n",
      "step:1750, loss train:2.4102, loss val:2.4429\n",
      "step:2000, loss train:2.4049, loss val:2.4613\n",
      "step:2250, loss train:2.4028, loss val:2.4496\n",
      "step:2500, loss train:2.3984, loss val:2.4417\n",
      "step:2750, loss train:2.4119, loss val:2.4490\n",
      "step:3000, loss train:2.4188, loss val:2.4765\n",
      "step:3250, loss train:2.3906, loss val:2.4462\n",
      "step:3500, loss train:2.4013, loss val:2.4213\n",
      "step:3750, loss train:2.4356, loss val:2.4314\n",
      "step:4000, loss train:2.4215, loss val:2.4552\n",
      "step:4250, loss train:2.4125, loss val:2.4580\n",
      "step:4500, loss train:2.4117, loss val:2.4692\n",
      "step:4750, loss train:2.4103, loss val:2.4619\n",
      "step:5000, loss train:2.4132, loss val:2.4376\n",
      "step:5250, loss train:2.4235, loss val:2.4512\n",
      "step:5500, loss train:2.3985, loss val:2.4603\n",
      "step:5750, loss train:2.4211, loss val:2.4406\n",
      "step:6000, loss train:2.4169, loss val:2.4551\n",
      "step:6250, loss train:2.4097, loss val:2.4525\n",
      "step:6500, loss train:2.4073, loss val:2.4570\n",
      "step:6750, loss train:2.4084, loss val:2.4338\n",
      "step:7000, loss train:2.4005, loss val:2.4368\n",
      "step:7250, loss train:2.4162, loss val:2.4317\n",
      "step:7500, loss train:2.4196, loss val:2.4498\n",
      "step:7750, loss train:2.4130, loss val:2.4379\n",
      "step:8000, loss train:2.4305, loss val:2.4389\n",
      "step:8250, loss train:2.4146, loss val:2.4513\n",
      "step:8500, loss train:2.4188, loss val:2.4458\n",
      "step:8750, loss train:2.4115, loss val:2.4408\n",
      "step:9000, loss train:2.4145, loss val:2.4482\n",
      "step:9250, loss train:2.4257, loss val:2.4755\n",
      "step:9500, loss train:2.3969, loss val:2.4495\n",
      "step:9750, loss train:2.4142, loss val:2.4817\n",
      "batch:69\n",
      "step:0, loss train:2.4334, loss val:2.4352\n",
      "step:250, loss train:2.4066, loss val:2.4431\n",
      "step:500, loss train:2.4148, loss val:2.4357\n",
      "step:750, loss train:2.4107, loss val:2.4334\n",
      "step:1000, loss train:2.4180, loss val:2.4574\n",
      "step:1250, loss train:2.4134, loss val:2.4488\n",
      "step:1500, loss train:2.4104, loss val:2.4522\n",
      "step:1750, loss train:2.4020, loss val:2.4286\n",
      "step:2000, loss train:2.4063, loss val:2.4444\n",
      "step:2250, loss train:2.4227, loss val:2.4237\n",
      "step:2500, loss train:2.3964, loss val:2.4661\n",
      "step:2750, loss train:2.4046, loss val:2.4492\n",
      "step:3000, loss train:2.4107, loss val:2.4575\n",
      "step:3250, loss train:2.3995, loss val:2.4585\n",
      "step:3500, loss train:2.4062, loss val:2.4567\n",
      "step:3750, loss train:2.4056, loss val:2.4563\n",
      "step:4000, loss train:2.4076, loss val:2.4360\n",
      "step:4250, loss train:2.4060, loss val:2.4507\n",
      "step:4500, loss train:2.3994, loss val:2.4757\n",
      "step:4750, loss train:2.4194, loss val:2.4370\n",
      "step:5000, loss train:2.4010, loss val:2.4381\n",
      "step:5250, loss train:2.4192, loss val:2.4243\n",
      "step:5500, loss train:2.4146, loss val:2.4533\n",
      "step:5750, loss train:2.4191, loss val:2.4391\n",
      "step:6000, loss train:2.4043, loss val:2.4660\n",
      "step:6250, loss train:2.4115, loss val:2.4420\n",
      "step:6500, loss train:2.4036, loss val:2.4710\n",
      "step:6750, loss train:2.4220, loss val:2.4651\n",
      "step:7000, loss train:2.4159, loss val:2.4579\n",
      "step:7250, loss train:2.4171, loss val:2.4531\n",
      "step:7500, loss train:2.4050, loss val:2.4437\n",
      "step:7750, loss train:2.4279, loss val:2.4544\n",
      "step:8000, loss train:2.4014, loss val:2.4456\n",
      "step:8250, loss train:2.4130, loss val:2.4561\n",
      "step:8500, loss train:2.4102, loss val:2.4474\n",
      "step:8750, loss train:2.4153, loss val:2.4339\n",
      "step:9000, loss train:2.4135, loss val:2.4430\n",
      "step:9250, loss train:2.4255, loss val:2.4871\n",
      "step:9500, loss train:2.4249, loss val:2.4495\n",
      "step:9750, loss train:2.4040, loss val:2.4391\n",
      "batch:70\n",
      "step:0, loss train:2.4117, loss val:2.4475\n",
      "step:250, loss train:2.4228, loss val:2.4631\n",
      "step:500, loss train:2.4098, loss val:2.4586\n",
      "step:750, loss train:2.4013, loss val:2.4622\n",
      "step:1000, loss train:2.4076, loss val:2.4615\n",
      "step:1250, loss train:2.4141, loss val:2.4570\n",
      "step:1500, loss train:2.4136, loss val:2.4374\n",
      "step:1750, loss train:2.4023, loss val:2.4442\n",
      "step:2000, loss train:2.4088, loss val:2.4535\n",
      "step:2250, loss train:2.4176, loss val:2.4204\n",
      "step:2500, loss train:2.4085, loss val:2.4370\n",
      "step:2750, loss train:2.4237, loss val:2.4429\n",
      "step:3000, loss train:2.4093, loss val:2.4403\n",
      "step:3250, loss train:2.4349, loss val:2.4597\n",
      "step:3500, loss train:2.4051, loss val:2.4242\n",
      "step:3750, loss train:2.4087, loss val:2.4545\n",
      "step:4000, loss train:2.4081, loss val:2.4279\n",
      "step:4250, loss train:2.4117, loss val:2.4491\n",
      "step:4500, loss train:2.4266, loss val:2.4321\n",
      "step:4750, loss train:2.3969, loss val:2.4499\n",
      "step:5000, loss train:2.4190, loss val:2.4358\n",
      "step:5250, loss train:2.4213, loss val:2.4459\n",
      "step:5500, loss train:2.4382, loss val:2.4636\n",
      "step:5750, loss train:2.4059, loss val:2.4300\n",
      "step:6000, loss train:2.4241, loss val:2.4527\n",
      "step:6250, loss train:2.4259, loss val:2.4485\n",
      "step:6500, loss train:2.4216, loss val:2.4739\n",
      "step:6750, loss train:2.3889, loss val:2.4337\n",
      "step:7000, loss train:2.4223, loss val:2.4240\n",
      "step:7250, loss train:2.4105, loss val:2.4426\n",
      "step:7500, loss train:2.4181, loss val:2.4516\n",
      "step:7750, loss train:2.4167, loss val:2.4612\n",
      "step:8000, loss train:2.3916, loss val:2.4473\n",
      "step:8250, loss train:2.4313, loss val:2.4341\n",
      "step:8500, loss train:2.4020, loss val:2.4281\n",
      "step:8750, loss train:2.4120, loss val:2.4526\n",
      "step:9000, loss train:2.4023, loss val:2.4433\n",
      "step:9250, loss train:2.4162, loss val:2.4414\n",
      "step:9500, loss train:2.3984, loss val:2.4495\n",
      "step:9750, loss train:2.3914, loss val:2.4363\n",
      "batch:71\n",
      "step:0, loss train:2.4090, loss val:2.4636\n",
      "step:250, loss train:2.4044, loss val:2.4215\n",
      "step:500, loss train:2.4051, loss val:2.4503\n",
      "step:750, loss train:2.4334, loss val:2.4257\n",
      "step:1000, loss train:2.4233, loss val:2.4538\n",
      "step:1250, loss train:2.4259, loss val:2.4549\n",
      "step:1500, loss train:2.4191, loss val:2.4460\n",
      "step:1750, loss train:2.4087, loss val:2.4729\n",
      "step:2000, loss train:2.4209, loss val:2.4517\n",
      "step:2250, loss train:2.4068, loss val:2.4379\n",
      "step:2500, loss train:2.4232, loss val:2.4355\n",
      "step:2750, loss train:2.4251, loss val:2.4559\n",
      "step:3000, loss train:2.4165, loss val:2.4415\n",
      "step:3250, loss train:2.3962, loss val:2.4670\n",
      "step:3500, loss train:2.4153, loss val:2.4507\n",
      "step:3750, loss train:2.3881, loss val:2.4667\n",
      "step:4000, loss train:2.4161, loss val:2.4485\n",
      "step:4250, loss train:2.3974, loss val:2.4369\n",
      "step:4500, loss train:2.4236, loss val:2.4513\n",
      "step:4750, loss train:2.4209, loss val:2.4541\n",
      "step:5000, loss train:2.4084, loss val:2.4479\n",
      "step:5250, loss train:2.4164, loss val:2.4515\n",
      "step:5500, loss train:2.4290, loss val:2.4592\n",
      "step:5750, loss train:2.4348, loss val:2.4529\n",
      "step:6000, loss train:2.4181, loss val:2.4553\n",
      "step:6250, loss train:2.4058, loss val:2.4570\n",
      "step:6500, loss train:2.3998, loss val:2.4401\n",
      "step:6750, loss train:2.3972, loss val:2.4541\n",
      "step:7000, loss train:2.3977, loss val:2.4546\n",
      "step:7250, loss train:2.4178, loss val:2.4193\n",
      "step:7500, loss train:2.3953, loss val:2.4384\n",
      "step:7750, loss train:2.4075, loss val:2.4441\n",
      "step:8000, loss train:2.3921, loss val:2.4586\n",
      "step:8250, loss train:2.3995, loss val:2.4540\n",
      "step:8500, loss train:2.4171, loss val:2.4616\n",
      "step:8750, loss train:2.4047, loss val:2.4444\n",
      "step:9000, loss train:2.4276, loss val:2.4484\n",
      "step:9250, loss train:2.4202, loss val:2.4467\n",
      "step:9500, loss train:2.4197, loss val:2.4317\n",
      "step:9750, loss train:2.4206, loss val:2.4683\n",
      "batch:72\n",
      "step:0, loss train:2.4202, loss val:2.4400\n",
      "step:250, loss train:2.4119, loss val:2.4469\n",
      "step:500, loss train:2.4108, loss val:2.4448\n",
      "step:750, loss train:2.4344, loss val:2.4488\n",
      "step:1000, loss train:2.4212, loss val:2.4544\n",
      "step:1250, loss train:2.4043, loss val:2.4623\n",
      "step:1500, loss train:2.3956, loss val:2.4279\n",
      "step:1750, loss train:2.4184, loss val:2.4510\n",
      "step:2000, loss train:2.4018, loss val:2.4354\n",
      "step:2250, loss train:2.4214, loss val:2.4295\n",
      "step:2500, loss train:2.4154, loss val:2.4689\n",
      "step:2750, loss train:2.4163, loss val:2.4385\n",
      "step:3000, loss train:2.4157, loss val:2.4301\n",
      "step:3250, loss train:2.4060, loss val:2.4453\n",
      "step:3500, loss train:2.4169, loss val:2.4276\n",
      "step:3750, loss train:2.4239, loss val:2.4465\n",
      "step:4000, loss train:2.4115, loss val:2.4203\n",
      "step:4250, loss train:2.4224, loss val:2.4431\n",
      "step:4500, loss train:2.4261, loss val:2.4506\n",
      "step:4750, loss train:2.4034, loss val:2.4526\n",
      "step:5000, loss train:2.4249, loss val:2.4311\n",
      "step:5250, loss train:2.4039, loss val:2.4167\n",
      "step:5500, loss train:2.3896, loss val:2.4044\n",
      "step:5750, loss train:2.4216, loss val:2.4412\n",
      "step:6000, loss train:2.4186, loss val:2.4458\n",
      "step:6250, loss train:2.4151, loss val:2.4406\n",
      "step:6500, loss train:2.4160, loss val:2.4570\n",
      "step:6750, loss train:2.4029, loss val:2.4496\n",
      "step:7000, loss train:2.4379, loss val:2.4556\n",
      "step:7250, loss train:2.4268, loss val:2.4624\n",
      "step:7500, loss train:2.4115, loss val:2.4696\n",
      "step:7750, loss train:2.4057, loss val:2.4408\n",
      "step:8000, loss train:2.3950, loss val:2.4414\n",
      "step:8250, loss train:2.4011, loss val:2.4396\n",
      "step:8500, loss train:2.4100, loss val:2.4538\n",
      "step:8750, loss train:2.4122, loss val:2.4484\n",
      "step:9000, loss train:2.4248, loss val:2.4598\n",
      "step:9250, loss train:2.4137, loss val:2.4614\n",
      "step:9500, loss train:2.4214, loss val:2.4508\n",
      "step:9750, loss train:2.4029, loss val:2.4552\n",
      "batch:73\n",
      "step:0, loss train:2.3954, loss val:2.4458\n",
      "step:250, loss train:2.4018, loss val:2.4694\n",
      "step:500, loss train:2.4429, loss val:2.4473\n",
      "step:750, loss train:2.4134, loss val:2.4633\n",
      "step:1000, loss train:2.4103, loss val:2.4707\n",
      "step:1250, loss train:2.4333, loss val:2.4468\n",
      "step:1500, loss train:2.4207, loss val:2.4570\n",
      "step:1750, loss train:2.4248, loss val:2.4525\n",
      "step:2000, loss train:2.3984, loss val:2.4584\n",
      "step:2250, loss train:2.4125, loss val:2.4342\n",
      "step:2500, loss train:2.4065, loss val:2.4489\n",
      "step:2750, loss train:2.4102, loss val:2.4622\n",
      "step:3000, loss train:2.4073, loss val:2.4603\n",
      "step:3250, loss train:2.4059, loss val:2.4517\n",
      "step:3500, loss train:2.4272, loss val:2.4444\n",
      "step:3750, loss train:2.4270, loss val:2.4552\n",
      "step:4000, loss train:2.4082, loss val:2.4336\n",
      "step:4250, loss train:2.4104, loss val:2.4587\n",
      "step:4500, loss train:2.3907, loss val:2.4323\n",
      "step:4750, loss train:2.4345, loss val:2.4424\n",
      "step:5000, loss train:2.4011, loss val:2.4338\n",
      "step:5250, loss train:2.4030, loss val:2.4448\n",
      "step:5500, loss train:2.4076, loss val:2.4517\n",
      "step:5750, loss train:2.4388, loss val:2.4231\n",
      "step:6000, loss train:2.4211, loss val:2.4541\n",
      "step:6250, loss train:2.4270, loss val:2.4561\n",
      "step:6500, loss train:2.4208, loss val:2.4690\n",
      "step:6750, loss train:2.3943, loss val:2.4575\n",
      "step:7000, loss train:2.3955, loss val:2.4334\n",
      "step:7250, loss train:2.4132, loss val:2.4477\n",
      "step:7500, loss train:2.3939, loss val:2.4467\n",
      "step:7750, loss train:2.3879, loss val:2.4525\n",
      "step:8000, loss train:2.4143, loss val:2.4576\n",
      "step:8250, loss train:2.4081, loss val:2.4544\n",
      "step:8500, loss train:2.4031, loss val:2.4308\n",
      "step:8750, loss train:2.4225, loss val:2.4584\n",
      "step:9000, loss train:2.4178, loss val:2.4497\n",
      "step:9250, loss train:2.4135, loss val:2.4243\n",
      "step:9500, loss train:2.4360, loss val:2.4419\n",
      "step:9750, loss train:2.4387, loss val:2.4499\n",
      "batch:74\n",
      "step:0, loss train:2.4145, loss val:2.4418\n",
      "step:250, loss train:2.4112, loss val:2.4678\n",
      "step:500, loss train:2.4037, loss val:2.4419\n",
      "step:750, loss train:2.3968, loss val:2.4333\n",
      "step:1000, loss train:2.4286, loss val:2.4686\n",
      "step:1250, loss train:2.3968, loss val:2.4481\n",
      "step:1500, loss train:2.4200, loss val:2.4537\n",
      "step:1750, loss train:2.4084, loss val:2.4516\n",
      "step:2000, loss train:2.4107, loss val:2.4351\n",
      "step:2250, loss train:2.4280, loss val:2.4441\n",
      "step:2500, loss train:2.4050, loss val:2.4151\n",
      "step:2750, loss train:2.4045, loss val:2.4253\n",
      "step:3000, loss train:2.4077, loss val:2.4502\n",
      "step:3250, loss train:2.4180, loss val:2.4552\n",
      "step:3500, loss train:2.4031, loss val:2.4491\n",
      "step:3750, loss train:2.4160, loss val:2.4599\n",
      "step:4000, loss train:2.4165, loss val:2.4391\n",
      "step:4250, loss train:2.4135, loss val:2.4525\n",
      "step:4500, loss train:2.4163, loss val:2.4686\n",
      "step:4750, loss train:2.4052, loss val:2.4455\n",
      "step:5000, loss train:2.4223, loss val:2.4439\n",
      "step:5250, loss train:2.4271, loss val:2.4519\n",
      "step:5500, loss train:2.4141, loss val:2.4443\n",
      "step:5750, loss train:2.3949, loss val:2.4459\n",
      "step:6000, loss train:2.4116, loss val:2.4602\n",
      "step:6250, loss train:2.3982, loss val:2.4367\n",
      "step:6500, loss train:2.4106, loss val:2.4412\n",
      "step:6750, loss train:2.4155, loss val:2.4515\n",
      "step:7000, loss train:2.4004, loss val:2.4461\n",
      "step:7250, loss train:2.4058, loss val:2.4420\n",
      "step:7500, loss train:2.4215, loss val:2.4436\n",
      "step:7750, loss train:2.4084, loss val:2.4426\n",
      "step:8000, loss train:2.4181, loss val:2.4524\n",
      "step:8250, loss train:2.4550, loss val:2.4496\n",
      "step:8500, loss train:2.4210, loss val:2.4261\n",
      "step:8750, loss train:2.4128, loss val:2.4512\n",
      "step:9000, loss train:2.4296, loss val:2.4461\n",
      "step:9250, loss train:2.4124, loss val:2.4457\n",
      "step:9500, loss train:2.4229, loss val:2.4432\n",
      "step:9750, loss train:2.4199, loss val:2.4460\n",
      "batch:75\n",
      "step:0, loss train:2.4076, loss val:2.4545\n",
      "step:250, loss train:2.4092, loss val:2.4600\n",
      "step:500, loss train:2.4279, loss val:2.4494\n",
      "step:750, loss train:2.4097, loss val:2.4531\n",
      "step:1000, loss train:2.4237, loss val:2.4297\n",
      "step:1250, loss train:2.4113, loss val:2.4308\n",
      "step:1500, loss train:2.4128, loss val:2.4430\n",
      "step:1750, loss train:2.3994, loss val:2.4762\n",
      "step:2000, loss train:2.4123, loss val:2.4518\n",
      "step:2250, loss train:2.4240, loss val:2.4476\n",
      "step:2500, loss train:2.3948, loss val:2.4400\n",
      "step:2750, loss train:2.4093, loss val:2.4295\n",
      "step:3000, loss train:2.4052, loss val:2.4220\n",
      "step:3250, loss train:2.4229, loss val:2.4429\n",
      "step:3500, loss train:2.4115, loss val:2.4379\n",
      "step:3750, loss train:2.4067, loss val:2.4371\n",
      "step:4000, loss train:2.4227, loss val:2.4572\n",
      "step:4250, loss train:2.4278, loss val:2.4384\n",
      "step:4500, loss train:2.3923, loss val:2.4458\n",
      "step:4750, loss train:2.4064, loss val:2.4634\n",
      "step:5000, loss train:2.4056, loss val:2.4166\n",
      "step:5250, loss train:2.4261, loss val:2.4451\n",
      "step:5500, loss train:2.4347, loss val:2.4680\n",
      "step:5750, loss train:2.4150, loss val:2.4376\n",
      "step:6000, loss train:2.3927, loss val:2.4432\n",
      "step:6250, loss train:2.4078, loss val:2.4586\n",
      "step:6500, loss train:2.4040, loss val:2.4417\n",
      "step:6750, loss train:2.4156, loss val:2.4323\n",
      "step:7000, loss train:2.4249, loss val:2.4631\n",
      "step:7250, loss train:2.4104, loss val:2.4213\n",
      "step:7500, loss train:2.4221, loss val:2.4482\n",
      "step:7750, loss train:2.4033, loss val:2.4495\n",
      "step:8000, loss train:2.4151, loss val:2.4439\n",
      "step:8250, loss train:2.4176, loss val:2.4619\n",
      "step:8500, loss train:2.4083, loss val:2.4603\n",
      "step:8750, loss train:2.4206, loss val:2.4614\n",
      "step:9000, loss train:2.4104, loss val:2.4613\n",
      "step:9250, loss train:2.4244, loss val:2.4256\n",
      "step:9500, loss train:2.4209, loss val:2.4662\n",
      "step:9750, loss train:2.4132, loss val:2.4473\n",
      "batch:76\n",
      "step:0, loss train:2.4131, loss val:2.4392\n",
      "step:250, loss train:2.4147, loss val:2.4363\n",
      "step:500, loss train:2.4041, loss val:2.4170\n",
      "step:750, loss train:2.4259, loss val:2.4235\n",
      "step:1000, loss train:2.4137, loss val:2.4582\n",
      "step:1250, loss train:2.4064, loss val:2.4330\n",
      "step:1500, loss train:2.4039, loss val:2.4435\n",
      "step:1750, loss train:2.4264, loss val:2.4343\n",
      "step:2000, loss train:2.4198, loss val:2.4293\n",
      "step:2250, loss train:2.4082, loss val:2.4544\n",
      "step:2500, loss train:2.4159, loss val:2.4494\n",
      "step:2750, loss train:2.3923, loss val:2.4490\n",
      "step:3000, loss train:2.4111, loss val:2.4609\n",
      "step:3250, loss train:2.4082, loss val:2.4534\n",
      "step:3500, loss train:2.4045, loss val:2.4476\n",
      "step:3750, loss train:2.4109, loss val:2.4607\n",
      "step:4000, loss train:2.4053, loss val:2.4366\n",
      "step:4250, loss train:2.4174, loss val:2.4421\n",
      "step:4500, loss train:2.4082, loss val:2.4437\n",
      "step:4750, loss train:2.4109, loss val:2.4330\n",
      "step:5000, loss train:2.4350, loss val:2.4458\n",
      "step:5250, loss train:2.4002, loss val:2.4444\n",
      "step:5500, loss train:2.4115, loss val:2.4484\n",
      "step:5750, loss train:2.4199, loss val:2.4360\n",
      "step:6000, loss train:2.4127, loss val:2.4235\n",
      "step:6250, loss train:2.4302, loss val:2.4535\n",
      "step:6500, loss train:2.3994, loss val:2.4450\n",
      "step:6750, loss train:2.4064, loss val:2.4325\n",
      "step:7000, loss train:2.3991, loss val:2.4278\n",
      "step:7250, loss train:2.4168, loss val:2.4411\n",
      "step:7500, loss train:2.4197, loss val:2.4336\n",
      "step:7750, loss train:2.4196, loss val:2.4732\n",
      "step:8000, loss train:2.4226, loss val:2.4603\n",
      "step:8250, loss train:2.4308, loss val:2.4462\n",
      "step:8500, loss train:2.4101, loss val:2.4411\n",
      "step:8750, loss train:2.3997, loss val:2.4493\n",
      "step:9000, loss train:2.4164, loss val:2.4364\n",
      "step:9250, loss train:2.4233, loss val:2.4530\n",
      "step:9500, loss train:2.4262, loss val:2.4443\n",
      "step:9750, loss train:2.4268, loss val:2.4180\n",
      "batch:77\n",
      "step:0, loss train:2.4138, loss val:2.4413\n",
      "step:250, loss train:2.4157, loss val:2.4345\n",
      "step:500, loss train:2.4231, loss val:2.4540\n",
      "step:750, loss train:2.4176, loss val:2.4388\n",
      "step:1000, loss train:2.4402, loss val:2.4570\n",
      "step:1250, loss train:2.4242, loss val:2.4244\n",
      "step:1500, loss train:2.4204, loss val:2.4399\n",
      "step:1750, loss train:2.4124, loss val:2.4340\n",
      "step:2000, loss train:2.4100, loss val:2.4601\n",
      "step:2250, loss train:2.4165, loss val:2.4327\n",
      "step:2500, loss train:2.4249, loss val:2.4359\n",
      "step:2750, loss train:2.4246, loss val:2.4154\n",
      "step:3000, loss train:2.3990, loss val:2.4526\n",
      "step:3250, loss train:2.4165, loss val:2.4606\n",
      "step:3500, loss train:2.4202, loss val:2.4377\n",
      "step:3750, loss train:2.4041, loss val:2.4647\n",
      "step:4000, loss train:2.4080, loss val:2.4387\n",
      "step:4250, loss train:2.4142, loss val:2.4608\n",
      "step:4500, loss train:2.4226, loss val:2.4454\n",
      "step:4750, loss train:2.4237, loss val:2.4464\n",
      "step:5000, loss train:2.4093, loss val:2.4336\n",
      "step:5250, loss train:2.4185, loss val:2.4449\n",
      "step:5500, loss train:2.4103, loss val:2.4384\n",
      "step:5750, loss train:2.4048, loss val:2.4185\n",
      "step:6000, loss train:2.4099, loss val:2.4321\n",
      "step:6250, loss train:2.3937, loss val:2.4415\n",
      "step:6500, loss train:2.4088, loss val:2.4561\n",
      "step:6750, loss train:2.4060, loss val:2.4416\n",
      "step:7000, loss train:2.3981, loss val:2.4333\n",
      "step:7250, loss train:2.4021, loss val:2.4527\n",
      "step:7500, loss train:2.4166, loss val:2.4448\n",
      "step:7750, loss train:2.4243, loss val:2.4316\n",
      "step:8000, loss train:2.4077, loss val:2.4537\n",
      "step:8250, loss train:2.4353, loss val:2.4380\n",
      "step:8500, loss train:2.4212, loss val:2.4519\n",
      "step:8750, loss train:2.4238, loss val:2.4584\n",
      "step:9000, loss train:2.4053, loss val:2.4590\n",
      "step:9250, loss train:2.4095, loss val:2.4742\n",
      "step:9500, loss train:2.4269, loss val:2.4699\n",
      "step:9750, loss train:2.3914, loss val:2.4437\n",
      "batch:78\n",
      "step:0, loss train:2.4120, loss val:2.4477\n",
      "step:250, loss train:2.4120, loss val:2.4506\n",
      "step:500, loss train:2.4192, loss val:2.4492\n",
      "step:750, loss train:2.3958, loss val:2.4443\n",
      "step:1000, loss train:2.4198, loss val:2.4470\n",
      "step:1250, loss train:2.4152, loss val:2.4595\n",
      "step:1500, loss train:2.4158, loss val:2.4330\n",
      "step:1750, loss train:2.4231, loss val:2.4472\n",
      "step:2000, loss train:2.4143, loss val:2.4356\n",
      "step:2250, loss train:2.4117, loss val:2.4492\n",
      "step:2500, loss train:2.4069, loss val:2.4482\n",
      "step:2750, loss train:2.4168, loss val:2.4501\n",
      "step:3000, loss train:2.4266, loss val:2.4178\n",
      "step:3250, loss train:2.4163, loss val:2.4549\n",
      "step:3500, loss train:2.4144, loss val:2.4332\n",
      "step:3750, loss train:2.4194, loss val:2.4398\n",
      "step:4000, loss train:2.4030, loss val:2.4126\n",
      "step:4250, loss train:2.4083, loss val:2.4532\n",
      "step:4500, loss train:2.4074, loss val:2.4528\n",
      "step:4750, loss train:2.4219, loss val:2.4616\n",
      "step:5000, loss train:2.4062, loss val:2.4280\n",
      "step:5250, loss train:2.4051, loss val:2.4502\n",
      "step:5500, loss train:2.4215, loss val:2.4377\n",
      "step:5750, loss train:2.4113, loss val:2.4615\n",
      "step:6000, loss train:2.4080, loss val:2.4342\n",
      "step:6250, loss train:2.4098, loss val:2.4388\n",
      "step:6500, loss train:2.4261, loss val:2.4356\n",
      "step:6750, loss train:2.4379, loss val:2.4408\n",
      "step:7000, loss train:2.4105, loss val:2.4572\n",
      "step:7250, loss train:2.4253, loss val:2.4511\n",
      "step:7500, loss train:2.4073, loss val:2.4468\n",
      "step:7750, loss train:2.4191, loss val:2.4382\n",
      "step:8000, loss train:2.4051, loss val:2.4694\n",
      "step:8250, loss train:2.4085, loss val:2.4499\n",
      "step:8500, loss train:2.4109, loss val:2.4533\n",
      "step:8750, loss train:2.4094, loss val:2.4826\n",
      "step:9000, loss train:2.4175, loss val:2.4269\n",
      "step:9250, loss train:2.4056, loss val:2.4406\n",
      "step:9500, loss train:2.4187, loss val:2.4427\n",
      "step:9750, loss train:2.4064, loss val:2.4319\n",
      "batch:79\n",
      "step:0, loss train:2.4106, loss val:2.4494\n",
      "step:250, loss train:2.4156, loss val:2.4537\n",
      "step:500, loss train:2.4284, loss val:2.4492\n",
      "step:750, loss train:2.4146, loss val:2.4497\n",
      "step:1000, loss train:2.4058, loss val:2.4404\n",
      "step:1250, loss train:2.4068, loss val:2.4563\n",
      "step:1500, loss train:2.4185, loss val:2.4580\n",
      "step:1750, loss train:2.4142, loss val:2.4517\n",
      "step:2000, loss train:2.4140, loss val:2.4588\n",
      "step:2250, loss train:2.4134, loss val:2.4434\n",
      "step:2500, loss train:2.4116, loss val:2.4418\n",
      "step:2750, loss train:2.4012, loss val:2.4649\n",
      "step:3000, loss train:2.4092, loss val:2.4486\n",
      "step:3250, loss train:2.4093, loss val:2.4435\n",
      "step:3500, loss train:2.4056, loss val:2.4403\n",
      "step:3750, loss train:2.4076, loss val:2.4500\n",
      "step:4000, loss train:2.4239, loss val:2.4502\n",
      "step:4250, loss train:2.4194, loss val:2.4717\n",
      "step:4500, loss train:2.4309, loss val:2.4252\n",
      "step:4750, loss train:2.3993, loss val:2.4476\n",
      "step:5000, loss train:2.4134, loss val:2.4312\n",
      "step:5250, loss train:2.4033, loss val:2.4508\n",
      "step:5500, loss train:2.4138, loss val:2.4539\n",
      "step:5750, loss train:2.4107, loss val:2.4570\n",
      "step:6000, loss train:2.3975, loss val:2.4523\n",
      "step:6250, loss train:2.4215, loss val:2.4464\n",
      "step:6500, loss train:2.4201, loss val:2.4473\n",
      "step:6750, loss train:2.4217, loss val:2.4640\n",
      "step:7000, loss train:2.4283, loss val:2.4449\n",
      "step:7250, loss train:2.4114, loss val:2.4532\n",
      "step:7500, loss train:2.4230, loss val:2.4460\n",
      "step:7750, loss train:2.4164, loss val:2.4349\n",
      "step:8000, loss train:2.4093, loss val:2.4241\n",
      "step:8250, loss train:2.3967, loss val:2.4458\n",
      "step:8500, loss train:2.4069, loss val:2.4260\n",
      "step:8750, loss train:2.4209, loss val:2.4428\n",
      "step:9000, loss train:2.4101, loss val:2.4425\n",
      "step:9250, loss train:2.4265, loss val:2.4403\n",
      "step:9500, loss train:2.4140, loss val:2.4654\n",
      "step:9750, loss train:2.4173, loss val:2.4473\n",
      "batch:80\n",
      "step:0, loss train:2.4159, loss val:2.4394\n",
      "step:250, loss train:2.4328, loss val:2.4608\n",
      "step:500, loss train:2.4200, loss val:2.4362\n",
      "step:750, loss train:2.4252, loss val:2.4481\n",
      "step:1000, loss train:2.4099, loss val:2.4551\n",
      "step:1250, loss train:2.4070, loss val:2.4496\n",
      "step:1500, loss train:2.4009, loss val:2.4271\n",
      "step:1750, loss train:2.4188, loss val:2.4403\n",
      "step:2000, loss train:2.4205, loss val:2.4732\n",
      "step:2250, loss train:2.4273, loss val:2.4556\n",
      "step:2500, loss train:2.4133, loss val:2.4267\n",
      "step:2750, loss train:2.4179, loss val:2.4477\n",
      "step:3000, loss train:2.4005, loss val:2.4624\n",
      "step:3250, loss train:2.4031, loss val:2.4516\n",
      "step:3500, loss train:2.4150, loss val:2.4501\n",
      "step:3750, loss train:2.4141, loss val:2.4618\n",
      "step:4000, loss train:2.4063, loss val:2.4399\n",
      "step:4250, loss train:2.4056, loss val:2.4272\n",
      "step:4500, loss train:2.4283, loss val:2.4347\n",
      "step:4750, loss train:2.4108, loss val:2.4519\n",
      "step:5000, loss train:2.4230, loss val:2.4472\n",
      "step:5250, loss train:2.4133, loss val:2.4525\n",
      "step:5500, loss train:2.4092, loss val:2.4343\n",
      "step:5750, loss train:2.4129, loss val:2.4692\n",
      "step:6000, loss train:2.4032, loss val:2.4496\n",
      "step:6250, loss train:2.4151, loss val:2.4453\n",
      "step:6500, loss train:2.4180, loss val:2.4548\n",
      "step:6750, loss train:2.4221, loss val:2.4263\n",
      "step:7000, loss train:2.4132, loss val:2.4504\n",
      "step:7250, loss train:2.3997, loss val:2.4482\n",
      "step:7500, loss train:2.3987, loss val:2.4588\n",
      "step:7750, loss train:2.4109, loss val:2.4495\n",
      "step:8000, loss train:2.3946, loss val:2.4568\n",
      "step:8250, loss train:2.4213, loss val:2.4252\n",
      "step:8500, loss train:2.4092, loss val:2.4314\n",
      "step:8750, loss train:2.4169, loss val:2.4333\n",
      "step:9000, loss train:2.4026, loss val:2.4414\n",
      "step:9250, loss train:2.4186, loss val:2.4287\n",
      "step:9500, loss train:2.4173, loss val:2.4487\n",
      "step:9750, loss train:2.4212, loss val:2.4508\n",
      "batch:81\n",
      "step:0, loss train:2.4175, loss val:2.4643\n",
      "step:250, loss train:2.4040, loss val:2.4154\n",
      "step:500, loss train:2.4042, loss val:2.4304\n",
      "step:750, loss train:2.4131, loss val:2.4298\n",
      "step:1000, loss train:2.4063, loss val:2.4492\n",
      "step:1250, loss train:2.4153, loss val:2.4569\n",
      "step:1500, loss train:2.4043, loss val:2.4506\n",
      "step:1750, loss train:2.4031, loss val:2.4430\n",
      "step:2000, loss train:2.3943, loss val:2.4395\n",
      "step:2250, loss train:2.4043, loss val:2.4357\n",
      "step:2500, loss train:2.4402, loss val:2.4429\n",
      "step:2750, loss train:2.4064, loss val:2.4515\n",
      "step:3000, loss train:2.4057, loss val:2.4420\n",
      "step:3250, loss train:2.4180, loss val:2.4328\n",
      "step:3500, loss train:2.4109, loss val:2.4379\n",
      "step:3750, loss train:2.4083, loss val:2.4444\n",
      "step:4000, loss train:2.4036, loss val:2.4533\n",
      "step:4250, loss train:2.4144, loss val:2.4333\n",
      "step:4500, loss train:2.4078, loss val:2.4381\n",
      "step:4750, loss train:2.4214, loss val:2.4210\n",
      "step:5000, loss train:2.3949, loss val:2.4443\n",
      "step:5250, loss train:2.4174, loss val:2.4578\n",
      "step:5500, loss train:2.3991, loss val:2.4317\n",
      "step:5750, loss train:2.4180, loss val:2.4436\n",
      "step:6000, loss train:2.3999, loss val:2.4331\n",
      "step:6250, loss train:2.4157, loss val:2.4496\n",
      "step:6500, loss train:2.4185, loss val:2.4399\n",
      "step:6750, loss train:2.4123, loss val:2.4350\n",
      "step:7000, loss train:2.4067, loss val:2.4343\n",
      "step:7250, loss train:2.4191, loss val:2.4427\n",
      "step:7500, loss train:2.4177, loss val:2.4534\n",
      "step:7750, loss train:2.3901, loss val:2.4296\n",
      "step:8000, loss train:2.3975, loss val:2.4561\n",
      "step:8250, loss train:2.4210, loss val:2.4464\n",
      "step:8500, loss train:2.4322, loss val:2.4429\n",
      "step:8750, loss train:2.4104, loss val:2.4543\n",
      "step:9000, loss train:2.4046, loss val:2.4161\n",
      "step:9250, loss train:2.4131, loss val:2.4558\n",
      "step:9500, loss train:2.4174, loss val:2.4341\n",
      "step:9750, loss train:2.4349, loss val:2.4252\n",
      "batch:82\n",
      "step:0, loss train:2.4312, loss val:2.4358\n",
      "step:250, loss train:2.4118, loss val:2.4446\n",
      "step:500, loss train:2.3999, loss val:2.4495\n",
      "step:750, loss train:2.4092, loss val:2.4458\n",
      "step:1000, loss train:2.4189, loss val:2.4112\n",
      "step:1250, loss train:2.3971, loss val:2.4505\n",
      "step:1500, loss train:2.4221, loss val:2.4378\n",
      "step:1750, loss train:2.4128, loss val:2.4518\n",
      "step:2000, loss train:2.4096, loss val:2.4344\n",
      "step:2250, loss train:2.3964, loss val:2.4461\n",
      "step:2500, loss train:2.4048, loss val:2.4631\n",
      "step:2750, loss train:2.4284, loss val:2.4426\n",
      "step:3000, loss train:2.4224, loss val:2.4504\n",
      "step:3250, loss train:2.4352, loss val:2.4547\n",
      "step:3500, loss train:2.4149, loss val:2.4164\n",
      "step:3750, loss train:2.4159, loss val:2.4567\n",
      "step:4000, loss train:2.4146, loss val:2.4542\n",
      "step:4250, loss train:2.4247, loss val:2.4496\n",
      "step:4500, loss train:2.4099, loss val:2.4378\n",
      "step:4750, loss train:2.4062, loss val:2.4298\n",
      "step:5000, loss train:2.4039, loss val:2.4654\n",
      "step:5250, loss train:2.4210, loss val:2.4466\n",
      "step:5500, loss train:2.4103, loss val:2.4430\n",
      "step:5750, loss train:2.4097, loss val:2.4475\n",
      "step:6000, loss train:2.4296, loss val:2.4286\n",
      "step:6250, loss train:2.4205, loss val:2.4376\n",
      "step:6500, loss train:2.4257, loss val:2.4583\n",
      "step:6750, loss train:2.4169, loss val:2.4587\n",
      "step:7000, loss train:2.4225, loss val:2.4488\n",
      "step:7250, loss train:2.3996, loss val:2.4371\n",
      "step:7500, loss train:2.3987, loss val:2.4274\n",
      "step:7750, loss train:2.4051, loss val:2.4534\n",
      "step:8000, loss train:2.4154, loss val:2.4376\n",
      "step:8250, loss train:2.4005, loss val:2.4585\n",
      "step:8500, loss train:2.4106, loss val:2.4489\n",
      "step:8750, loss train:2.4087, loss val:2.4474\n",
      "step:9000, loss train:2.4107, loss val:2.4413\n",
      "step:9250, loss train:2.4332, loss val:2.4593\n",
      "step:9500, loss train:2.4088, loss val:2.4334\n",
      "step:9750, loss train:2.4299, loss val:2.4396\n",
      "batch:83\n",
      "step:0, loss train:2.4219, loss val:2.4539\n",
      "step:250, loss train:2.4020, loss val:2.4398\n",
      "step:500, loss train:2.4048, loss val:2.4402\n",
      "step:750, loss train:2.4180, loss val:2.4547\n",
      "step:1000, loss train:2.4270, loss val:2.4492\n",
      "step:1250, loss train:2.4199, loss val:2.4561\n",
      "step:1500, loss train:2.4079, loss val:2.4443\n",
      "step:1750, loss train:2.4202, loss val:2.4148\n",
      "step:2000, loss train:2.4056, loss val:2.4537\n",
      "step:2250, loss train:2.4040, loss val:2.4382\n",
      "step:2500, loss train:2.4218, loss val:2.4312\n",
      "step:2750, loss train:2.4171, loss val:2.4311\n",
      "step:3000, loss train:2.4063, loss val:2.4485\n",
      "step:3250, loss train:2.3918, loss val:2.4512\n",
      "step:3500, loss train:2.4230, loss val:2.4498\n",
      "step:3750, loss train:2.4244, loss val:2.4319\n",
      "step:4000, loss train:2.4132, loss val:2.4730\n",
      "step:4250, loss train:2.4399, loss val:2.4502\n",
      "step:4500, loss train:2.4296, loss val:2.4656\n",
      "step:4750, loss train:2.4147, loss val:2.4480\n",
      "step:5000, loss train:2.4057, loss val:2.4503\n",
      "step:5250, loss train:2.4057, loss val:2.4422\n",
      "step:5500, loss train:2.3994, loss val:2.4505\n",
      "step:5750, loss train:2.4231, loss val:2.4302\n",
      "step:6000, loss train:2.4061, loss val:2.4321\n",
      "step:6250, loss train:2.4080, loss val:2.4460\n",
      "step:6500, loss train:2.4256, loss val:2.4458\n",
      "step:6750, loss train:2.4144, loss val:2.4236\n",
      "step:7000, loss train:2.4024, loss val:2.4511\n",
      "step:7250, loss train:2.4147, loss val:2.4225\n",
      "step:7500, loss train:2.4150, loss val:2.4533\n",
      "step:7750, loss train:2.4196, loss val:2.4334\n",
      "step:8000, loss train:2.4126, loss val:2.4592\n",
      "step:8250, loss train:2.4135, loss val:2.4481\n",
      "step:8500, loss train:2.4164, loss val:2.4456\n",
      "step:8750, loss train:2.4170, loss val:2.4311\n",
      "step:9000, loss train:2.4078, loss val:2.4729\n",
      "step:9250, loss train:2.4206, loss val:2.4252\n",
      "step:9500, loss train:2.4055, loss val:2.4611\n",
      "step:9750, loss train:2.4122, loss val:2.4336\n",
      "batch:84\n",
      "step:0, loss train:2.3943, loss val:2.4312\n",
      "step:250, loss train:2.4404, loss val:2.4419\n",
      "step:500, loss train:2.4124, loss val:2.4471\n",
      "step:750, loss train:2.4140, loss val:2.4286\n",
      "step:1000, loss train:2.4189, loss val:2.4516\n",
      "step:1250, loss train:2.4254, loss val:2.4402\n",
      "step:1500, loss train:2.3916, loss val:2.4551\n",
      "step:1750, loss train:2.4107, loss val:2.4675\n",
      "step:2000, loss train:2.4098, loss val:2.4519\n",
      "step:2250, loss train:2.3752, loss val:2.4479\n",
      "step:2500, loss train:2.4003, loss val:2.4443\n",
      "step:2750, loss train:2.4245, loss val:2.4711\n",
      "step:3000, loss train:2.4040, loss val:2.4278\n",
      "step:3250, loss train:2.4032, loss val:2.4451\n",
      "step:3500, loss train:2.3950, loss val:2.4496\n",
      "step:3750, loss train:2.4107, loss val:2.4385\n",
      "step:4000, loss train:2.4041, loss val:2.4670\n",
      "step:4250, loss train:2.4042, loss val:2.4437\n",
      "step:4500, loss train:2.4346, loss val:2.4456\n",
      "step:4750, loss train:2.4133, loss val:2.4412\n",
      "step:5000, loss train:2.4122, loss val:2.4674\n",
      "step:5250, loss train:2.4187, loss val:2.4492\n",
      "step:5500, loss train:2.4246, loss val:2.4334\n",
      "step:5750, loss train:2.3966, loss val:2.4606\n",
      "step:6000, loss train:2.4069, loss val:2.4536\n",
      "step:6250, loss train:2.3900, loss val:2.4389\n",
      "step:6500, loss train:2.4109, loss val:2.4521\n",
      "step:6750, loss train:2.4255, loss val:2.4561\n",
      "step:7000, loss train:2.4101, loss val:2.4553\n",
      "step:7250, loss train:2.4057, loss val:2.4551\n",
      "step:7500, loss train:2.4051, loss val:2.4278\n",
      "step:7750, loss train:2.4091, loss val:2.4495\n",
      "step:8000, loss train:2.4115, loss val:2.4419\n",
      "step:8250, loss train:2.4079, loss val:2.4275\n",
      "step:8500, loss train:2.3968, loss val:2.4313\n",
      "step:8750, loss train:2.4124, loss val:2.4373\n",
      "step:9000, loss train:2.4093, loss val:2.4541\n",
      "step:9250, loss train:2.4165, loss val:2.4264\n",
      "step:9500, loss train:2.4226, loss val:2.4474\n",
      "step:9750, loss train:2.4014, loss val:2.4418\n",
      "batch:85\n",
      "step:0, loss train:2.3960, loss val:2.4548\n",
      "step:250, loss train:2.4261, loss val:2.4220\n",
      "step:500, loss train:2.4160, loss val:2.4409\n",
      "step:750, loss train:2.4096, loss val:2.4660\n",
      "step:1000, loss train:2.4047, loss val:2.4310\n",
      "step:1250, loss train:2.3948, loss val:2.4595\n",
      "step:1500, loss train:2.4200, loss val:2.4457\n",
      "step:1750, loss train:2.4268, loss val:2.4522\n",
      "step:2000, loss train:2.4092, loss val:2.4501\n",
      "step:2250, loss train:2.4063, loss val:2.4449\n",
      "step:2500, loss train:2.4016, loss val:2.4496\n",
      "step:2750, loss train:2.4169, loss val:2.4467\n",
      "step:3000, loss train:2.4167, loss val:2.4537\n",
      "step:3250, loss train:2.3969, loss val:2.4593\n",
      "step:3500, loss train:2.4053, loss val:2.4418\n",
      "step:3750, loss train:2.4018, loss val:2.4613\n",
      "step:4000, loss train:2.4254, loss val:2.4728\n",
      "step:4250, loss train:2.4228, loss val:2.4671\n",
      "step:4500, loss train:2.4194, loss val:2.4507\n",
      "step:4750, loss train:2.4128, loss val:2.4430\n",
      "step:5000, loss train:2.4214, loss val:2.4625\n",
      "step:5250, loss train:2.4214, loss val:2.4672\n",
      "step:5500, loss train:2.4183, loss val:2.4509\n",
      "step:5750, loss train:2.4116, loss val:2.4361\n",
      "step:6000, loss train:2.4130, loss val:2.4454\n",
      "step:6250, loss train:2.4013, loss val:2.4386\n",
      "step:6500, loss train:2.4103, loss val:2.4306\n",
      "step:6750, loss train:2.4174, loss val:2.4381\n",
      "step:7000, loss train:2.4189, loss val:2.4612\n",
      "step:7250, loss train:2.3914, loss val:2.4629\n",
      "step:7500, loss train:2.4138, loss val:2.4571\n",
      "step:7750, loss train:2.4226, loss val:2.4605\n",
      "step:8000, loss train:2.3844, loss val:2.4502\n",
      "step:8250, loss train:2.4120, loss val:2.4618\n",
      "step:8500, loss train:2.3971, loss val:2.4301\n",
      "step:8750, loss train:2.4052, loss val:2.4589\n",
      "step:9000, loss train:2.4192, loss val:2.4474\n",
      "step:9250, loss train:2.3935, loss val:2.4467\n",
      "step:9500, loss train:2.4149, loss val:2.4562\n",
      "step:9750, loss train:2.4113, loss val:2.4452\n",
      "batch:86\n",
      "step:0, loss train:2.4134, loss val:2.4278\n",
      "step:250, loss train:2.4156, loss val:2.4496\n",
      "step:500, loss train:2.4213, loss val:2.4688\n",
      "step:750, loss train:2.4106, loss val:2.4678\n",
      "step:1000, loss train:2.4217, loss val:2.4483\n",
      "step:1250, loss train:2.4179, loss val:2.4370\n",
      "step:1500, loss train:2.4184, loss val:2.4477\n",
      "step:1750, loss train:2.4084, loss val:2.4636\n",
      "step:2000, loss train:2.4216, loss val:2.4618\n",
      "step:2250, loss train:2.4108, loss val:2.4455\n",
      "step:2500, loss train:2.4164, loss val:2.4564\n",
      "step:2750, loss train:2.4148, loss val:2.4704\n",
      "step:3000, loss train:2.4001, loss val:2.4479\n",
      "step:3250, loss train:2.3987, loss val:2.4507\n",
      "step:3500, loss train:2.4062, loss val:2.4387\n",
      "step:3750, loss train:2.4031, loss val:2.4536\n",
      "step:4000, loss train:2.4076, loss val:2.4632\n",
      "step:4250, loss train:2.4123, loss val:2.4578\n",
      "step:4500, loss train:2.4177, loss val:2.4657\n",
      "step:4750, loss train:2.4241, loss val:2.4737\n",
      "step:5000, loss train:2.3961, loss val:2.4251\n",
      "step:5250, loss train:2.4218, loss val:2.4487\n",
      "step:5500, loss train:2.4234, loss val:2.4468\n",
      "step:5750, loss train:2.3948, loss val:2.4619\n",
      "step:6000, loss train:2.4127, loss val:2.4282\n",
      "step:6250, loss train:2.4272, loss val:2.4499\n",
      "step:6500, loss train:2.4076, loss val:2.4562\n",
      "step:6750, loss train:2.4035, loss val:2.4417\n",
      "step:7000, loss train:2.4035, loss val:2.4304\n",
      "step:7250, loss train:2.4046, loss val:2.4552\n",
      "step:7500, loss train:2.4133, loss val:2.4473\n",
      "step:7750, loss train:2.4054, loss val:2.4482\n",
      "step:8000, loss train:2.4066, loss val:2.4369\n",
      "step:8250, loss train:2.3836, loss val:2.4606\n",
      "step:8500, loss train:2.4053, loss val:2.4289\n",
      "step:8750, loss train:2.4011, loss val:2.4541\n",
      "step:9000, loss train:2.4204, loss val:2.4436\n",
      "step:9250, loss train:2.4047, loss val:2.4513\n",
      "step:9500, loss train:2.4170, loss val:2.4557\n",
      "step:9750, loss train:2.4125, loss val:2.4436\n",
      "batch:87\n",
      "step:0, loss train:2.4010, loss val:2.4328\n",
      "step:250, loss train:2.4214, loss val:2.4547\n",
      "step:500, loss train:2.4148, loss val:2.4675\n",
      "step:750, loss train:2.4170, loss val:2.4559\n",
      "step:1000, loss train:2.4193, loss val:2.4565\n",
      "step:1250, loss train:2.4258, loss val:2.4389\n",
      "step:1500, loss train:2.4129, loss val:2.4189\n",
      "step:1750, loss train:2.3963, loss val:2.4539\n",
      "step:2000, loss train:2.4205, loss val:2.4609\n",
      "step:2250, loss train:2.4150, loss val:2.4625\n",
      "step:2500, loss train:2.4059, loss val:2.4470\n",
      "step:2750, loss train:2.4256, loss val:2.4792\n",
      "step:3000, loss train:2.4142, loss val:2.4417\n",
      "step:3250, loss train:2.4151, loss val:2.4364\n",
      "step:3500, loss train:2.4077, loss val:2.4118\n",
      "step:3750, loss train:2.4085, loss val:2.4513\n",
      "step:4000, loss train:2.4084, loss val:2.4655\n",
      "step:4250, loss train:2.4042, loss val:2.4415\n",
      "step:4500, loss train:2.3989, loss val:2.4332\n",
      "step:4750, loss train:2.4076, loss val:2.4494\n",
      "step:5000, loss train:2.3996, loss val:2.4718\n",
      "step:5250, loss train:2.4035, loss val:2.4660\n",
      "step:5500, loss train:2.4198, loss val:2.4570\n",
      "step:5750, loss train:2.4258, loss val:2.4559\n",
      "step:6000, loss train:2.4112, loss val:2.4559\n",
      "step:6250, loss train:2.4222, loss val:2.4587\n",
      "step:6500, loss train:2.4038, loss val:2.4632\n",
      "step:6750, loss train:2.3947, loss val:2.4669\n",
      "step:7000, loss train:2.4148, loss val:2.4609\n",
      "step:7250, loss train:2.4032, loss val:2.4296\n",
      "step:7500, loss train:2.4217, loss val:2.4739\n",
      "step:7750, loss train:2.4158, loss val:2.4542\n",
      "step:8000, loss train:2.4177, loss val:2.4247\n",
      "step:8250, loss train:2.4034, loss val:2.4524\n",
      "step:8500, loss train:2.4108, loss val:2.4311\n",
      "step:8750, loss train:2.4359, loss val:2.4559\n",
      "step:9000, loss train:2.4071, loss val:2.4445\n",
      "step:9250, loss train:2.4172, loss val:2.4400\n",
      "step:9500, loss train:2.4029, loss val:2.4221\n",
      "step:9750, loss train:2.4160, loss val:2.4413\n",
      "batch:88\n",
      "step:0, loss train:2.3903, loss val:2.4435\n",
      "step:250, loss train:2.4064, loss val:2.4676\n",
      "step:500, loss train:2.4122, loss val:2.4580\n",
      "step:750, loss train:2.4116, loss val:2.4385\n",
      "step:1000, loss train:2.4339, loss val:2.4354\n",
      "step:1250, loss train:2.4196, loss val:2.4379\n",
      "step:1500, loss train:2.4174, loss val:2.4588\n",
      "step:1750, loss train:2.4044, loss val:2.4443\n",
      "step:2000, loss train:2.4212, loss val:2.4462\n",
      "step:2250, loss train:2.4057, loss val:2.4531\n",
      "step:2500, loss train:2.4056, loss val:2.4521\n",
      "step:2750, loss train:2.4269, loss val:2.4634\n",
      "step:3000, loss train:2.3857, loss val:2.4315\n",
      "step:3250, loss train:2.4255, loss val:2.4412\n",
      "step:3500, loss train:2.4046, loss val:2.4488\n",
      "step:3750, loss train:2.4162, loss val:2.4465\n",
      "step:4000, loss train:2.4234, loss val:2.4592\n",
      "step:4250, loss train:2.4099, loss val:2.4185\n",
      "step:4500, loss train:2.4149, loss val:2.4404\n",
      "step:4750, loss train:2.4289, loss val:2.4481\n",
      "step:5000, loss train:2.4118, loss val:2.4527\n",
      "step:5250, loss train:2.4164, loss val:2.4612\n",
      "step:5500, loss train:2.4112, loss val:2.4445\n",
      "step:5750, loss train:2.4252, loss val:2.4383\n",
      "step:6000, loss train:2.4066, loss val:2.4440\n",
      "step:6250, loss train:2.4204, loss val:2.4465\n",
      "step:6500, loss train:2.4136, loss val:2.4383\n",
      "step:6750, loss train:2.4219, loss val:2.4441\n",
      "step:7000, loss train:2.4010, loss val:2.4403\n",
      "step:7250, loss train:2.4048, loss val:2.4536\n",
      "step:7500, loss train:2.4222, loss val:2.4364\n",
      "step:7750, loss train:2.4182, loss val:2.4618\n",
      "step:8000, loss train:2.4320, loss val:2.4354\n",
      "step:8250, loss train:2.4059, loss val:2.4347\n",
      "step:8500, loss train:2.4269, loss val:2.4429\n",
      "step:8750, loss train:2.4256, loss val:2.4522\n",
      "step:9000, loss train:2.4067, loss val:2.4738\n",
      "step:9250, loss train:2.4017, loss val:2.4326\n",
      "step:9500, loss train:2.3954, loss val:2.4402\n",
      "step:9750, loss train:2.4055, loss val:2.4679\n",
      "batch:89\n",
      "step:0, loss train:2.4138, loss val:2.4599\n",
      "step:250, loss train:2.4062, loss val:2.4384\n",
      "step:500, loss train:2.3986, loss val:2.4427\n",
      "step:750, loss train:2.4172, loss val:2.4532\n",
      "step:1000, loss train:2.4087, loss val:2.4370\n",
      "step:1250, loss train:2.4151, loss val:2.4447\n",
      "step:1500, loss train:2.4011, loss val:2.4468\n",
      "step:1750, loss train:2.3883, loss val:2.4518\n",
      "step:2000, loss train:2.4149, loss val:2.4598\n",
      "step:2250, loss train:2.3932, loss val:2.4546\n",
      "step:2500, loss train:2.4055, loss val:2.4431\n",
      "step:2750, loss train:2.4402, loss val:2.4290\n",
      "step:3000, loss train:2.4266, loss val:2.4560\n",
      "step:3250, loss train:2.4205, loss val:2.4363\n",
      "step:3500, loss train:2.4440, loss val:2.4306\n",
      "step:3750, loss train:2.4008, loss val:2.4472\n",
      "step:4000, loss train:2.4327, loss val:2.4313\n",
      "step:4250, loss train:2.4172, loss val:2.4469\n",
      "step:4500, loss train:2.4259, loss val:2.4433\n",
      "step:4750, loss train:2.4172, loss val:2.4211\n",
      "step:5000, loss train:2.4119, loss val:2.4378\n",
      "step:5250, loss train:2.4022, loss val:2.4396\n",
      "step:5500, loss train:2.4172, loss val:2.4544\n",
      "step:5750, loss train:2.3933, loss val:2.4411\n",
      "step:6000, loss train:2.4305, loss val:2.4604\n",
      "step:6250, loss train:2.4057, loss val:2.4765\n",
      "step:6500, loss train:2.4004, loss val:2.4420\n",
      "step:6750, loss train:2.4201, loss val:2.4616\n",
      "step:7000, loss train:2.4054, loss val:2.4314\n",
      "step:7250, loss train:2.3993, loss val:2.4492\n",
      "step:7500, loss train:2.4019, loss val:2.4408\n",
      "step:7750, loss train:2.4019, loss val:2.4686\n",
      "step:8000, loss train:2.3976, loss val:2.4570\n",
      "step:8250, loss train:2.4092, loss val:2.4318\n",
      "step:8500, loss train:2.4113, loss val:2.4471\n",
      "step:8750, loss train:2.4245, loss val:2.4312\n",
      "step:9000, loss train:2.4152, loss val:2.4511\n",
      "step:9250, loss train:2.4073, loss val:2.4376\n",
      "step:9500, loss train:2.4130, loss val:2.4576\n",
      "step:9750, loss train:2.4101, loss val:2.4357\n",
      "batch:90\n",
      "step:0, loss train:2.3893, loss val:2.4425\n",
      "step:250, loss train:2.4161, loss val:2.4505\n",
      "step:500, loss train:2.4154, loss val:2.4612\n",
      "step:750, loss train:2.4167, loss val:2.4170\n",
      "step:1000, loss train:2.3867, loss val:2.4442\n",
      "step:1250, loss train:2.4063, loss val:2.4568\n",
      "step:1500, loss train:2.4054, loss val:2.4654\n",
      "step:1750, loss train:2.4253, loss val:2.4504\n",
      "step:2000, loss train:2.4073, loss val:2.4392\n",
      "step:2250, loss train:2.4248, loss val:2.4667\n",
      "step:2500, loss train:2.4109, loss val:2.4712\n",
      "step:2750, loss train:2.3975, loss val:2.4617\n",
      "step:3000, loss train:2.4153, loss val:2.4723\n",
      "step:3250, loss train:2.4204, loss val:2.4542\n",
      "step:3500, loss train:2.4225, loss val:2.4728\n",
      "step:3750, loss train:2.3970, loss val:2.4277\n",
      "step:4000, loss train:2.4100, loss val:2.4529\n",
      "step:4250, loss train:2.3954, loss val:2.4620\n",
      "step:4500, loss train:2.4033, loss val:2.4408\n",
      "step:4750, loss train:2.4262, loss val:2.4582\n",
      "step:5000, loss train:2.3830, loss val:2.4398\n",
      "step:5250, loss train:2.4122, loss val:2.4734\n",
      "step:5500, loss train:2.4279, loss val:2.4349\n",
      "step:5750, loss train:2.4226, loss val:2.4445\n",
      "step:6000, loss train:2.4063, loss val:2.4541\n",
      "step:6250, loss train:2.4063, loss val:2.4416\n",
      "step:6500, loss train:2.4123, loss val:2.4776\n",
      "step:6750, loss train:2.4050, loss val:2.4560\n",
      "step:7000, loss train:2.4130, loss val:2.4678\n",
      "step:7250, loss train:2.4047, loss val:2.4582\n",
      "step:7500, loss train:2.4011, loss val:2.4371\n",
      "step:7750, loss train:2.4193, loss val:2.4518\n",
      "step:8000, loss train:2.4133, loss val:2.4212\n",
      "step:8250, loss train:2.4202, loss val:2.4225\n",
      "step:8500, loss train:2.4036, loss val:2.4237\n",
      "step:8750, loss train:2.4216, loss val:2.4459\n",
      "step:9000, loss train:2.4116, loss val:2.4467\n",
      "step:9250, loss train:2.4191, loss val:2.4591\n",
      "step:9500, loss train:2.4146, loss val:2.4383\n",
      "step:9750, loss train:2.3931, loss val:2.4525\n",
      "batch:91\n",
      "step:0, loss train:2.4264, loss val:2.4420\n",
      "step:250, loss train:2.4090, loss val:2.4541\n",
      "step:500, loss train:2.4287, loss val:2.4620\n",
      "step:750, loss train:2.4118, loss val:2.4371\n",
      "step:1000, loss train:2.4074, loss val:2.4177\n",
      "step:1250, loss train:2.4195, loss val:2.4504\n",
      "step:1500, loss train:2.4147, loss val:2.4431\n",
      "step:1750, loss train:2.4032, loss val:2.4286\n",
      "step:2000, loss train:2.4237, loss val:2.4242\n",
      "step:2250, loss train:2.4217, loss val:2.4639\n",
      "step:2500, loss train:2.4286, loss val:2.4830\n",
      "step:2750, loss train:2.4035, loss val:2.4494\n",
      "step:3000, loss train:2.4228, loss val:2.4479\n",
      "step:3250, loss train:2.4183, loss val:2.4299\n",
      "step:3500, loss train:2.4016, loss val:2.4566\n",
      "step:3750, loss train:2.4109, loss val:2.4436\n",
      "step:4000, loss train:2.4109, loss val:2.4470\n",
      "step:4250, loss train:2.4207, loss val:2.4576\n",
      "step:4500, loss train:2.4025, loss val:2.4627\n",
      "step:4750, loss train:2.4202, loss val:2.4615\n",
      "step:5000, loss train:2.4156, loss val:2.4383\n",
      "step:5250, loss train:2.4171, loss val:2.4318\n",
      "step:5500, loss train:2.4054, loss val:2.4431\n",
      "step:5750, loss train:2.4097, loss val:2.4462\n",
      "step:6000, loss train:2.4081, loss val:2.4383\n",
      "step:6250, loss train:2.4215, loss val:2.4512\n",
      "step:6500, loss train:2.4012, loss val:2.4588\n",
      "step:6750, loss train:2.3937, loss val:2.4734\n",
      "step:7000, loss train:2.3991, loss val:2.4485\n",
      "step:7250, loss train:2.4358, loss val:2.4506\n",
      "step:7500, loss train:2.4216, loss val:2.4659\n",
      "step:7750, loss train:2.4111, loss val:2.4543\n",
      "step:8000, loss train:2.3762, loss val:2.4483\n",
      "step:8250, loss train:2.4028, loss val:2.4343\n",
      "step:8500, loss train:2.4321, loss val:2.4415\n",
      "step:8750, loss train:2.4081, loss val:2.4461\n",
      "step:9000, loss train:2.4058, loss val:2.4565\n",
      "step:9250, loss train:2.4324, loss val:2.4298\n",
      "step:9500, loss train:2.4237, loss val:2.4355\n",
      "step:9750, loss train:2.3950, loss val:2.4516\n",
      "batch:92\n",
      "step:0, loss train:2.4092, loss val:2.4602\n",
      "step:250, loss train:2.4278, loss val:2.4634\n",
      "step:500, loss train:2.4089, loss val:2.4185\n",
      "step:750, loss train:2.3995, loss val:2.4386\n",
      "step:1000, loss train:2.3973, loss val:2.4505\n",
      "step:1250, loss train:2.4199, loss val:2.4337\n",
      "step:1500, loss train:2.4212, loss val:2.4567\n",
      "step:1750, loss train:2.4094, loss val:2.4423\n",
      "step:2000, loss train:2.4053, loss val:2.4340\n",
      "step:2250, loss train:2.4080, loss val:2.4596\n",
      "step:2500, loss train:2.4174, loss val:2.4271\n",
      "step:2750, loss train:2.4326, loss val:2.4488\n",
      "step:3000, loss train:2.4119, loss val:2.4385\n",
      "step:3250, loss train:2.4035, loss val:2.4467\n",
      "step:3500, loss train:2.4069, loss val:2.4324\n",
      "step:3750, loss train:2.4201, loss val:2.4377\n",
      "step:4000, loss train:2.4084, loss val:2.4526\n",
      "step:4250, loss train:2.3997, loss val:2.4496\n",
      "step:4500, loss train:2.4007, loss val:2.4346\n",
      "step:4750, loss train:2.4074, loss val:2.4455\n",
      "step:5000, loss train:2.3966, loss val:2.4354\n",
      "step:5250, loss train:2.4153, loss val:2.4433\n",
      "step:5500, loss train:2.4046, loss val:2.4347\n",
      "step:5750, loss train:2.3928, loss val:2.4241\n",
      "step:6000, loss train:2.4298, loss val:2.4602\n",
      "step:6250, loss train:2.4198, loss val:2.4480\n",
      "step:6500, loss train:2.4234, loss val:2.4509\n",
      "step:6750, loss train:2.4232, loss val:2.4264\n",
      "step:7000, loss train:2.4227, loss val:2.4534\n",
      "step:7250, loss train:2.4361, loss val:2.4562\n",
      "step:7500, loss train:2.4211, loss val:2.4219\n",
      "step:7750, loss train:2.4146, loss val:2.4303\n",
      "step:8000, loss train:2.4100, loss val:2.4514\n",
      "step:8250, loss train:2.4101, loss val:2.4505\n",
      "step:8500, loss train:2.4277, loss val:2.4430\n",
      "step:8750, loss train:2.4253, loss val:2.4578\n",
      "step:9000, loss train:2.4026, loss val:2.4432\n",
      "step:9250, loss train:2.4146, loss val:2.4533\n",
      "step:9500, loss train:2.4201, loss val:2.4549\n",
      "step:9750, loss train:2.4032, loss val:2.4505\n",
      "batch:93\n",
      "step:0, loss train:2.4023, loss val:2.4259\n",
      "step:250, loss train:2.4025, loss val:2.4294\n",
      "step:500, loss train:2.4175, loss val:2.4321\n",
      "step:750, loss train:2.4178, loss val:2.4349\n",
      "step:1000, loss train:2.4316, loss val:2.4459\n",
      "step:1250, loss train:2.4181, loss val:2.4407\n",
      "step:1500, loss train:2.4402, loss val:2.4487\n",
      "step:1750, loss train:2.4173, loss val:2.4268\n",
      "step:2000, loss train:2.4211, loss val:2.4611\n",
      "step:2250, loss train:2.4073, loss val:2.4216\n",
      "step:2500, loss train:2.3977, loss val:2.4394\n",
      "step:2750, loss train:2.4353, loss val:2.4434\n",
      "step:3000, loss train:2.4163, loss val:2.4394\n",
      "step:3250, loss train:2.4261, loss val:2.4370\n",
      "step:3500, loss train:2.4006, loss val:2.4457\n",
      "step:3750, loss train:2.4088, loss val:2.4659\n",
      "step:4000, loss train:2.4068, loss val:2.4602\n",
      "step:4250, loss train:2.4047, loss val:2.4508\n",
      "step:4500, loss train:2.4146, loss val:2.4485\n",
      "step:4750, loss train:2.4325, loss val:2.4535\n",
      "step:5000, loss train:2.4162, loss val:2.4350\n",
      "step:5250, loss train:2.4099, loss val:2.4309\n",
      "step:5500, loss train:2.4134, loss val:2.4523\n",
      "step:5750, loss train:2.4102, loss val:2.4443\n",
      "step:6000, loss train:2.4193, loss val:2.4467\n",
      "step:6250, loss train:2.4171, loss val:2.4408\n",
      "step:6500, loss train:2.3908, loss val:2.4603\n",
      "step:6750, loss train:2.4188, loss val:2.4315\n",
      "step:7000, loss train:2.4059, loss val:2.4590\n",
      "step:7250, loss train:2.3962, loss val:2.4500\n",
      "step:7500, loss train:2.4218, loss val:2.4703\n",
      "step:7750, loss train:2.3985, loss val:2.4316\n",
      "step:8000, loss train:2.4117, loss val:2.4605\n",
      "step:8250, loss train:2.4084, loss val:2.4419\n",
      "step:8500, loss train:2.4036, loss val:2.4367\n",
      "step:8750, loss train:2.4082, loss val:2.4239\n",
      "step:9000, loss train:2.3937, loss val:2.4394\n",
      "step:9250, loss train:2.4116, loss val:2.4409\n",
      "step:9500, loss train:2.4036, loss val:2.4421\n",
      "step:9750, loss train:2.4251, loss val:2.4613\n",
      "batch:94\n",
      "step:0, loss train:2.4074, loss val:2.4257\n",
      "step:250, loss train:2.4194, loss val:2.4390\n",
      "step:500, loss train:2.4068, loss val:2.4521\n",
      "step:750, loss train:2.4106, loss val:2.4561\n",
      "step:1000, loss train:2.4210, loss val:2.4387\n",
      "step:1250, loss train:2.4003, loss val:2.4403\n",
      "step:1500, loss train:2.4056, loss val:2.4474\n",
      "step:1750, loss train:2.4010, loss val:2.4484\n",
      "step:2000, loss train:2.4201, loss val:2.4255\n",
      "step:2250, loss train:2.3952, loss val:2.4266\n",
      "step:2500, loss train:2.4268, loss val:2.4458\n",
      "step:2750, loss train:2.4097, loss val:2.4410\n",
      "step:3000, loss train:2.4177, loss val:2.4871\n",
      "step:3250, loss train:2.3982, loss val:2.4420\n",
      "step:3500, loss train:2.4054, loss val:2.4593\n",
      "step:3750, loss train:2.4008, loss val:2.4396\n",
      "step:4000, loss train:2.4012, loss val:2.4631\n",
      "step:4250, loss train:2.4107, loss val:2.4257\n",
      "step:4500, loss train:2.4141, loss val:2.4320\n",
      "step:4750, loss train:2.4146, loss val:2.4492\n",
      "step:5000, loss train:2.4192, loss val:2.4632\n",
      "step:5250, loss train:2.3898, loss val:2.4334\n",
      "step:5500, loss train:2.4315, loss val:2.4503\n",
      "step:5750, loss train:2.4111, loss val:2.4600\n",
      "step:6000, loss train:2.4154, loss val:2.4291\n",
      "step:6250, loss train:2.4082, loss val:2.4406\n",
      "step:6500, loss train:2.4098, loss val:2.4241\n",
      "step:6750, loss train:2.4077, loss val:2.4699\n",
      "step:7000, loss train:2.4227, loss val:2.4398\n",
      "step:7250, loss train:2.4074, loss val:2.4487\n",
      "step:7500, loss train:2.4057, loss val:2.4491\n",
      "step:7750, loss train:2.4181, loss val:2.4486\n",
      "step:8000, loss train:2.4229, loss val:2.4538\n",
      "step:8250, loss train:2.3779, loss val:2.4490\n",
      "step:8500, loss train:2.4069, loss val:2.4374\n",
      "step:8750, loss train:2.4111, loss val:2.4430\n",
      "step:9000, loss train:2.4212, loss val:2.4460\n",
      "step:9250, loss train:2.4064, loss val:2.4244\n",
      "step:9500, loss train:2.4107, loss val:2.4123\n",
      "step:9750, loss train:2.4207, loss val:2.4536\n",
      "batch:95\n",
      "step:0, loss train:2.4080, loss val:2.4223\n",
      "step:250, loss train:2.4165, loss val:2.4573\n",
      "step:500, loss train:2.4206, loss val:2.4350\n",
      "step:750, loss train:2.4180, loss val:2.4733\n",
      "step:1000, loss train:2.4204, loss val:2.4362\n",
      "step:1250, loss train:2.4256, loss val:2.4432\n",
      "step:1500, loss train:2.3978, loss val:2.4324\n",
      "step:1750, loss train:2.4131, loss val:2.4480\n",
      "step:2000, loss train:2.3986, loss val:2.4385\n",
      "step:2250, loss train:2.4140, loss val:2.4507\n",
      "step:2500, loss train:2.4096, loss val:2.4424\n",
      "step:2750, loss train:2.4080, loss val:2.4247\n",
      "step:3000, loss train:2.3859, loss val:2.4449\n",
      "step:3250, loss train:2.4017, loss val:2.4538\n",
      "step:3500, loss train:2.4184, loss val:2.4540\n",
      "step:3750, loss train:2.4011, loss val:2.4495\n",
      "step:4000, loss train:2.4171, loss val:2.4650\n",
      "step:4250, loss train:2.4178, loss val:2.4246\n",
      "step:4500, loss train:2.4289, loss val:2.4691\n",
      "step:4750, loss train:2.4130, loss val:2.4311\n",
      "step:5000, loss train:2.4229, loss val:2.4312\n",
      "step:5250, loss train:2.4050, loss val:2.4356\n",
      "step:5500, loss train:2.3998, loss val:2.4240\n",
      "step:5750, loss train:2.4063, loss val:2.4532\n",
      "step:6000, loss train:2.4140, loss val:2.4455\n",
      "step:6250, loss train:2.4120, loss val:2.4683\n",
      "step:6500, loss train:2.4138, loss val:2.4380\n",
      "step:6750, loss train:2.4021, loss val:2.4449\n",
      "step:7000, loss train:2.4125, loss val:2.4410\n",
      "step:7250, loss train:2.4075, loss val:2.4731\n",
      "step:7500, loss train:2.4101, loss val:2.4326\n",
      "step:7750, loss train:2.4196, loss val:2.4553\n",
      "step:8000, loss train:2.4015, loss val:2.4527\n",
      "step:8250, loss train:2.4286, loss val:2.4613\n",
      "step:8500, loss train:2.3935, loss val:2.4508\n",
      "step:8750, loss train:2.4164, loss val:2.4524\n",
      "step:9000, loss train:2.4393, loss val:2.4496\n",
      "step:9250, loss train:2.4170, loss val:2.4303\n",
      "step:9500, loss train:2.4036, loss val:2.4511\n",
      "step:9750, loss train:2.4176, loss val:2.4280\n",
      "batch:96\n",
      "step:0, loss train:2.4241, loss val:2.4463\n",
      "step:250, loss train:2.4436, loss val:2.4606\n",
      "step:500, loss train:2.4083, loss val:2.4703\n",
      "step:750, loss train:2.4015, loss val:2.4664\n",
      "step:1000, loss train:2.4161, loss val:2.4469\n",
      "step:1250, loss train:2.4232, loss val:2.4226\n",
      "step:1500, loss train:2.4309, loss val:2.4272\n",
      "step:1750, loss train:2.4024, loss val:2.4410\n",
      "step:2000, loss train:2.4095, loss val:2.4271\n",
      "step:2250, loss train:2.4157, loss val:2.4513\n",
      "step:2500, loss train:2.4222, loss val:2.4337\n",
      "step:2750, loss train:2.4194, loss val:2.4288\n",
      "step:3000, loss train:2.4053, loss val:2.4382\n",
      "step:3250, loss train:2.4049, loss val:2.4371\n",
      "step:3500, loss train:2.4073, loss val:2.4373\n",
      "step:3750, loss train:2.4203, loss val:2.4419\n",
      "step:4000, loss train:2.4087, loss val:2.4495\n",
      "step:4250, loss train:2.4112, loss val:2.4422\n",
      "step:4500, loss train:2.4218, loss val:2.4485\n",
      "step:4750, loss train:2.4400, loss val:2.4487\n",
      "step:5000, loss train:2.4007, loss val:2.4365\n",
      "step:5250, loss train:2.4127, loss val:2.4431\n",
      "step:5500, loss train:2.4025, loss val:2.4445\n",
      "step:5750, loss train:2.4177, loss val:2.4273\n",
      "step:6000, loss train:2.4350, loss val:2.4627\n",
      "step:6250, loss train:2.4141, loss val:2.4645\n",
      "step:6500, loss train:2.4196, loss val:2.4373\n",
      "step:6750, loss train:2.4149, loss val:2.4528\n",
      "step:7000, loss train:2.4157, loss val:2.4349\n",
      "step:7250, loss train:2.4066, loss val:2.4265\n",
      "step:7500, loss train:2.4242, loss val:2.4323\n",
      "step:7750, loss train:2.4192, loss val:2.4387\n",
      "step:8000, loss train:2.4103, loss val:2.4335\n",
      "step:8250, loss train:2.4124, loss val:2.4340\n",
      "step:8500, loss train:2.4234, loss val:2.4522\n",
      "step:8750, loss train:2.4158, loss val:2.4395\n",
      "step:9000, loss train:2.4130, loss val:2.4572\n",
      "step:9250, loss train:2.4192, loss val:2.4362\n",
      "step:9500, loss train:2.4108, loss val:2.4487\n",
      "step:9750, loss train:2.4170, loss val:2.4413\n",
      "batch:97\n",
      "step:0, loss train:2.4222, loss val:2.4453\n",
      "step:250, loss train:2.4095, loss val:2.4437\n",
      "step:500, loss train:2.4266, loss val:2.4165\n",
      "step:750, loss train:2.4019, loss val:2.4322\n",
      "step:1000, loss train:2.4002, loss val:2.4363\n",
      "step:1250, loss train:2.4105, loss val:2.4570\n",
      "step:1500, loss train:2.3883, loss val:2.4335\n",
      "step:1750, loss train:2.4122, loss val:2.4494\n",
      "step:2000, loss train:2.4144, loss val:2.4758\n",
      "step:2250, loss train:2.4018, loss val:2.4517\n",
      "step:2500, loss train:2.3986, loss val:2.4632\n",
      "step:2750, loss train:2.4076, loss val:2.4500\n",
      "step:3000, loss train:2.4102, loss val:2.4823\n",
      "step:3250, loss train:2.4025, loss val:2.4378\n",
      "step:3500, loss train:2.4199, loss val:2.4271\n",
      "step:3750, loss train:2.4147, loss val:2.4753\n",
      "step:4000, loss train:2.4004, loss val:2.4515\n",
      "step:4250, loss train:2.4151, loss val:2.4588\n",
      "step:4500, loss train:2.4194, loss val:2.4384\n",
      "step:4750, loss train:2.4041, loss val:2.4325\n",
      "step:5000, loss train:2.4168, loss val:2.4111\n",
      "step:5250, loss train:2.4253, loss val:2.4348\n",
      "step:5500, loss train:2.4121, loss val:2.4671\n",
      "step:5750, loss train:2.4007, loss val:2.4599\n",
      "step:6000, loss train:2.4042, loss val:2.4396\n",
      "step:6250, loss train:2.4071, loss val:2.4653\n",
      "step:6500, loss train:2.4014, loss val:2.4562\n",
      "step:6750, loss train:2.4109, loss val:2.4514\n",
      "step:7000, loss train:2.3986, loss val:2.4499\n",
      "step:7250, loss train:2.4134, loss val:2.4589\n",
      "step:7500, loss train:2.4179, loss val:2.4363\n",
      "step:7750, loss train:2.4110, loss val:2.4505\n",
      "step:8000, loss train:2.4055, loss val:2.4213\n",
      "step:8250, loss train:2.4162, loss val:2.4424\n",
      "step:8500, loss train:2.4072, loss val:2.4334\n",
      "step:8750, loss train:2.4080, loss val:2.4454\n",
      "step:9000, loss train:2.4067, loss val:2.4350\n",
      "step:9250, loss train:2.3914, loss val:2.4551\n",
      "step:9500, loss train:2.4001, loss val:2.4558\n",
      "step:9750, loss train:2.4261, loss val:2.4788\n",
      "batch:98\n",
      "step:0, loss train:2.4095, loss val:2.4423\n",
      "step:250, loss train:2.4282, loss val:2.4630\n",
      "step:500, loss train:2.4153, loss val:2.4622\n",
      "step:750, loss train:2.4081, loss val:2.4478\n",
      "step:1000, loss train:2.4041, loss val:2.4354\n",
      "step:1250, loss train:2.4160, loss val:2.4515\n",
      "step:1500, loss train:2.4202, loss val:2.4566\n",
      "step:1750, loss train:2.4044, loss val:2.4335\n",
      "step:2000, loss train:2.4061, loss val:2.4308\n",
      "step:2250, loss train:2.3965, loss val:2.4579\n",
      "step:2500, loss train:2.3858, loss val:2.4408\n",
      "step:2750, loss train:2.3961, loss val:2.4610\n",
      "step:3000, loss train:2.4086, loss val:2.4521\n",
      "step:3250, loss train:2.4118, loss val:2.4296\n",
      "step:3500, loss train:2.3997, loss val:2.4254\n",
      "step:3750, loss train:2.4141, loss val:2.4387\n",
      "step:4000, loss train:2.4086, loss val:2.4523\n",
      "step:4250, loss train:2.3953, loss val:2.4470\n",
      "step:4500, loss train:2.4201, loss val:2.4477\n",
      "step:4750, loss train:2.4289, loss val:2.4615\n",
      "step:5000, loss train:2.4206, loss val:2.4562\n",
      "step:5250, loss train:2.4143, loss val:2.4614\n",
      "step:5500, loss train:2.4122, loss val:2.4406\n",
      "step:5750, loss train:2.4064, loss val:2.4415\n",
      "step:6000, loss train:2.4123, loss val:2.4369\n",
      "step:6250, loss train:2.4025, loss val:2.4641\n",
      "step:6500, loss train:2.4092, loss val:2.4533\n",
      "step:6750, loss train:2.4055, loss val:2.4345\n",
      "step:7000, loss train:2.4249, loss val:2.4564\n",
      "step:7250, loss train:2.4035, loss val:2.4473\n",
      "step:7500, loss train:2.4142, loss val:2.4315\n",
      "step:7750, loss train:2.3962, loss val:2.4569\n",
      "step:8000, loss train:2.4279, loss val:2.4592\n",
      "step:8250, loss train:2.4117, loss val:2.4493\n",
      "step:8500, loss train:2.4131, loss val:2.4491\n",
      "step:8750, loss train:2.4152, loss val:2.4436\n",
      "step:9000, loss train:2.4220, loss val:2.4579\n",
      "step:9250, loss train:2.4166, loss val:2.4667\n",
      "step:9500, loss train:2.4229, loss val:2.4272\n",
      "step:9750, loss train:2.3976, loss val:2.4502\n",
      "batch:99\n",
      "step:0, loss train:2.4124, loss val:2.4540\n",
      "step:250, loss train:2.4129, loss val:2.4560\n",
      "step:500, loss train:2.4177, loss val:2.4348\n",
      "step:750, loss train:2.4086, loss val:2.4309\n",
      "step:1000, loss train:2.4187, loss val:2.4534\n",
      "step:1250, loss train:2.4122, loss val:2.4374\n",
      "step:1500, loss train:2.4179, loss val:2.4474\n",
      "step:1750, loss train:2.3969, loss val:2.4377\n",
      "step:2000, loss train:2.4129, loss val:2.4374\n",
      "step:2250, loss train:2.4168, loss val:2.4363\n",
      "step:2500, loss train:2.4109, loss val:2.4454\n",
      "step:2750, loss train:2.4157, loss val:2.4604\n",
      "step:3000, loss train:2.4246, loss val:2.4525\n",
      "step:3250, loss train:2.4006, loss val:2.4356\n",
      "step:3500, loss train:2.4225, loss val:2.4624\n",
      "step:3750, loss train:2.4142, loss val:2.4536\n",
      "step:4000, loss train:2.4104, loss val:2.4665\n",
      "step:4250, loss train:2.4154, loss val:2.4173\n",
      "step:4500, loss train:2.4130, loss val:2.4316\n",
      "step:4750, loss train:2.4166, loss val:2.4391\n",
      "step:5000, loss train:2.4104, loss val:2.4624\n",
      "step:5250, loss train:2.4211, loss val:2.4388\n",
      "step:5500, loss train:2.4138, loss val:2.4394\n",
      "step:5750, loss train:2.4032, loss val:2.4546\n",
      "step:6000, loss train:2.4171, loss val:2.4314\n",
      "step:6250, loss train:2.4237, loss val:2.4556\n",
      "step:6500, loss train:2.4139, loss val:2.4516\n",
      "step:6750, loss train:2.4098, loss val:2.4772\n",
      "step:7000, loss train:2.4052, loss val:2.4770\n",
      "step:7250, loss train:2.4039, loss val:2.4328\n",
      "step:7500, loss train:2.4242, loss val:2.4400\n",
      "step:7750, loss train:2.4305, loss val:2.4497\n",
      "step:8000, loss train:2.3991, loss val:2.4337\n",
      "step:8250, loss train:2.4098, loss val:2.4252\n",
      "step:8500, loss train:2.4209, loss val:2.4274\n",
      "step:8750, loss train:2.4223, loss val:2.4475\n",
      "step:9000, loss train:2.4115, loss val:2.4545\n",
      "step:9250, loss train:2.3984, loss val:2.4607\n",
      "step:9500, loss train:2.4023, loss val:2.4417\n",
      "step:9750, loss train:2.4099, loss val:2.4365\n",
      "batch:100\n",
      "step:0, loss train:2.4214, loss val:2.4001\n",
      "step:250, loss train:2.3957, loss val:2.4263\n",
      "step:500, loss train:2.4049, loss val:2.4362\n",
      "step:750, loss train:2.4196, loss val:2.4506\n",
      "step:1000, loss train:2.4196, loss val:2.4411\n",
      "step:1250, loss train:2.4198, loss val:2.4432\n",
      "step:1500, loss train:2.4171, loss val:2.4455\n",
      "step:1750, loss train:2.4234, loss val:2.4425\n",
      "step:2000, loss train:2.4081, loss val:2.4461\n",
      "step:2250, loss train:2.4167, loss val:2.4247\n",
      "step:2500, loss train:2.4048, loss val:2.4377\n",
      "step:2750, loss train:2.4025, loss val:2.4570\n",
      "step:3000, loss train:2.4082, loss val:2.4299\n",
      "step:3250, loss train:2.4090, loss val:2.4379\n",
      "step:3500, loss train:2.3940, loss val:2.4705\n",
      "step:3750, loss train:2.4430, loss val:2.4371\n",
      "step:4000, loss train:2.4144, loss val:2.4234\n",
      "step:4250, loss train:2.4053, loss val:2.4266\n",
      "step:4500, loss train:2.4144, loss val:2.4229\n",
      "step:4750, loss train:2.3889, loss val:2.4359\n",
      "step:5000, loss train:2.4095, loss val:2.4385\n",
      "step:5250, loss train:2.3993, loss val:2.4506\n",
      "step:5500, loss train:2.4177, loss val:2.4353\n",
      "step:5750, loss train:2.4168, loss val:2.4509\n",
      "step:6000, loss train:2.4082, loss val:2.4406\n",
      "step:6250, loss train:2.4021, loss val:2.4503\n",
      "step:6500, loss train:2.3991, loss val:2.4417\n",
      "step:6750, loss train:2.4066, loss val:2.4604\n",
      "step:7000, loss train:2.4196, loss val:2.4434\n",
      "step:7250, loss train:2.4218, loss val:2.4450\n",
      "step:7500, loss train:2.4085, loss val:2.4624\n",
      "step:7750, loss train:2.4099, loss val:2.4634\n",
      "step:8000, loss train:2.4188, loss val:2.4579\n",
      "step:8250, loss train:2.4343, loss val:2.4472\n",
      "step:8500, loss train:2.3980, loss val:2.4494\n",
      "step:8750, loss train:2.4190, loss val:2.4302\n",
      "step:9000, loss train:2.3889, loss val:2.4534\n",
      "step:9250, loss train:2.4068, loss val:2.4407\n",
      "step:9500, loss train:2.4176, loss val:2.4391\n",
      "step:9750, loss train:2.4058, loss val:2.4618\n",
      "batch:101\n",
      "step:0, loss train:2.4023, loss val:2.4590\n",
      "step:250, loss train:2.4344, loss val:2.4582\n",
      "step:500, loss train:2.4145, loss val:2.4390\n",
      "step:750, loss train:2.4273, loss val:2.4356\n",
      "step:1000, loss train:2.4248, loss val:2.4392\n",
      "step:1250, loss train:2.4263, loss val:2.4494\n",
      "step:1500, loss train:2.4149, loss val:2.4170\n",
      "step:1750, loss train:2.4274, loss val:2.4161\n",
      "step:2000, loss train:2.4121, loss val:2.4574\n",
      "step:2250, loss train:2.4032, loss val:2.4436\n",
      "step:2500, loss train:2.4024, loss val:2.4394\n",
      "step:2750, loss train:2.4001, loss val:2.4375\n",
      "step:3000, loss train:2.4293, loss val:2.4464\n",
      "step:3250, loss train:2.3874, loss val:2.4454\n",
      "step:3500, loss train:2.4078, loss val:2.4437\n",
      "step:3750, loss train:2.4202, loss val:2.4496\n",
      "step:4000, loss train:2.4153, loss val:2.4377\n",
      "step:4250, loss train:2.4113, loss val:2.4301\n",
      "step:4500, loss train:2.3999, loss val:2.4191\n",
      "step:4750, loss train:2.4203, loss val:2.4426\n",
      "step:5000, loss train:2.4044, loss val:2.4459\n",
      "step:5250, loss train:2.4094, loss val:2.4811\n",
      "step:5500, loss train:2.3857, loss val:2.4529\n",
      "step:5750, loss train:2.4104, loss val:2.4462\n",
      "step:6000, loss train:2.4135, loss val:2.4626\n",
      "step:6250, loss train:2.4189, loss val:2.4568\n",
      "step:6500, loss train:2.4118, loss val:2.4686\n",
      "step:6750, loss train:2.4249, loss val:2.4522\n",
      "step:7000, loss train:2.4165, loss val:2.4524\n",
      "step:7250, loss train:2.4185, loss val:2.4178\n",
      "step:7500, loss train:2.4010, loss val:2.4446\n",
      "step:7750, loss train:2.4265, loss val:2.4513\n",
      "step:8000, loss train:2.4103, loss val:2.4453\n",
      "step:8250, loss train:2.4308, loss val:2.4636\n",
      "step:8500, loss train:2.3991, loss val:2.4326\n",
      "step:8750, loss train:2.4035, loss val:2.4580\n",
      "step:9000, loss train:2.3994, loss val:2.4384\n",
      "step:9250, loss train:2.4114, loss val:2.4533\n",
      "step:9500, loss train:2.4169, loss val:2.4707\n",
      "step:9750, loss train:2.4206, loss val:2.4339\n",
      "batch:102\n",
      "step:0, loss train:2.4070, loss val:2.4443\n",
      "step:250, loss train:2.4346, loss val:2.4503\n",
      "step:500, loss train:2.4022, loss val:2.4525\n",
      "step:750, loss train:2.4026, loss val:2.4368\n",
      "step:1000, loss train:2.4278, loss val:2.4520\n",
      "step:1250, loss train:2.4274, loss val:2.4593\n",
      "step:1500, loss train:2.4272, loss val:2.4580\n",
      "step:1750, loss train:2.4092, loss val:2.4278\n",
      "step:2000, loss train:2.4305, loss val:2.4239\n",
      "step:2250, loss train:2.4163, loss val:2.4333\n",
      "step:2500, loss train:2.4075, loss val:2.4557\n",
      "step:2750, loss train:2.4119, loss val:2.4166\n",
      "step:3000, loss train:2.4152, loss val:2.4547\n",
      "step:3250, loss train:2.4331, loss val:2.4530\n",
      "step:3500, loss train:2.4127, loss val:2.4486\n",
      "step:3750, loss train:2.4151, loss val:2.4583\n",
      "step:4000, loss train:2.4173, loss val:2.4450\n",
      "step:4250, loss train:2.4036, loss val:2.4425\n",
      "step:4500, loss train:2.4063, loss val:2.4551\n",
      "step:4750, loss train:2.4278, loss val:2.4358\n",
      "step:5000, loss train:2.3990, loss val:2.4335\n",
      "step:5250, loss train:2.4099, loss val:2.4532\n",
      "step:5500, loss train:2.4102, loss val:2.4604\n",
      "step:5750, loss train:2.4150, loss val:2.4564\n",
      "step:6000, loss train:2.4187, loss val:2.4482\n",
      "step:6250, loss train:2.4157, loss val:2.4477\n",
      "step:6500, loss train:2.4145, loss val:2.4370\n",
      "step:6750, loss train:2.4100, loss val:2.4587\n",
      "step:7000, loss train:2.4141, loss val:2.4344\n",
      "step:7250, loss train:2.4041, loss val:2.4337\n",
      "step:7500, loss train:2.4006, loss val:2.4278\n",
      "step:7750, loss train:2.4109, loss val:2.4450\n",
      "step:8000, loss train:2.4149, loss val:2.4384\n",
      "step:8250, loss train:2.4225, loss val:2.4554\n",
      "step:8500, loss train:2.4119, loss val:2.4303\n",
      "step:8750, loss train:2.4068, loss val:2.4502\n",
      "step:9000, loss train:2.4070, loss val:2.4479\n",
      "step:9250, loss train:2.4356, loss val:2.4523\n",
      "step:9500, loss train:2.4218, loss val:2.4265\n",
      "step:9750, loss train:2.4123, loss val:2.4383\n",
      "batch:103\n",
      "step:0, loss train:2.3950, loss val:2.4695\n",
      "step:250, loss train:2.4191, loss val:2.4579\n",
      "step:500, loss train:2.4188, loss val:2.4457\n",
      "step:750, loss train:2.4253, loss val:2.4399\n",
      "step:1000, loss train:2.3937, loss val:2.4405\n",
      "step:1250, loss train:2.4108, loss val:2.4603\n",
      "step:1500, loss train:2.4075, loss val:2.4621\n",
      "step:1750, loss train:2.4286, loss val:2.4628\n",
      "step:2000, loss train:2.4077, loss val:2.4348\n",
      "step:2250, loss train:2.4130, loss val:2.4397\n",
      "step:2500, loss train:2.4012, loss val:2.4605\n",
      "step:2750, loss train:2.4237, loss val:2.4629\n",
      "step:3000, loss train:2.4178, loss val:2.4203\n",
      "step:3250, loss train:2.3927, loss val:2.4543\n",
      "step:3500, loss train:2.4285, loss val:2.4545\n",
      "step:3750, loss train:2.4155, loss val:2.4550\n",
      "step:4000, loss train:2.4166, loss val:2.4497\n",
      "step:4250, loss train:2.4170, loss val:2.4510\n",
      "step:4500, loss train:2.4112, loss val:2.4402\n",
      "step:4750, loss train:2.4197, loss val:2.4487\n",
      "step:5000, loss train:2.4165, loss val:2.4312\n",
      "step:5250, loss train:2.4248, loss val:2.4554\n",
      "step:5500, loss train:2.4285, loss val:2.4302\n",
      "step:5750, loss train:2.4079, loss val:2.4463\n",
      "step:6000, loss train:2.4210, loss val:2.4454\n",
      "step:6250, loss train:2.4071, loss val:2.4351\n",
      "step:6500, loss train:2.4166, loss val:2.4433\n",
      "step:6750, loss train:2.4286, loss val:2.4621\n",
      "step:7000, loss train:2.4025, loss val:2.4659\n",
      "step:7250, loss train:2.4049, loss val:2.4412\n",
      "step:7500, loss train:2.4193, loss val:2.4535\n",
      "step:7750, loss train:2.3992, loss val:2.4653\n",
      "step:8000, loss train:2.4127, loss val:2.4323\n",
      "step:8250, loss train:2.4059, loss val:2.4616\n",
      "step:8500, loss train:2.4133, loss val:2.4269\n",
      "step:8750, loss train:2.4140, loss val:2.4515\n",
      "step:9000, loss train:2.3967, loss val:2.4246\n",
      "step:9250, loss train:2.4164, loss val:2.4519\n",
      "step:9500, loss train:2.4085, loss val:2.4565\n",
      "step:9750, loss train:2.4122, loss val:2.4306\n",
      "batch:104\n",
      "step:0, loss train:2.4037, loss val:2.4518\n",
      "step:250, loss train:2.4302, loss val:2.4594\n",
      "step:500, loss train:2.4141, loss val:2.4757\n",
      "step:750, loss train:2.4088, loss val:2.4377\n",
      "step:1000, loss train:2.4075, loss val:2.4464\n",
      "step:1250, loss train:2.4161, loss val:2.4485\n",
      "step:1500, loss train:2.4070, loss val:2.4524\n",
      "step:1750, loss train:2.4217, loss val:2.4333\n",
      "step:2000, loss train:2.4197, loss val:2.4646\n",
      "step:2250, loss train:2.4181, loss val:2.4465\n",
      "step:2500, loss train:2.4282, loss val:2.4413\n",
      "step:2750, loss train:2.4080, loss val:2.4563\n",
      "step:3000, loss train:2.3996, loss val:2.4484\n",
      "step:3250, loss train:2.4196, loss val:2.4888\n",
      "step:3500, loss train:2.4081, loss val:2.4567\n",
      "step:3750, loss train:2.4134, loss val:2.4563\n",
      "step:4000, loss train:2.4088, loss val:2.4277\n",
      "step:4250, loss train:2.4184, loss val:2.4613\n",
      "step:4500, loss train:2.4016, loss val:2.4545\n",
      "step:4750, loss train:2.4164, loss val:2.4593\n",
      "step:5000, loss train:2.4195, loss val:2.4654\n",
      "step:5250, loss train:2.4047, loss val:2.4570\n",
      "step:5500, loss train:2.4216, loss val:2.4572\n",
      "step:5750, loss train:2.4289, loss val:2.4434\n",
      "step:6000, loss train:2.4031, loss val:2.4404\n",
      "step:6250, loss train:2.4139, loss val:2.4492\n",
      "step:6500, loss train:2.4161, loss val:2.4535\n",
      "step:6750, loss train:2.3955, loss val:2.4333\n",
      "step:7000, loss train:2.4244, loss val:2.4387\n",
      "step:7250, loss train:2.4109, loss val:2.4409\n",
      "step:7500, loss train:2.4060, loss val:2.4450\n",
      "step:7750, loss train:2.4020, loss val:2.4592\n",
      "step:8000, loss train:2.4058, loss val:2.4447\n",
      "step:8250, loss train:2.4094, loss val:2.4529\n",
      "step:8500, loss train:2.4205, loss val:2.4281\n",
      "step:8750, loss train:2.3930, loss val:2.4569\n",
      "step:9000, loss train:2.3843, loss val:2.4513\n",
      "step:9250, loss train:2.3983, loss val:2.4461\n",
      "step:9500, loss train:2.4241, loss val:2.4535\n",
      "step:9750, loss train:2.4112, loss val:2.4507\n",
      "batch:105\n",
      "step:0, loss train:2.4091, loss val:2.4625\n",
      "step:250, loss train:2.3940, loss val:2.4555\n",
      "step:500, loss train:2.4143, loss val:2.4301\n",
      "step:750, loss train:2.4356, loss val:2.4425\n",
      "step:1000, loss train:2.4106, loss val:2.4643\n",
      "step:1250, loss train:2.4085, loss val:2.4415\n",
      "step:1500, loss train:2.4045, loss val:2.4364\n",
      "step:1750, loss train:2.4048, loss val:2.4587\n",
      "step:2000, loss train:2.4195, loss val:2.4572\n",
      "step:2250, loss train:2.4136, loss val:2.4334\n",
      "step:2500, loss train:2.4074, loss val:2.4384\n",
      "step:2750, loss train:2.4221, loss val:2.4676\n",
      "step:3000, loss train:2.4079, loss val:2.4594\n",
      "step:3250, loss train:2.4017, loss val:2.4440\n",
      "step:3500, loss train:2.4097, loss val:2.4324\n",
      "step:3750, loss train:2.4081, loss val:2.4322\n",
      "step:4000, loss train:2.3959, loss val:2.4338\n",
      "step:4250, loss train:2.4068, loss val:2.4650\n",
      "step:4500, loss train:2.4266, loss val:2.4403\n",
      "step:4750, loss train:2.4266, loss val:2.4387\n",
      "step:5000, loss train:2.4213, loss val:2.4307\n",
      "step:5250, loss train:2.4119, loss val:2.4263\n",
      "step:5500, loss train:2.4176, loss val:2.4268\n",
      "step:5750, loss train:2.4085, loss val:2.4455\n",
      "step:6000, loss train:2.4179, loss val:2.4823\n",
      "step:6250, loss train:2.4142, loss val:2.4472\n",
      "step:6500, loss train:2.4145, loss val:2.4635\n",
      "step:6750, loss train:2.4051, loss val:2.4732\n",
      "step:7000, loss train:2.4085, loss val:2.4610\n",
      "step:7250, loss train:2.4078, loss val:2.4509\n",
      "step:7500, loss train:2.3889, loss val:2.4456\n",
      "step:7750, loss train:2.4250, loss val:2.4485\n",
      "step:8000, loss train:2.4268, loss val:2.4291\n",
      "step:8250, loss train:2.4138, loss val:2.4448\n",
      "step:8500, loss train:2.4191, loss val:2.4388\n",
      "step:8750, loss train:2.4199, loss val:2.4450\n",
      "step:9000, loss train:2.4125, loss val:2.4476\n",
      "step:9250, loss train:2.4396, loss val:2.4499\n",
      "step:9500, loss train:2.3927, loss val:2.4621\n",
      "step:9750, loss train:2.4234, loss val:2.4479\n",
      "batch:106\n",
      "step:0, loss train:2.4040, loss val:2.4401\n",
      "step:250, loss train:2.4094, loss val:2.4482\n",
      "step:500, loss train:2.4042, loss val:2.4335\n",
      "step:750, loss train:2.4017, loss val:2.4425\n",
      "step:1000, loss train:2.4029, loss val:2.4564\n",
      "step:1250, loss train:2.4054, loss val:2.4506\n",
      "step:1500, loss train:2.4247, loss val:2.4430\n",
      "step:1750, loss train:2.4158, loss val:2.4372\n",
      "step:2000, loss train:2.4229, loss val:2.4267\n",
      "step:2250, loss train:2.3984, loss val:2.4483\n",
      "step:2500, loss train:2.4281, loss val:2.4439\n",
      "step:2750, loss train:2.4089, loss val:2.4536\n",
      "step:3000, loss train:2.4133, loss val:2.4487\n",
      "step:3250, loss train:2.4042, loss val:2.4583\n",
      "step:3500, loss train:2.4224, loss val:2.4549\n",
      "step:3750, loss train:2.3903, loss val:2.4614\n",
      "step:4000, loss train:2.4148, loss val:2.4791\n",
      "step:4250, loss train:2.4246, loss val:2.4391\n",
      "step:4500, loss train:2.4051, loss val:2.4793\n",
      "step:4750, loss train:2.4249, loss val:2.4670\n",
      "step:5000, loss train:2.4089, loss val:2.4320\n",
      "step:5250, loss train:2.4065, loss val:2.4286\n",
      "step:5500, loss train:2.4054, loss val:2.4773\n",
      "step:5750, loss train:2.4060, loss val:2.4598\n",
      "step:6000, loss train:2.4041, loss val:2.4427\n",
      "step:6250, loss train:2.4083, loss val:2.4555\n",
      "step:6500, loss train:2.3931, loss val:2.4408\n",
      "step:6750, loss train:2.4217, loss val:2.4333\n",
      "step:7000, loss train:2.4209, loss val:2.4392\n",
      "step:7250, loss train:2.4137, loss val:2.4433\n",
      "step:7500, loss train:2.4111, loss val:2.4504\n",
      "step:7750, loss train:2.4072, loss val:2.4456\n",
      "step:8000, loss train:2.4064, loss val:2.4298\n",
      "step:8250, loss train:2.4225, loss val:2.4440\n",
      "step:8500, loss train:2.4181, loss val:2.4356\n",
      "step:8750, loss train:2.4144, loss val:2.4471\n",
      "step:9000, loss train:2.4218, loss val:2.4311\n",
      "step:9250, loss train:2.4441, loss val:2.4387\n",
      "step:9500, loss train:2.4194, loss val:2.4550\n",
      "step:9750, loss train:2.4096, loss val:2.4500\n",
      "batch:107\n",
      "step:0, loss train:2.4110, loss val:2.4437\n",
      "step:250, loss train:2.4272, loss val:2.4505\n",
      "step:500, loss train:2.4026, loss val:2.4369\n",
      "step:750, loss train:2.3830, loss val:2.4241\n",
      "step:1000, loss train:2.4112, loss val:2.4345\n",
      "step:1250, loss train:2.4245, loss val:2.4511\n",
      "step:1500, loss train:2.4137, loss val:2.4653\n",
      "step:1750, loss train:2.4196, loss val:2.4775\n",
      "step:2000, loss train:2.3986, loss val:2.4496\n",
      "step:2250, loss train:2.3841, loss val:2.4496\n",
      "step:2500, loss train:2.4125, loss val:2.4715\n",
      "step:2750, loss train:2.4065, loss val:2.4660\n",
      "step:3000, loss train:2.4275, loss val:2.4671\n",
      "step:3250, loss train:2.4178, loss val:2.4506\n",
      "step:3500, loss train:2.4255, loss val:2.4391\n",
      "step:3750, loss train:2.4118, loss val:2.4484\n",
      "step:4000, loss train:2.4347, loss val:2.4343\n",
      "step:4250, loss train:2.4214, loss val:2.4586\n",
      "step:4500, loss train:2.4020, loss val:2.4289\n",
      "step:4750, loss train:2.4082, loss val:2.4198\n",
      "step:5000, loss train:2.3995, loss val:2.4559\n",
      "step:5250, loss train:2.3960, loss val:2.4567\n",
      "step:5500, loss train:2.4241, loss val:2.4584\n",
      "step:5750, loss train:2.4263, loss val:2.4513\n",
      "step:6000, loss train:2.4096, loss val:2.4504\n",
      "step:6250, loss train:2.4147, loss val:2.4564\n",
      "step:6500, loss train:2.4250, loss val:2.4589\n",
      "step:6750, loss train:2.4207, loss val:2.4313\n",
      "step:7000, loss train:2.4084, loss val:2.4602\n",
      "step:7250, loss train:2.4138, loss val:2.4277\n",
      "step:7500, loss train:2.4058, loss val:2.4475\n",
      "step:7750, loss train:2.4097, loss val:2.4288\n",
      "step:8000, loss train:2.4332, loss val:2.4114\n",
      "step:8250, loss train:2.4029, loss val:2.4512\n",
      "step:8500, loss train:2.3831, loss val:2.4294\n",
      "step:8750, loss train:2.3980, loss val:2.4365\n",
      "step:9000, loss train:2.4261, loss val:2.4510\n",
      "step:9250, loss train:2.4131, loss val:2.4535\n",
      "step:9500, loss train:2.4096, loss val:2.4526\n",
      "step:9750, loss train:2.4078, loss val:2.4713\n",
      "batch:108\n",
      "step:0, loss train:2.3863, loss val:2.4268\n",
      "step:250, loss train:2.4214, loss val:2.4154\n",
      "step:500, loss train:2.4086, loss val:2.4338\n",
      "step:750, loss train:2.4148, loss val:2.4472\n",
      "step:1000, loss train:2.4356, loss val:2.4491\n",
      "step:1250, loss train:2.4153, loss val:2.4421\n",
      "step:1500, loss train:2.4168, loss val:2.4518\n",
      "step:1750, loss train:2.3945, loss val:2.4581\n",
      "step:2000, loss train:2.3974, loss val:2.4552\n",
      "step:2250, loss train:2.4004, loss val:2.4519\n",
      "step:2500, loss train:2.3989, loss val:2.4408\n",
      "step:2750, loss train:2.3984, loss val:2.4664\n",
      "step:3000, loss train:2.4127, loss val:2.4383\n",
      "step:3250, loss train:2.4029, loss val:2.4362\n",
      "step:3500, loss train:2.4120, loss val:2.4434\n",
      "step:3750, loss train:2.4130, loss val:2.4456\n",
      "step:4000, loss train:2.4250, loss val:2.4812\n",
      "step:4250, loss train:2.3998, loss val:2.4595\n",
      "step:4500, loss train:2.4124, loss val:2.4597\n",
      "step:4750, loss train:2.4146, loss val:2.4500\n",
      "step:5000, loss train:2.4031, loss val:2.4443\n",
      "step:5250, loss train:2.4091, loss val:2.4592\n",
      "step:5500, loss train:2.4176, loss val:2.4426\n",
      "step:5750, loss train:2.4070, loss val:2.4383\n",
      "step:6000, loss train:2.4090, loss val:2.4710\n",
      "step:6250, loss train:2.4014, loss val:2.4400\n",
      "step:6500, loss train:2.4013, loss val:2.4529\n",
      "step:6750, loss train:2.4090, loss val:2.4450\n",
      "step:7000, loss train:2.4148, loss val:2.4417\n",
      "step:7250, loss train:2.4352, loss val:2.4515\n",
      "step:7500, loss train:2.4238, loss val:2.4431\n",
      "step:7750, loss train:2.4168, loss val:2.4572\n",
      "step:8000, loss train:2.3948, loss val:2.4375\n",
      "step:8250, loss train:2.4077, loss val:2.4417\n",
      "step:8500, loss train:2.4052, loss val:2.4655\n",
      "step:8750, loss train:2.3967, loss val:2.4443\n",
      "step:9000, loss train:2.4210, loss val:2.4431\n",
      "step:9250, loss train:2.4352, loss val:2.4498\n",
      "step:9500, loss train:2.4129, loss val:2.4272\n",
      "step:9750, loss train:2.4292, loss val:2.4191\n",
      "batch:109\n",
      "step:0, loss train:2.4143, loss val:2.4294\n",
      "step:250, loss train:2.4026, loss val:2.4568\n",
      "step:500, loss train:2.4125, loss val:2.4281\n",
      "step:750, loss train:2.4107, loss val:2.4464\n",
      "step:1000, loss train:2.4052, loss val:2.4402\n",
      "step:1250, loss train:2.4248, loss val:2.4420\n",
      "step:1500, loss train:2.4091, loss val:2.4626\n",
      "step:1750, loss train:2.4253, loss val:2.4437\n",
      "step:2000, loss train:2.4257, loss val:2.4480\n",
      "step:2250, loss train:2.4268, loss val:2.4445\n",
      "step:2500, loss train:2.4161, loss val:2.4307\n",
      "step:2750, loss train:2.4080, loss val:2.4615\n",
      "step:3000, loss train:2.4112, loss val:2.4531\n",
      "step:3250, loss train:2.4239, loss val:2.4489\n",
      "step:3500, loss train:2.4090, loss val:2.4381\n",
      "step:3750, loss train:2.4198, loss val:2.4722\n",
      "step:4000, loss train:2.4180, loss val:2.4657\n",
      "step:4250, loss train:2.4238, loss val:2.4312\n",
      "step:4500, loss train:2.4118, loss val:2.4470\n",
      "step:4750, loss train:2.4246, loss val:2.4275\n",
      "step:5000, loss train:2.4019, loss val:2.4512\n",
      "step:5250, loss train:2.4276, loss val:2.4281\n",
      "step:5500, loss train:2.4187, loss val:2.4617\n",
      "step:5750, loss train:2.4084, loss val:2.4440\n",
      "step:6000, loss train:2.4055, loss val:2.4415\n",
      "step:6250, loss train:2.4224, loss val:2.4559\n",
      "step:6500, loss train:2.4133, loss val:2.4219\n",
      "step:6750, loss train:2.4149, loss val:2.4401\n",
      "step:7000, loss train:2.4039, loss val:2.4237\n",
      "step:7250, loss train:2.4052, loss val:2.4510\n",
      "step:7500, loss train:2.4197, loss val:2.4342\n",
      "step:7750, loss train:2.4029, loss val:2.4505\n",
      "step:8000, loss train:2.4132, loss val:2.4641\n",
      "step:8250, loss train:2.4059, loss val:2.4404\n",
      "step:8500, loss train:2.4149, loss val:2.4563\n",
      "step:8750, loss train:2.4135, loss val:2.4360\n",
      "step:9000, loss train:2.4252, loss val:2.4516\n",
      "step:9250, loss train:2.4134, loss val:2.4563\n",
      "step:9500, loss train:2.4129, loss val:2.4439\n",
      "step:9750, loss train:2.3994, loss val:2.4550\n",
      "batch:110\n",
      "step:0, loss train:2.4271, loss val:2.4298\n",
      "step:250, loss train:2.4120, loss val:2.4474\n",
      "step:500, loss train:2.4186, loss val:2.4589\n",
      "step:750, loss train:2.4115, loss val:2.4563\n",
      "step:1000, loss train:2.4235, loss val:2.4265\n",
      "step:1250, loss train:2.4074, loss val:2.4411\n",
      "step:1500, loss train:2.4228, loss val:2.4553\n",
      "step:1750, loss train:2.4337, loss val:2.4265\n",
      "step:2000, loss train:2.3971, loss val:2.4350\n",
      "step:2250, loss train:2.4207, loss val:2.4400\n",
      "step:2500, loss train:2.4054, loss val:2.4541\n",
      "step:2750, loss train:2.4112, loss val:2.4548\n",
      "step:3000, loss train:2.4142, loss val:2.4385\n",
      "step:3250, loss train:2.4137, loss val:2.4395\n",
      "step:3500, loss train:2.4068, loss val:2.4646\n",
      "step:3750, loss train:2.4217, loss val:2.4229\n",
      "step:4000, loss train:2.4097, loss val:2.4417\n",
      "step:4250, loss train:2.3979, loss val:2.4421\n",
      "step:4500, loss train:2.4207, loss val:2.4531\n",
      "step:4750, loss train:2.3948, loss val:2.4561\n",
      "step:5000, loss train:2.4317, loss val:2.4491\n",
      "step:5250, loss train:2.4052, loss val:2.4454\n",
      "step:5500, loss train:2.4120, loss val:2.4536\n",
      "step:5750, loss train:2.4190, loss val:2.4481\n",
      "step:6000, loss train:2.4071, loss val:2.4606\n",
      "step:6250, loss train:2.4166, loss val:2.4530\n",
      "step:6500, loss train:2.4063, loss val:2.4497\n",
      "step:6750, loss train:2.4076, loss val:2.4484\n",
      "step:7000, loss train:2.4077, loss val:2.4607\n",
      "step:7250, loss train:2.4126, loss val:2.4252\n",
      "step:7500, loss train:2.4079, loss val:2.4478\n",
      "step:7750, loss train:2.4260, loss val:2.4549\n",
      "step:8000, loss train:2.4114, loss val:2.4638\n",
      "step:8250, loss train:2.4285, loss val:2.4545\n",
      "step:8500, loss train:2.4042, loss val:2.4532\n",
      "step:8750, loss train:2.4170, loss val:2.4401\n",
      "step:9000, loss train:2.3997, loss val:2.4142\n",
      "step:9250, loss train:2.4235, loss val:2.4306\n",
      "step:9500, loss train:2.4167, loss val:2.4349\n",
      "step:9750, loss train:2.3915, loss val:2.4400\n",
      "batch:111\n",
      "step:0, loss train:2.4166, loss val:2.4517\n",
      "step:250, loss train:2.4109, loss val:2.4321\n",
      "step:500, loss train:2.4125, loss val:2.4367\n",
      "step:750, loss train:2.4136, loss val:2.4077\n",
      "step:1000, loss train:2.4275, loss val:2.4444\n",
      "step:1250, loss train:2.4063, loss val:2.4459\n",
      "step:1500, loss train:2.4204, loss val:2.4609\n",
      "step:1750, loss train:2.4187, loss val:2.4686\n",
      "step:2000, loss train:2.4203, loss val:2.4586\n",
      "step:2250, loss train:2.4201, loss val:2.4654\n",
      "step:2500, loss train:2.4058, loss val:2.4669\n",
      "step:2750, loss train:2.4203, loss val:2.4281\n",
      "step:3000, loss train:2.4150, loss val:2.4450\n",
      "step:3250, loss train:2.4219, loss val:2.4454\n",
      "step:3500, loss train:2.4229, loss val:2.4379\n",
      "step:3750, loss train:2.4191, loss val:2.4388\n",
      "step:4000, loss train:2.4123, loss val:2.4753\n",
      "step:4250, loss train:2.4126, loss val:2.4191\n",
      "step:4500, loss train:2.4085, loss val:2.4696\n",
      "step:4750, loss train:2.4055, loss val:2.4554\n",
      "step:5000, loss train:2.4104, loss val:2.4513\n",
      "step:5250, loss train:2.4074, loss val:2.4303\n",
      "step:5500, loss train:2.3987, loss val:2.4241\n",
      "step:5750, loss train:2.4230, loss val:2.4512\n",
      "step:6000, loss train:2.3987, loss val:2.4558\n",
      "step:6250, loss train:2.4286, loss val:2.4466\n",
      "step:6500, loss train:2.3996, loss val:2.4538\n",
      "step:6750, loss train:2.4032, loss val:2.4558\n",
      "step:7000, loss train:2.4158, loss val:2.4476\n",
      "step:7250, loss train:2.4044, loss val:2.4472\n",
      "step:7500, loss train:2.4098, loss val:2.4710\n",
      "step:7750, loss train:2.4185, loss val:2.4611\n",
      "step:8000, loss train:2.4173, loss val:2.4513\n",
      "step:8250, loss train:2.4093, loss val:2.4489\n",
      "step:8500, loss train:2.4074, loss val:2.4438\n",
      "step:8750, loss train:2.4167, loss val:2.4378\n",
      "step:9000, loss train:2.3979, loss val:2.4502\n",
      "step:9250, loss train:2.4127, loss val:2.4244\n",
      "step:9500, loss train:2.4175, loss val:2.4360\n",
      "step:9750, loss train:2.4119, loss val:2.4741\n",
      "batch:112\n",
      "step:0, loss train:2.4194, loss val:2.4303\n",
      "step:250, loss train:2.4145, loss val:2.4428\n",
      "step:500, loss train:2.4136, loss val:2.4658\n",
      "step:750, loss train:2.3969, loss val:2.4459\n",
      "step:1000, loss train:2.4176, loss val:2.4346\n",
      "step:1250, loss train:2.4105, loss val:2.4366\n",
      "step:1500, loss train:2.3981, loss val:2.4528\n",
      "step:1750, loss train:2.4231, loss val:2.4456\n",
      "step:2000, loss train:2.4141, loss val:2.4428\n",
      "step:2250, loss train:2.4041, loss val:2.4460\n",
      "step:2500, loss train:2.3940, loss val:2.4379\n",
      "step:2750, loss train:2.4066, loss val:2.4435\n",
      "step:3000, loss train:2.3989, loss val:2.4435\n",
      "step:3250, loss train:2.3965, loss val:2.4353\n",
      "step:3500, loss train:2.4053, loss val:2.4595\n",
      "step:3750, loss train:2.4142, loss val:2.4720\n",
      "step:4000, loss train:2.4112, loss val:2.4267\n",
      "step:4250, loss train:2.4231, loss val:2.4785\n",
      "step:4500, loss train:2.4326, loss val:2.4436\n",
      "step:4750, loss train:2.4160, loss val:2.4714\n",
      "step:5000, loss train:2.4038, loss val:2.4499\n",
      "step:5250, loss train:2.4297, loss val:2.4474\n",
      "step:5500, loss train:2.4088, loss val:2.4466\n",
      "step:5750, loss train:2.4171, loss val:2.4726\n",
      "step:6000, loss train:2.4011, loss val:2.4494\n",
      "step:6250, loss train:2.4005, loss val:2.4322\n",
      "step:6500, loss train:2.3967, loss val:2.4155\n",
      "step:6750, loss train:2.4115, loss val:2.4304\n",
      "step:7000, loss train:2.4177, loss val:2.4449\n",
      "step:7250, loss train:2.4107, loss val:2.4450\n",
      "step:7500, loss train:2.4234, loss val:2.4689\n",
      "step:7750, loss train:2.4117, loss val:2.4597\n",
      "step:8000, loss train:2.3977, loss val:2.4600\n",
      "step:8250, loss train:2.4116, loss val:2.4531\n",
      "step:8500, loss train:2.4066, loss val:2.4509\n",
      "step:8750, loss train:2.4303, loss val:2.4575\n",
      "step:9000, loss train:2.4289, loss val:2.4615\n",
      "step:9250, loss train:2.4277, loss val:2.4569\n",
      "step:9500, loss train:2.4191, loss val:2.4453\n",
      "step:9750, loss train:2.4161, loss val:2.4489\n",
      "batch:113\n",
      "step:0, loss train:2.4107, loss val:2.4465\n",
      "step:250, loss train:2.4084, loss val:2.4379\n",
      "step:500, loss train:2.4001, loss val:2.4572\n",
      "step:750, loss train:2.4095, loss val:2.4342\n",
      "step:1000, loss train:2.4132, loss val:2.4397\n",
      "step:1250, loss train:2.4295, loss val:2.4622\n",
      "step:1500, loss train:2.3975, loss val:2.4372\n",
      "step:1750, loss train:2.3914, loss val:2.4495\n",
      "step:2000, loss train:2.4044, loss val:2.4451\n",
      "step:2250, loss train:2.4007, loss val:2.4337\n",
      "step:2500, loss train:2.4021, loss val:2.4541\n",
      "step:2750, loss train:2.4147, loss val:2.4667\n",
      "step:3000, loss train:2.4100, loss val:2.4398\n",
      "step:3250, loss train:2.4091, loss val:2.4440\n",
      "step:3500, loss train:2.4144, loss val:2.4485\n",
      "step:3750, loss train:2.4081, loss val:2.4362\n",
      "step:4000, loss train:2.4168, loss val:2.4504\n",
      "step:4250, loss train:2.4272, loss val:2.4719\n",
      "step:4500, loss train:2.3978, loss val:2.4372\n",
      "step:4750, loss train:2.4029, loss val:2.4476\n",
      "step:5000, loss train:2.4167, loss val:2.4428\n",
      "step:5250, loss train:2.4055, loss val:2.4400\n",
      "step:5500, loss train:2.3846, loss val:2.4322\n",
      "step:5750, loss train:2.4154, loss val:2.4496\n",
      "step:6000, loss train:2.4074, loss val:2.4532\n",
      "step:6250, loss train:2.4129, loss val:2.4421\n",
      "step:6500, loss train:2.4188, loss val:2.4519\n",
      "step:6750, loss train:2.4092, loss val:2.4677\n",
      "step:7000, loss train:2.4134, loss val:2.4350\n",
      "step:7250, loss train:2.4049, loss val:2.4415\n",
      "step:7500, loss train:2.4027, loss val:2.4304\n",
      "step:7750, loss train:2.4290, loss val:2.4348\n",
      "step:8000, loss train:2.4009, loss val:2.4603\n",
      "step:8250, loss train:2.4174, loss val:2.4235\n",
      "step:8500, loss train:2.4071, loss val:2.4513\n",
      "step:8750, loss train:2.4220, loss val:2.4574\n",
      "step:9000, loss train:2.4299, loss val:2.4512\n",
      "step:9250, loss train:2.4088, loss val:2.4645\n",
      "step:9500, loss train:2.3961, loss val:2.4392\n",
      "step:9750, loss train:2.4164, loss val:2.4536\n",
      "batch:114\n",
      "step:0, loss train:2.4213, loss val:2.4462\n",
      "step:250, loss train:2.4000, loss val:2.4341\n",
      "step:500, loss train:2.4165, loss val:2.4351\n",
      "step:750, loss train:2.4044, loss val:2.4272\n",
      "step:1000, loss train:2.4269, loss val:2.4255\n",
      "step:1250, loss train:2.4172, loss val:2.4352\n",
      "step:1500, loss train:2.4246, loss val:2.4518\n",
      "step:1750, loss train:2.4109, loss val:2.4572\n",
      "step:2000, loss train:2.3963, loss val:2.4360\n",
      "step:2250, loss train:2.4248, loss val:2.4332\n",
      "step:2500, loss train:2.4247, loss val:2.4226\n",
      "step:2750, loss train:2.4123, loss val:2.4330\n",
      "step:3000, loss train:2.4212, loss val:2.4399\n",
      "step:3250, loss train:2.3987, loss val:2.4470\n",
      "step:3500, loss train:2.4242, loss val:2.4406\n",
      "step:3750, loss train:2.4296, loss val:2.4493\n",
      "step:4000, loss train:2.4287, loss val:2.4427\n",
      "step:4250, loss train:2.4146, loss val:2.4502\n",
      "step:4500, loss train:2.4115, loss val:2.4478\n",
      "step:4750, loss train:2.4025, loss val:2.4419\n",
      "step:5000, loss train:2.4005, loss val:2.4453\n",
      "step:5250, loss train:2.4172, loss val:2.4590\n",
      "step:5500, loss train:2.4197, loss val:2.4228\n",
      "step:5750, loss train:2.4099, loss val:2.4643\n",
      "step:6000, loss train:2.4026, loss val:2.4593\n",
      "step:6250, loss train:2.3904, loss val:2.4458\n",
      "step:6500, loss train:2.4056, loss val:2.4396\n",
      "step:6750, loss train:2.4268, loss val:2.4275\n",
      "step:7000, loss train:2.4076, loss val:2.4285\n",
      "step:7250, loss train:2.4308, loss val:2.4378\n",
      "step:7500, loss train:2.4156, loss val:2.4455\n",
      "step:7750, loss train:2.4070, loss val:2.4512\n",
      "step:8000, loss train:2.4144, loss val:2.4453\n",
      "step:8250, loss train:2.4000, loss val:2.4658\n",
      "step:8500, loss train:2.4098, loss val:2.4399\n",
      "step:8750, loss train:2.4127, loss val:2.4498\n",
      "step:9000, loss train:2.4133, loss val:2.4507\n",
      "step:9250, loss train:2.4186, loss val:2.4604\n",
      "step:9500, loss train:2.4064, loss val:2.4330\n",
      "step:9750, loss train:2.3968, loss val:2.4488\n",
      "batch:115\n",
      "step:0, loss train:2.4198, loss val:2.4453\n",
      "step:250, loss train:2.4148, loss val:2.4560\n",
      "step:500, loss train:2.4060, loss val:2.4544\n",
      "step:750, loss train:2.3967, loss val:2.4462\n",
      "step:1000, loss train:2.4137, loss val:2.4525\n",
      "step:1250, loss train:2.4019, loss val:2.4556\n",
      "step:1500, loss train:2.4075, loss val:2.4480\n",
      "step:1750, loss train:2.4061, loss val:2.4302\n",
      "step:2000, loss train:2.4184, loss val:2.4545\n",
      "step:2250, loss train:2.4111, loss val:2.4317\n",
      "step:2500, loss train:2.4272, loss val:2.4625\n",
      "step:2750, loss train:2.4011, loss val:2.4701\n",
      "step:3000, loss train:2.4018, loss val:2.4741\n",
      "step:3250, loss train:2.4292, loss val:2.4417\n",
      "step:3500, loss train:2.4077, loss val:2.4493\n",
      "step:3750, loss train:2.4174, loss val:2.4406\n",
      "step:4000, loss train:2.4049, loss val:2.4580\n",
      "step:4250, loss train:2.4076, loss val:2.4459\n",
      "step:4500, loss train:2.4329, loss val:2.4449\n",
      "step:4750, loss train:2.4224, loss val:2.4456\n",
      "step:5000, loss train:2.3971, loss val:2.4367\n",
      "step:5250, loss train:2.4064, loss val:2.4508\n",
      "step:5500, loss train:2.4042, loss val:2.4342\n",
      "step:5750, loss train:2.3997, loss val:2.4659\n",
      "step:6000, loss train:2.4193, loss val:2.4438\n",
      "step:6250, loss train:2.4180, loss val:2.4592\n",
      "step:6500, loss train:2.4150, loss val:2.4615\n",
      "step:6750, loss train:2.4028, loss val:2.4305\n",
      "step:7000, loss train:2.4276, loss val:2.4408\n",
      "step:7250, loss train:2.4204, loss val:2.4405\n",
      "step:7500, loss train:2.4100, loss val:2.4419\n",
      "step:7750, loss train:2.4251, loss val:2.4459\n",
      "step:8000, loss train:2.4010, loss val:2.4502\n",
      "step:8250, loss train:2.4085, loss val:2.4417\n",
      "step:8500, loss train:2.4001, loss val:2.4417\n",
      "step:8750, loss train:2.4060, loss val:2.4385\n",
      "step:9000, loss train:2.4243, loss val:2.4282\n",
      "step:9250, loss train:2.4208, loss val:2.4330\n",
      "step:9500, loss train:2.3973, loss val:2.4381\n",
      "step:9750, loss train:2.4105, loss val:2.4485\n",
      "batch:116\n",
      "step:0, loss train:2.4132, loss val:2.4239\n",
      "step:250, loss train:2.4152, loss val:2.4508\n",
      "step:500, loss train:2.4067, loss val:2.4455\n",
      "step:750, loss train:2.4034, loss val:2.4360\n",
      "step:1000, loss train:2.4122, loss val:2.4545\n",
      "step:1250, loss train:2.4129, loss val:2.4626\n",
      "step:1500, loss train:2.4161, loss val:2.4487\n",
      "step:1750, loss train:2.4151, loss val:2.4244\n",
      "step:2000, loss train:2.4020, loss val:2.4666\n",
      "step:2250, loss train:2.4016, loss val:2.4314\n",
      "step:2500, loss train:2.4104, loss val:2.4180\n",
      "step:2750, loss train:2.4208, loss val:2.4397\n",
      "step:3000, loss train:2.4118, loss val:2.4276\n",
      "step:3250, loss train:2.4074, loss val:2.4611\n",
      "step:3500, loss train:2.4250, loss val:2.4726\n",
      "step:3750, loss train:2.4103, loss val:2.4619\n",
      "step:4000, loss train:2.3964, loss val:2.4212\n",
      "step:4250, loss train:2.4099, loss val:2.4522\n",
      "step:4500, loss train:2.4007, loss val:2.4480\n",
      "step:4750, loss train:2.4031, loss val:2.4333\n",
      "step:5000, loss train:2.4179, loss val:2.4564\n",
      "step:5250, loss train:2.4109, loss val:2.4553\n",
      "step:5500, loss train:2.4197, loss val:2.4482\n",
      "step:5750, loss train:2.4188, loss val:2.4534\n",
      "step:6000, loss train:2.3991, loss val:2.4380\n",
      "step:6250, loss train:2.4186, loss val:2.4434\n",
      "step:6500, loss train:2.4147, loss val:2.4445\n",
      "step:6750, loss train:2.4218, loss val:2.4491\n",
      "step:7000, loss train:2.4236, loss val:2.4482\n",
      "step:7250, loss train:2.4079, loss val:2.4397\n",
      "step:7500, loss train:2.4246, loss val:2.4648\n",
      "step:7750, loss train:2.4059, loss val:2.4218\n",
      "step:8000, loss train:2.4230, loss val:2.4491\n",
      "step:8250, loss train:2.3972, loss val:2.4513\n",
      "step:8500, loss train:2.4084, loss val:2.4234\n",
      "step:8750, loss train:2.4102, loss val:2.4439\n",
      "step:9000, loss train:2.4126, loss val:2.4465\n",
      "step:9250, loss train:2.4046, loss val:2.4260\n",
      "step:9500, loss train:2.4205, loss val:2.4389\n",
      "step:9750, loss train:2.4075, loss val:2.4467\n",
      "batch:117\n",
      "step:0, loss train:2.3928, loss val:2.4331\n",
      "step:250, loss train:2.4136, loss val:2.4478\n",
      "step:500, loss train:2.4193, loss val:2.4263\n",
      "step:750, loss train:2.4040, loss val:2.4384\n",
      "step:1000, loss train:2.4163, loss val:2.4712\n",
      "step:1250, loss train:2.4041, loss val:2.4646\n",
      "step:1500, loss train:2.4166, loss val:2.4526\n",
      "step:1750, loss train:2.4243, loss val:2.4594\n",
      "step:2000, loss train:2.4058, loss val:2.4570\n",
      "step:2250, loss train:2.3982, loss val:2.4371\n",
      "step:2500, loss train:2.4012, loss val:2.4448\n",
      "step:2750, loss train:2.4052, loss val:2.4423\n",
      "step:3000, loss train:2.3859, loss val:2.4335\n",
      "step:3250, loss train:2.3939, loss val:2.4481\n",
      "step:3500, loss train:2.4117, loss val:2.4592\n",
      "step:3750, loss train:2.4055, loss val:2.4478\n",
      "step:4000, loss train:2.4166, loss val:2.4528\n",
      "step:4250, loss train:2.4139, loss val:2.4285\n",
      "step:4500, loss train:2.3914, loss val:2.4305\n",
      "step:4750, loss train:2.4157, loss val:2.4518\n",
      "step:5000, loss train:2.4261, loss val:2.4473\n",
      "step:5250, loss train:2.4033, loss val:2.4492\n",
      "step:5500, loss train:2.4230, loss val:2.4515\n",
      "step:5750, loss train:2.4246, loss val:2.4240\n",
      "step:6000, loss train:2.4019, loss val:2.4441\n",
      "step:6250, loss train:2.4251, loss val:2.4535\n",
      "step:6500, loss train:2.4070, loss val:2.4540\n",
      "step:6750, loss train:2.4381, loss val:2.4641\n",
      "step:7000, loss train:2.3905, loss val:2.4329\n",
      "step:7250, loss train:2.4209, loss val:2.4508\n",
      "step:7500, loss train:2.4125, loss val:2.4282\n",
      "step:7750, loss train:2.4113, loss val:2.4290\n",
      "step:8000, loss train:2.4264, loss val:2.4237\n",
      "step:8250, loss train:2.4286, loss val:2.4411\n",
      "step:8500, loss train:2.4121, loss val:2.4473\n",
      "step:8750, loss train:2.3943, loss val:2.4682\n",
      "step:9000, loss train:2.4335, loss val:2.4241\n",
      "step:9250, loss train:2.4396, loss val:2.4654\n",
      "step:9500, loss train:2.4127, loss val:2.4602\n",
      "step:9750, loss train:2.4188, loss val:2.4531\n",
      "batch:118\n",
      "step:0, loss train:2.4128, loss val:2.4540\n",
      "step:250, loss train:2.4059, loss val:2.4357\n",
      "step:500, loss train:2.4249, loss val:2.4330\n",
      "step:750, loss train:2.4024, loss val:2.4566\n",
      "step:1000, loss train:2.4027, loss val:2.4325\n",
      "step:1250, loss train:2.4212, loss val:2.4380\n",
      "step:1500, loss train:2.3962, loss val:2.4354\n",
      "step:1750, loss train:2.4040, loss val:2.4495\n",
      "step:2000, loss train:2.4136, loss val:2.4132\n",
      "step:2250, loss train:2.4112, loss val:2.4631\n",
      "step:2500, loss train:2.4122, loss val:2.4621\n",
      "step:2750, loss train:2.4057, loss val:2.4213\n",
      "step:3000, loss train:2.4238, loss val:2.4469\n",
      "step:3250, loss train:2.4330, loss val:2.4142\n",
      "step:3500, loss train:2.4392, loss val:2.4436\n",
      "step:3750, loss train:2.4231, loss val:2.4364\n",
      "step:4000, loss train:2.4159, loss val:2.4487\n",
      "step:4250, loss train:2.4071, loss val:2.4407\n",
      "step:4500, loss train:2.4165, loss val:2.4589\n",
      "step:4750, loss train:2.4296, loss val:2.4423\n",
      "step:5000, loss train:2.4270, loss val:2.4445\n",
      "step:5250, loss train:2.4141, loss val:2.4392\n",
      "step:5500, loss train:2.4073, loss val:2.4348\n",
      "step:5750, loss train:2.4223, loss val:2.4414\n",
      "step:6000, loss train:2.4019, loss val:2.4579\n",
      "step:6250, loss train:2.4253, loss val:2.4525\n",
      "step:6500, loss train:2.4028, loss val:2.4556\n",
      "step:6750, loss train:2.4130, loss val:2.4457\n",
      "step:7000, loss train:2.4218, loss val:2.4390\n",
      "step:7250, loss train:2.4149, loss val:2.4359\n",
      "step:7500, loss train:2.3938, loss val:2.4390\n",
      "step:7750, loss train:2.4232, loss val:2.4262\n",
      "step:8000, loss train:2.4202, loss val:2.4575\n",
      "step:8250, loss train:2.4253, loss val:2.4377\n",
      "step:8500, loss train:2.4007, loss val:2.4326\n",
      "step:8750, loss train:2.4064, loss val:2.4343\n",
      "step:9000, loss train:2.3977, loss val:2.4435\n",
      "step:9250, loss train:2.4274, loss val:2.4454\n",
      "step:9500, loss train:2.4152, loss val:2.4759\n",
      "step:9750, loss train:2.3989, loss val:2.4383\n",
      "batch:119\n",
      "step:0, loss train:2.4139, loss val:2.4585\n",
      "step:250, loss train:2.4071, loss val:2.4365\n",
      "step:500, loss train:2.3829, loss val:2.4401\n",
      "step:750, loss train:2.4160, loss val:2.4466\n",
      "step:1000, loss train:2.4014, loss val:2.4581\n",
      "step:1250, loss train:2.4124, loss val:2.4356\n",
      "step:1500, loss train:2.4134, loss val:2.4582\n",
      "step:1750, loss train:2.3819, loss val:2.4417\n",
      "step:2000, loss train:2.4174, loss val:2.4511\n",
      "step:2250, loss train:2.4403, loss val:2.4255\n",
      "step:2500, loss train:2.4083, loss val:2.4386\n",
      "step:2750, loss train:2.4169, loss val:2.4478\n",
      "step:3000, loss train:2.4111, loss val:2.4331\n",
      "step:3250, loss train:2.3995, loss val:2.4628\n",
      "step:3500, loss train:2.4053, loss val:2.4375\n",
      "step:3750, loss train:2.3821, loss val:2.4798\n",
      "step:4000, loss train:2.3958, loss val:2.4409\n",
      "step:4250, loss train:2.4212, loss val:2.4562\n",
      "step:4500, loss train:2.4050, loss val:2.4323\n",
      "step:4750, loss train:2.4104, loss val:2.4277\n",
      "step:5000, loss train:2.4139, loss val:2.4531\n",
      "step:5250, loss train:2.4089, loss val:2.4476\n",
      "step:5500, loss train:2.4077, loss val:2.4355\n",
      "step:5750, loss train:2.4007, loss val:2.4393\n",
      "step:6000, loss train:2.3979, loss val:2.4406\n",
      "step:6250, loss train:2.4155, loss val:2.4431\n",
      "step:6500, loss train:2.4198, loss val:2.4428\n",
      "step:6750, loss train:2.3902, loss val:2.4285\n",
      "step:7000, loss train:2.4296, loss val:2.4391\n",
      "step:7250, loss train:2.4171, loss val:2.4489\n",
      "step:7500, loss train:2.4427, loss val:2.4815\n",
      "step:7750, loss train:2.4037, loss val:2.4634\n",
      "step:8000, loss train:2.4124, loss val:2.4531\n",
      "step:8250, loss train:2.4024, loss val:2.4470\n",
      "step:8500, loss train:2.4067, loss val:2.4597\n",
      "step:8750, loss train:2.3928, loss val:2.4582\n",
      "step:9000, loss train:2.3972, loss val:2.4448\n",
      "step:9250, loss train:2.3924, loss val:2.4293\n",
      "step:9500, loss train:2.4066, loss val:2.4658\n",
      "step:9750, loss train:2.4191, loss val:2.4526\n",
      "batch:120\n",
      "step:0, loss train:2.4234, loss val:2.4362\n",
      "step:250, loss train:2.4114, loss val:2.4192\n",
      "step:500, loss train:2.4089, loss val:2.4415\n",
      "step:750, loss train:2.4238, loss val:2.4369\n",
      "step:1000, loss train:2.4122, loss val:2.4456\n",
      "step:1250, loss train:2.4187, loss val:2.4567\n",
      "step:1500, loss train:2.4029, loss val:2.4522\n",
      "step:1750, loss train:2.4105, loss val:2.4478\n",
      "step:2000, loss train:2.4222, loss val:2.4462\n",
      "step:2250, loss train:2.4282, loss val:2.4791\n",
      "step:2500, loss train:2.4011, loss val:2.4447\n",
      "step:2750, loss train:2.4090, loss val:2.4476\n",
      "step:3000, loss train:2.3994, loss val:2.4460\n",
      "step:3250, loss train:2.4018, loss val:2.4494\n",
      "step:3500, loss train:2.4134, loss val:2.4462\n",
      "step:3750, loss train:2.3958, loss val:2.4488\n",
      "step:4000, loss train:2.4185, loss val:2.4606\n",
      "step:4250, loss train:2.4160, loss val:2.4510\n",
      "step:4500, loss train:2.4226, loss val:2.4384\n",
      "step:4750, loss train:2.4029, loss val:2.4513\n",
      "step:5000, loss train:2.4206, loss val:2.4506\n",
      "step:5250, loss train:2.3982, loss val:2.4389\n",
      "step:5500, loss train:2.4169, loss val:2.4269\n",
      "step:5750, loss train:2.3999, loss val:2.4502\n",
      "step:6000, loss train:2.4114, loss val:2.4415\n",
      "step:6250, loss train:2.4074, loss val:2.4592\n",
      "step:6500, loss train:2.4009, loss val:2.4427\n",
      "step:6750, loss train:2.4306, loss val:2.4592\n",
      "step:7000, loss train:2.4180, loss val:2.4398\n",
      "step:7250, loss train:2.4022, loss val:2.4562\n",
      "step:7500, loss train:2.3981, loss val:2.4277\n",
      "step:7750, loss train:2.4118, loss val:2.4435\n",
      "step:8000, loss train:2.4163, loss val:2.4458\n",
      "step:8250, loss train:2.4325, loss val:2.4421\n",
      "step:8500, loss train:2.4056, loss val:2.4411\n",
      "step:8750, loss train:2.4138, loss val:2.4576\n",
      "step:9000, loss train:2.4138, loss val:2.4438\n",
      "step:9250, loss train:2.3979, loss val:2.4635\n",
      "step:9500, loss train:2.4192, loss val:2.4722\n",
      "step:9750, loss train:2.4261, loss val:2.4348\n",
      "batch:121\n",
      "step:0, loss train:2.4136, loss val:2.4506\n",
      "step:250, loss train:2.3934, loss val:2.4360\n",
      "step:500, loss train:2.4097, loss val:2.4467\n",
      "step:750, loss train:2.3943, loss val:2.4585\n",
      "step:1000, loss train:2.4108, loss val:2.4573\n",
      "step:1250, loss train:2.3960, loss val:2.4259\n",
      "step:1500, loss train:2.3959, loss val:2.4275\n",
      "step:1750, loss train:2.4165, loss val:2.4391\n",
      "step:2000, loss train:2.4102, loss val:2.4429\n",
      "step:2250, loss train:2.4079, loss val:2.4738\n",
      "step:2500, loss train:2.4140, loss val:2.4520\n",
      "step:2750, loss train:2.4073, loss val:2.4344\n",
      "step:3000, loss train:2.4014, loss val:2.4340\n",
      "step:3250, loss train:2.4176, loss val:2.4391\n",
      "step:3500, loss train:2.4168, loss val:2.4625\n",
      "step:3750, loss train:2.4076, loss val:2.4378\n",
      "step:4000, loss train:2.3964, loss val:2.4605\n",
      "step:4250, loss train:2.4392, loss val:2.4428\n",
      "step:4500, loss train:2.4030, loss val:2.4556\n",
      "step:4750, loss train:2.4091, loss val:2.4515\n",
      "step:5000, loss train:2.4028, loss val:2.4561\n",
      "step:5250, loss train:2.4139, loss val:2.4387\n",
      "step:5500, loss train:2.4247, loss val:2.4433\n",
      "step:5750, loss train:2.4140, loss val:2.4396\n",
      "step:6000, loss train:2.4071, loss val:2.4344\n",
      "step:6250, loss train:2.4028, loss val:2.4577\n",
      "step:6500, loss train:2.4193, loss val:2.4561\n",
      "step:6750, loss train:2.4067, loss val:2.4602\n",
      "step:7000, loss train:2.4059, loss val:2.4422\n",
      "step:7250, loss train:2.4139, loss val:2.4488\n",
      "step:7500, loss train:2.4184, loss val:2.4323\n",
      "step:7750, loss train:2.4267, loss val:2.4613\n",
      "step:8000, loss train:2.4203, loss val:2.4538\n",
      "step:8250, loss train:2.4140, loss val:2.4546\n",
      "step:8500, loss train:2.3967, loss val:2.4440\n",
      "step:8750, loss train:2.4009, loss val:2.4547\n",
      "step:9000, loss train:2.3983, loss val:2.4487\n",
      "step:9250, loss train:2.3894, loss val:2.4709\n",
      "step:9500, loss train:2.4128, loss val:2.4307\n",
      "step:9750, loss train:2.4086, loss val:2.4355\n",
      "batch:122\n",
      "step:0, loss train:2.3989, loss val:2.4366\n",
      "step:250, loss train:2.4130, loss val:2.4583\n",
      "step:500, loss train:2.4107, loss val:2.4606\n",
      "step:750, loss train:2.4114, loss val:2.4494\n",
      "step:1000, loss train:2.4251, loss val:2.4348\n",
      "step:1250, loss train:2.4117, loss val:2.4470\n",
      "step:1500, loss train:2.4019, loss val:2.4626\n",
      "step:1750, loss train:2.4041, loss val:2.4460\n",
      "step:2000, loss train:2.4234, loss val:2.4547\n",
      "step:2250, loss train:2.4072, loss val:2.4724\n",
      "step:2500, loss train:2.4336, loss val:2.4105\n",
      "step:2750, loss train:2.4147, loss val:2.4479\n",
      "step:3000, loss train:2.4180, loss val:2.4379\n",
      "step:3250, loss train:2.4049, loss val:2.4293\n",
      "step:3500, loss train:2.4257, loss val:2.4111\n",
      "step:3750, loss train:2.3989, loss val:2.4480\n",
      "step:4000, loss train:2.4108, loss val:2.4344\n",
      "step:4250, loss train:2.4045, loss val:2.4498\n",
      "step:4500, loss train:2.4045, loss val:2.4650\n",
      "step:4750, loss train:2.4223, loss val:2.4564\n",
      "step:5000, loss train:2.4132, loss val:2.4459\n",
      "step:5250, loss train:2.4205, loss val:2.4440\n",
      "step:5500, loss train:2.4132, loss val:2.4159\n",
      "step:5750, loss train:2.4220, loss val:2.4398\n",
      "step:6000, loss train:2.4177, loss val:2.4572\n",
      "step:6250, loss train:2.4215, loss val:2.4178\n",
      "step:6500, loss train:2.3961, loss val:2.4437\n",
      "step:6750, loss train:2.3909, loss val:2.4585\n",
      "step:7000, loss train:2.4235, loss val:2.4511\n",
      "step:7250, loss train:2.4144, loss val:2.4152\n",
      "step:7500, loss train:2.3996, loss val:2.4593\n",
      "step:7750, loss train:2.4085, loss val:2.4512\n",
      "step:8000, loss train:2.4191, loss val:2.4491\n",
      "step:8250, loss train:2.4116, loss val:2.4598\n",
      "step:8500, loss train:2.4038, loss val:2.4227\n",
      "step:8750, loss train:2.4183, loss val:2.4471\n",
      "step:9000, loss train:2.4259, loss val:2.4310\n",
      "step:9250, loss train:2.4109, loss val:2.4436\n",
      "step:9500, loss train:2.4034, loss val:2.4411\n",
      "step:9750, loss train:2.4218, loss val:2.4369\n",
      "batch:123\n",
      "step:0, loss train:2.4084, loss val:2.4559\n",
      "step:250, loss train:2.4178, loss val:2.4361\n",
      "step:500, loss train:2.4213, loss val:2.4395\n",
      "step:750, loss train:2.4215, loss val:2.4516\n",
      "step:1000, loss train:2.4228, loss val:2.4537\n",
      "step:1250, loss train:2.4010, loss val:2.4591\n",
      "step:1500, loss train:2.4148, loss val:2.4450\n",
      "step:1750, loss train:2.4131, loss val:2.4492\n",
      "step:2000, loss train:2.4161, loss val:2.4508\n",
      "step:2250, loss train:2.4085, loss val:2.4299\n",
      "step:2500, loss train:2.4268, loss val:2.4542\n",
      "step:2750, loss train:2.4220, loss val:2.4667\n",
      "step:3000, loss train:2.4109, loss val:2.4491\n",
      "step:3250, loss train:2.4134, loss val:2.4501\n",
      "step:3500, loss train:2.4099, loss val:2.4351\n",
      "step:3750, loss train:2.4037, loss val:2.4256\n",
      "step:4000, loss train:2.4430, loss val:2.4556\n",
      "step:4250, loss train:2.4107, loss val:2.4308\n",
      "step:4500, loss train:2.4223, loss val:2.4531\n",
      "step:4750, loss train:2.4110, loss val:2.4369\n",
      "step:5000, loss train:2.4111, loss val:2.4598\n",
      "step:5250, loss train:2.3945, loss val:2.4439\n",
      "step:5500, loss train:2.4239, loss val:2.4529\n",
      "step:5750, loss train:2.4005, loss val:2.4385\n",
      "step:6000, loss train:2.4176, loss val:2.4434\n",
      "step:6250, loss train:2.4285, loss val:2.4250\n",
      "step:6500, loss train:2.3966, loss val:2.4466\n",
      "step:6750, loss train:2.4204, loss val:2.4426\n",
      "step:7000, loss train:2.4167, loss val:2.4542\n",
      "step:7250, loss train:2.3953, loss val:2.4472\n",
      "step:7500, loss train:2.4052, loss val:2.4366\n",
      "step:7750, loss train:2.4061, loss val:2.4455\n",
      "step:8000, loss train:2.4126, loss val:2.4557\n",
      "step:8250, loss train:2.4153, loss val:2.4451\n",
      "step:8500, loss train:2.4178, loss val:2.4433\n",
      "step:8750, loss train:2.4091, loss val:2.4318\n",
      "step:9000, loss train:2.4231, loss val:2.4439\n",
      "step:9250, loss train:2.4025, loss val:2.4628\n",
      "step:9500, loss train:2.4294, loss val:2.4451\n",
      "step:9750, loss train:2.4017, loss val:2.4378\n",
      "batch:124\n",
      "step:0, loss train:2.4066, loss val:2.4526\n",
      "step:250, loss train:2.4200, loss val:2.4487\n",
      "step:500, loss train:2.4124, loss val:2.4657\n",
      "step:750, loss train:2.4118, loss val:2.4521\n",
      "step:1000, loss train:2.4013, loss val:2.4167\n",
      "step:1250, loss train:2.4207, loss val:2.4496\n",
      "step:1500, loss train:2.4129, loss val:2.4508\n",
      "step:1750, loss train:2.3985, loss val:2.4542\n",
      "step:2000, loss train:2.4254, loss val:2.4541\n",
      "step:2250, loss train:2.3885, loss val:2.4385\n",
      "step:2500, loss train:2.4266, loss val:2.4454\n",
      "step:2750, loss train:2.4208, loss val:2.4490\n",
      "step:3000, loss train:2.4124, loss val:2.4542\n",
      "step:3250, loss train:2.4074, loss val:2.4683\n",
      "step:3500, loss train:2.4100, loss val:2.4402\n",
      "step:3750, loss train:2.4227, loss val:2.4217\n",
      "step:4000, loss train:2.4092, loss val:2.4404\n",
      "step:4250, loss train:2.4108, loss val:2.4532\n",
      "step:4500, loss train:2.4066, loss val:2.4626\n",
      "step:4750, loss train:2.4120, loss val:2.4587\n",
      "step:5000, loss train:2.3987, loss val:2.4601\n",
      "step:5250, loss train:2.4086, loss val:2.4555\n",
      "step:5500, loss train:2.3994, loss val:2.4562\n",
      "step:5750, loss train:2.4050, loss val:2.4565\n",
      "step:6000, loss train:2.4149, loss val:2.4484\n",
      "step:6250, loss train:2.4254, loss val:2.4519\n",
      "step:6500, loss train:2.4195, loss val:2.4428\n",
      "step:6750, loss train:2.4205, loss val:2.4621\n",
      "step:7000, loss train:2.4173, loss val:2.4382\n",
      "step:7250, loss train:2.4084, loss val:2.4701\n",
      "step:7500, loss train:2.4281, loss val:2.4288\n",
      "step:7750, loss train:2.4037, loss val:2.4379\n",
      "step:8000, loss train:2.3954, loss val:2.4493\n",
      "step:8250, loss train:2.3954, loss val:2.4292\n",
      "step:8500, loss train:2.4242, loss val:2.4254\n",
      "step:8750, loss train:2.4302, loss val:2.4566\n",
      "step:9000, loss train:2.4162, loss val:2.4333\n",
      "step:9250, loss train:2.4138, loss val:2.4108\n",
      "step:9500, loss train:2.4128, loss val:2.4610\n",
      "step:9750, loss train:2.4196, loss val:2.4590\n",
      "batch:125\n",
      "step:0, loss train:2.4023, loss val:2.4385\n",
      "step:250, loss train:2.4098, loss val:2.4672\n",
      "step:500, loss train:2.4207, loss val:2.4190\n",
      "step:750, loss train:2.4134, loss val:2.4628\n",
      "step:1000, loss train:2.4152, loss val:2.4552\n",
      "step:1250, loss train:2.4176, loss val:2.4367\n",
      "step:1500, loss train:2.4117, loss val:2.4560\n",
      "step:1750, loss train:2.4041, loss val:2.4626\n",
      "step:2000, loss train:2.4043, loss val:2.4304\n",
      "step:2250, loss train:2.4192, loss val:2.4354\n",
      "step:2500, loss train:2.4162, loss val:2.4382\n",
      "step:2750, loss train:2.4222, loss val:2.4458\n",
      "step:3000, loss train:2.4033, loss val:2.4369\n",
      "step:3250, loss train:2.4040, loss val:2.4565\n",
      "step:3500, loss train:2.4115, loss val:2.4602\n",
      "step:3750, loss train:2.4177, loss val:2.4578\n",
      "step:4000, loss train:2.4264, loss val:2.4463\n",
      "step:4250, loss train:2.4187, loss val:2.4382\n",
      "step:4500, loss train:2.4116, loss val:2.4505\n",
      "step:4750, loss train:2.4105, loss val:2.4738\n",
      "step:5000, loss train:2.4007, loss val:2.4353\n",
      "step:5250, loss train:2.4198, loss val:2.4459\n",
      "step:5500, loss train:2.4084, loss val:2.4256\n",
      "step:5750, loss train:2.4187, loss val:2.4552\n",
      "step:6000, loss train:2.4065, loss val:2.4588\n",
      "step:6250, loss train:2.4124, loss val:2.4531\n",
      "step:6500, loss train:2.4235, loss val:2.4519\n",
      "step:6750, loss train:2.4291, loss val:2.4646\n",
      "step:7000, loss train:2.4135, loss val:2.4560\n",
      "step:7250, loss train:2.4323, loss val:2.4633\n",
      "step:7500, loss train:2.4087, loss val:2.4240\n",
      "step:7750, loss train:2.4101, loss val:2.4460\n",
      "step:8000, loss train:2.4058, loss val:2.4513\n",
      "step:8250, loss train:2.4004, loss val:2.4517\n",
      "step:8500, loss train:2.4156, loss val:2.4355\n",
      "step:8750, loss train:2.4231, loss val:2.4598\n",
      "step:9000, loss train:2.4019, loss val:2.4448\n",
      "step:9250, loss train:2.4139, loss val:2.4455\n",
      "step:9500, loss train:2.4190, loss val:2.4478\n",
      "step:9750, loss train:2.4006, loss val:2.4456\n",
      "batch:126\n",
      "step:0, loss train:2.3949, loss val:2.4699\n",
      "step:250, loss train:2.3991, loss val:2.4273\n",
      "step:500, loss train:2.4134, loss val:2.4474\n",
      "step:750, loss train:2.4374, loss val:2.4288\n",
      "step:1000, loss train:2.4151, loss val:2.4506\n",
      "step:1250, loss train:2.4107, loss val:2.4354\n",
      "step:1500, loss train:2.4184, loss val:2.4473\n",
      "step:1750, loss train:2.3994, loss val:2.4196\n",
      "step:2000, loss train:2.4041, loss val:2.4500\n",
      "step:2250, loss train:2.4080, loss val:2.4393\n",
      "step:2500, loss train:2.4064, loss val:2.4476\n",
      "step:2750, loss train:2.4053, loss val:2.4346\n",
      "step:3000, loss train:2.4180, loss val:2.4441\n",
      "step:3250, loss train:2.4205, loss val:2.4286\n",
      "step:3500, loss train:2.4065, loss val:2.4311\n",
      "step:3750, loss train:2.4109, loss val:2.4661\n",
      "step:4000, loss train:2.4161, loss val:2.4537\n",
      "step:4250, loss train:2.4308, loss val:2.4623\n",
      "step:4500, loss train:2.4264, loss val:2.4477\n",
      "step:4750, loss train:2.4122, loss val:2.4470\n",
      "step:5000, loss train:2.4009, loss val:2.4411\n",
      "step:5250, loss train:2.4073, loss val:2.4440\n",
      "step:5500, loss train:2.4215, loss val:2.4405\n",
      "step:5750, loss train:2.4275, loss val:2.4650\n",
      "step:6000, loss train:2.4008, loss val:2.4382\n",
      "step:6250, loss train:2.4112, loss val:2.4343\n",
      "step:6500, loss train:2.4032, loss val:2.4313\n",
      "step:6750, loss train:2.4111, loss val:2.4534\n",
      "step:7000, loss train:2.4222, loss val:2.4563\n",
      "step:7250, loss train:2.4215, loss val:2.4512\n",
      "step:7500, loss train:2.4133, loss val:2.4542\n",
      "step:7750, loss train:2.4183, loss val:2.4577\n",
      "step:8000, loss train:2.4036, loss val:2.4631\n",
      "step:8250, loss train:2.3891, loss val:2.4686\n",
      "step:8500, loss train:2.4067, loss val:2.4392\n",
      "step:8750, loss train:2.4040, loss val:2.4462\n",
      "step:9000, loss train:2.4155, loss val:2.4536\n",
      "step:9250, loss train:2.4202, loss val:2.4407\n",
      "step:9500, loss train:2.4035, loss val:2.4404\n",
      "step:9750, loss train:2.4260, loss val:2.4505\n",
      "batch:127\n",
      "step:0, loss train:2.4181, loss val:2.4215\n",
      "step:250, loss train:2.4213, loss val:2.4272\n",
      "step:500, loss train:2.4095, loss val:2.4520\n",
      "step:750, loss train:2.4034, loss val:2.4414\n",
      "step:1000, loss train:2.4171, loss val:2.4518\n",
      "step:1250, loss train:2.4001, loss val:2.4453\n",
      "step:1500, loss train:2.4079, loss val:2.4789\n",
      "step:1750, loss train:2.4218, loss val:2.4283\n",
      "step:2000, loss train:2.4232, loss val:2.4305\n",
      "step:2250, loss train:2.4140, loss val:2.4401\n",
      "step:2500, loss train:2.4042, loss val:2.4506\n",
      "step:2750, loss train:2.3909, loss val:2.4300\n",
      "step:3000, loss train:2.4102, loss val:2.4585\n",
      "step:3250, loss train:2.4198, loss val:2.4250\n",
      "step:3500, loss train:2.4189, loss val:2.4368\n",
      "step:3750, loss train:2.4137, loss val:2.4459\n",
      "step:4000, loss train:2.4168, loss val:2.4192\n",
      "step:4250, loss train:2.4127, loss val:2.4529\n",
      "step:4500, loss train:2.4046, loss val:2.4483\n",
      "step:4750, loss train:2.4161, loss val:2.4465\n",
      "step:5000, loss train:2.4181, loss val:2.4473\n",
      "step:5250, loss train:2.4176, loss val:2.4406\n",
      "step:5500, loss train:2.4057, loss val:2.4379\n",
      "step:5750, loss train:2.4047, loss val:2.4340\n",
      "step:6000, loss train:2.4112, loss val:2.4501\n",
      "step:6250, loss train:2.4080, loss val:2.4496\n",
      "step:6500, loss train:2.4209, loss val:2.4451\n",
      "step:6750, loss train:2.4146, loss val:2.4676\n",
      "step:7000, loss train:2.3947, loss val:2.4511\n",
      "step:7250, loss train:2.4195, loss val:2.4558\n",
      "step:7500, loss train:2.4028, loss val:2.4406\n",
      "step:7750, loss train:2.4200, loss val:2.4642\n",
      "step:8000, loss train:2.4046, loss val:2.4491\n",
      "step:8250, loss train:2.3913, loss val:2.4398\n",
      "step:8500, loss train:2.4275, loss val:2.4568\n",
      "step:8750, loss train:2.4157, loss val:2.4269\n",
      "step:9000, loss train:2.4265, loss val:2.4316\n",
      "step:9250, loss train:2.4325, loss val:2.4372\n",
      "step:9500, loss train:2.4122, loss val:2.4172\n",
      "step:9750, loss train:2.4105, loss val:2.4455\n",
      "batch:128\n",
      "step:0, loss train:2.4135, loss val:2.4610\n",
      "step:250, loss train:2.4043, loss val:2.4616\n",
      "step:500, loss train:2.4165, loss val:2.4419\n",
      "step:750, loss train:2.4230, loss val:2.4454\n",
      "step:1000, loss train:2.4071, loss val:2.4492\n",
      "step:1250, loss train:2.4108, loss val:2.4341\n",
      "step:1500, loss train:2.4264, loss val:2.4362\n",
      "step:1750, loss train:2.4089, loss val:2.4695\n",
      "step:2000, loss train:2.4166, loss val:2.4394\n",
      "step:2250, loss train:2.4180, loss val:2.4540\n",
      "step:2500, loss train:2.4241, loss val:2.4314\n",
      "step:2750, loss train:2.4054, loss val:2.4543\n",
      "step:3000, loss train:2.4241, loss val:2.4480\n",
      "step:3250, loss train:2.3926, loss val:2.4368\n",
      "step:3500, loss train:2.4011, loss val:2.4255\n",
      "step:3750, loss train:2.4080, loss val:2.4587\n",
      "step:4000, loss train:2.4192, loss val:2.4547\n",
      "step:4250, loss train:2.4071, loss val:2.4380\n",
      "step:4500, loss train:2.4177, loss val:2.4387\n",
      "step:4750, loss train:2.4226, loss val:2.4474\n",
      "step:5000, loss train:2.4237, loss val:2.4444\n",
      "step:5250, loss train:2.4098, loss val:2.4450\n",
      "step:5500, loss train:2.4195, loss val:2.4518\n",
      "step:5750, loss train:2.4141, loss val:2.4650\n",
      "step:6000, loss train:2.4062, loss val:2.4437\n",
      "step:6250, loss train:2.4318, loss val:2.4490\n",
      "step:6500, loss train:2.4128, loss val:2.4339\n",
      "step:6750, loss train:2.4102, loss val:2.4402\n",
      "step:7000, loss train:2.3972, loss val:2.4609\n",
      "step:7250, loss train:2.4242, loss val:2.4536\n",
      "step:7500, loss train:2.4170, loss val:2.4445\n",
      "step:7750, loss train:2.4246, loss val:2.4290\n",
      "step:8000, loss train:2.3975, loss val:2.4579\n",
      "step:8250, loss train:2.4028, loss val:2.4516\n",
      "step:8500, loss train:2.4146, loss val:2.4229\n",
      "step:8750, loss train:2.4198, loss val:2.4637\n",
      "step:9000, loss train:2.4055, loss val:2.4326\n",
      "step:9250, loss train:2.4211, loss val:2.4394\n",
      "step:9500, loss train:2.4170, loss val:2.4468\n",
      "step:9750, loss train:2.4257, loss val:2.4658\n",
      "batch:129\n",
      "step:0, loss train:2.4086, loss val:2.4184\n",
      "step:250, loss train:2.4237, loss val:2.4434\n",
      "step:500, loss train:2.4171, loss val:2.4448\n",
      "step:750, loss train:2.4407, loss val:2.4399\n",
      "step:1000, loss train:2.4254, loss val:2.4415\n",
      "step:1250, loss train:2.3755, loss val:2.4410\n",
      "step:1500, loss train:2.4301, loss val:2.4492\n",
      "step:1750, loss train:2.4239, loss val:2.4457\n",
      "step:2000, loss train:2.4182, loss val:2.4497\n",
      "step:2250, loss train:2.4115, loss val:2.4569\n",
      "step:2500, loss train:2.4203, loss val:2.4493\n",
      "step:2750, loss train:2.3987, loss val:2.4244\n",
      "step:3000, loss train:2.4195, loss val:2.4266\n",
      "step:3250, loss train:2.4131, loss val:2.4508\n",
      "step:3500, loss train:2.4270, loss val:2.4433\n",
      "step:3750, loss train:2.4033, loss val:2.4464\n",
      "step:4000, loss train:2.4093, loss val:2.4453\n",
      "step:4250, loss train:2.4196, loss val:2.4226\n",
      "step:4500, loss train:2.4350, loss val:2.4337\n",
      "step:4750, loss train:2.4068, loss val:2.4588\n",
      "step:5000, loss train:2.4023, loss val:2.4431\n",
      "step:5250, loss train:2.4117, loss val:2.4477\n",
      "step:5500, loss train:2.4131, loss val:2.4487\n",
      "step:5750, loss train:2.4103, loss val:2.4245\n",
      "step:6000, loss train:2.4104, loss val:2.4600\n",
      "step:6250, loss train:2.4116, loss val:2.4523\n",
      "step:6500, loss train:2.4100, loss val:2.4664\n",
      "step:6750, loss train:2.4142, loss val:2.4498\n",
      "step:7000, loss train:2.4070, loss val:2.4354\n",
      "step:7250, loss train:2.4028, loss val:2.4619\n",
      "step:7500, loss train:2.4184, loss val:2.4352\n",
      "step:7750, loss train:2.4313, loss val:2.4698\n",
      "step:8000, loss train:2.3991, loss val:2.4416\n",
      "step:8250, loss train:2.4318, loss val:2.4450\n",
      "step:8500, loss train:2.3951, loss val:2.4398\n",
      "step:8750, loss train:2.3986, loss val:2.4539\n",
      "step:9000, loss train:2.4184, loss val:2.4654\n",
      "step:9250, loss train:2.4076, loss val:2.4513\n",
      "step:9500, loss train:2.4120, loss val:2.4310\n",
      "step:9750, loss train:2.4022, loss val:2.4375\n",
      "batch:130\n",
      "step:0, loss train:2.4376, loss val:2.4503\n",
      "step:250, loss train:2.4182, loss val:2.4422\n",
      "step:500, loss train:2.4107, loss val:2.4598\n",
      "step:750, loss train:2.4103, loss val:2.4518\n",
      "step:1000, loss train:2.4138, loss val:2.4516\n",
      "step:1250, loss train:2.4092, loss val:2.4741\n",
      "step:1500, loss train:2.4086, loss val:2.4350\n",
      "step:1750, loss train:2.4198, loss val:2.4599\n",
      "step:2000, loss train:2.4092, loss val:2.4460\n",
      "step:2250, loss train:2.4278, loss val:2.4259\n",
      "step:2500, loss train:2.4040, loss val:2.4512\n",
      "step:2750, loss train:2.4208, loss val:2.4535\n",
      "step:3000, loss train:2.4122, loss val:2.4397\n",
      "step:3250, loss train:2.4084, loss val:2.4407\n",
      "step:3500, loss train:2.4211, loss val:2.4578\n",
      "step:3750, loss train:2.4144, loss val:2.4574\n",
      "step:4000, loss train:2.3990, loss val:2.4623\n",
      "step:4250, loss train:2.4194, loss val:2.4432\n",
      "step:4500, loss train:2.4186, loss val:2.4571\n",
      "step:4750, loss train:2.4103, loss val:2.4425\n",
      "step:5000, loss train:2.4170, loss val:2.4283\n",
      "step:5250, loss train:2.4239, loss val:2.4612\n",
      "step:5500, loss train:2.4096, loss val:2.4503\n",
      "step:5750, loss train:2.4189, loss val:2.4377\n",
      "step:6000, loss train:2.4198, loss val:2.4637\n",
      "step:6250, loss train:2.4029, loss val:2.4362\n",
      "step:6500, loss train:2.4099, loss val:2.4308\n",
      "step:6750, loss train:2.4243, loss val:2.4641\n",
      "step:7000, loss train:2.4183, loss val:2.4242\n",
      "step:7250, loss train:2.4001, loss val:2.4393\n",
      "step:7500, loss train:2.4160, loss val:2.4680\n",
      "step:7750, loss train:2.4238, loss val:2.4425\n",
      "step:8000, loss train:2.4209, loss val:2.4445\n",
      "step:8250, loss train:2.4121, loss val:2.4536\n",
      "step:8500, loss train:2.4019, loss val:2.4143\n",
      "step:8750, loss train:2.4019, loss val:2.4356\n",
      "step:9000, loss train:2.4025, loss val:2.4399\n",
      "step:9250, loss train:2.4163, loss val:2.4424\n",
      "step:9500, loss train:2.4104, loss val:2.4445\n",
      "step:9750, loss train:2.4102, loss val:2.4469\n",
      "batch:131\n",
      "step:0, loss train:2.4178, loss val:2.4372\n",
      "step:250, loss train:2.4098, loss val:2.4415\n",
      "step:500, loss train:2.4186, loss val:2.4463\n",
      "step:750, loss train:2.4038, loss val:2.4444\n",
      "step:1000, loss train:2.4122, loss val:2.4455\n",
      "step:1250, loss train:2.4106, loss val:2.4479\n",
      "step:1500, loss train:2.4028, loss val:2.4223\n",
      "step:1750, loss train:2.4035, loss val:2.4662\n",
      "step:2000, loss train:2.4035, loss val:2.4450\n",
      "step:2250, loss train:2.4161, loss val:2.4251\n",
      "step:2500, loss train:2.4071, loss val:2.4384\n",
      "step:2750, loss train:2.4317, loss val:2.4425\n",
      "step:3000, loss train:2.3899, loss val:2.4572\n",
      "step:3250, loss train:2.4129, loss val:2.4408\n",
      "step:3500, loss train:2.3997, loss val:2.4333\n",
      "step:3750, loss train:2.4023, loss val:2.4374\n",
      "step:4000, loss train:2.4073, loss val:2.4381\n",
      "step:4250, loss train:2.4142, loss val:2.4455\n",
      "step:4500, loss train:2.4098, loss val:2.4764\n",
      "step:4750, loss train:2.4083, loss val:2.4422\n",
      "step:5000, loss train:2.4022, loss val:2.4494\n",
      "step:5250, loss train:2.3942, loss val:2.4660\n",
      "step:5500, loss train:2.4217, loss val:2.4381\n",
      "step:5750, loss train:2.4015, loss val:2.4491\n",
      "step:6000, loss train:2.4149, loss val:2.4156\n",
      "step:6250, loss train:2.4132, loss val:2.4300\n",
      "step:6500, loss train:2.4290, loss val:2.4485\n",
      "step:6750, loss train:2.4152, loss val:2.4247\n",
      "step:7000, loss train:2.4023, loss val:2.4780\n",
      "step:7250, loss train:2.4037, loss val:2.4305\n",
      "step:7500, loss train:2.4061, loss val:2.4481\n",
      "step:7750, loss train:2.4124, loss val:2.4453\n",
      "step:8000, loss train:2.4052, loss val:2.4425\n",
      "step:8250, loss train:2.4112, loss val:2.4625\n",
      "step:8500, loss train:2.4086, loss val:2.4586\n",
      "step:8750, loss train:2.4112, loss val:2.4476\n",
      "step:9000, loss train:2.4095, loss val:2.4338\n",
      "step:9250, loss train:2.4051, loss val:2.4319\n",
      "step:9500, loss train:2.4260, loss val:2.4452\n",
      "step:9750, loss train:2.3937, loss val:2.4423\n",
      "batch:132\n",
      "step:0, loss train:2.4176, loss val:2.4261\n",
      "step:250, loss train:2.4042, loss val:2.4431\n",
      "step:500, loss train:2.4184, loss val:2.4483\n",
      "step:750, loss train:2.4096, loss val:2.4723\n",
      "step:1000, loss train:2.4020, loss val:2.4594\n",
      "step:1250, loss train:2.4125, loss val:2.4541\n",
      "step:1500, loss train:2.4232, loss val:2.4487\n",
      "step:1750, loss train:2.4253, loss val:2.4410\n",
      "step:2000, loss train:2.4072, loss val:2.4510\n",
      "step:2250, loss train:2.4194, loss val:2.4457\n",
      "step:2500, loss train:2.4087, loss val:2.4282\n",
      "step:2750, loss train:2.4083, loss val:2.4277\n",
      "step:3000, loss train:2.4267, loss val:2.4466\n",
      "step:3250, loss train:2.4133, loss val:2.4492\n",
      "step:3500, loss train:2.4252, loss val:2.4537\n",
      "step:3750, loss train:2.3887, loss val:2.4361\n",
      "step:4000, loss train:2.4176, loss val:2.4515\n",
      "step:4250, loss train:2.4069, loss val:2.4479\n",
      "step:4500, loss train:2.4253, loss val:2.4679\n",
      "step:4750, loss train:2.4023, loss val:2.4337\n",
      "step:5000, loss train:2.4198, loss val:2.4655\n",
      "step:5250, loss train:2.4182, loss val:2.4433\n",
      "step:5500, loss train:2.4066, loss val:2.4436\n",
      "step:5750, loss train:2.4130, loss val:2.4423\n",
      "step:6000, loss train:2.4191, loss val:2.4537\n",
      "step:6250, loss train:2.4148, loss val:2.4475\n",
      "step:6500, loss train:2.4098, loss val:2.4395\n",
      "step:6750, loss train:2.4004, loss val:2.4392\n",
      "step:7000, loss train:2.4004, loss val:2.4416\n",
      "step:7250, loss train:2.3934, loss val:2.4501\n",
      "step:7500, loss train:2.4092, loss val:2.4399\n",
      "step:7750, loss train:2.4064, loss val:2.4474\n",
      "step:8000, loss train:2.4111, loss val:2.4470\n",
      "step:8250, loss train:2.4381, loss val:2.4558\n",
      "step:8500, loss train:2.4030, loss val:2.4386\n",
      "step:8750, loss train:2.4029, loss val:2.4498\n",
      "step:9000, loss train:2.4014, loss val:2.4304\n",
      "step:9250, loss train:2.3954, loss val:2.4411\n",
      "step:9500, loss train:2.3989, loss val:2.4475\n",
      "step:9750, loss train:2.4173, loss val:2.4299\n",
      "batch:133\n",
      "step:0, loss train:2.4252, loss val:2.4243\n",
      "step:250, loss train:2.4049, loss val:2.4308\n",
      "step:500, loss train:2.4040, loss val:2.4317\n",
      "step:750, loss train:2.4122, loss val:2.4441\n",
      "step:1000, loss train:2.4234, loss val:2.4437\n",
      "step:1250, loss train:2.4120, loss val:2.4141\n",
      "step:1500, loss train:2.4151, loss val:2.4552\n",
      "step:1750, loss train:2.3999, loss val:2.4312\n",
      "step:2000, loss train:2.3977, loss val:2.4367\n",
      "step:2250, loss train:2.4305, loss val:2.4495\n",
      "step:2500, loss train:2.4144, loss val:2.4279\n",
      "step:2750, loss train:2.4144, loss val:2.4355\n",
      "step:3000, loss train:2.3958, loss val:2.4574\n",
      "step:3250, loss train:2.4090, loss val:2.4621\n",
      "step:3500, loss train:2.4180, loss val:2.4536\n",
      "step:3750, loss train:2.4061, loss val:2.4516\n",
      "step:4000, loss train:2.4274, loss val:2.4346\n",
      "step:4250, loss train:2.4255, loss val:2.4549\n",
      "step:4500, loss train:2.4172, loss val:2.4533\n",
      "step:4750, loss train:2.4026, loss val:2.4256\n",
      "step:5000, loss train:2.4134, loss val:2.4484\n",
      "step:5250, loss train:2.4160, loss val:2.4348\n",
      "step:5500, loss train:2.4055, loss val:2.4284\n",
      "step:5750, loss train:2.4020, loss val:2.4431\n",
      "step:6000, loss train:2.3999, loss val:2.4557\n",
      "step:6250, loss train:2.4197, loss val:2.4729\n",
      "step:6500, loss train:2.4296, loss val:2.4472\n",
      "step:6750, loss train:2.4075, loss val:2.4381\n",
      "step:7000, loss train:2.4023, loss val:2.4525\n",
      "step:7250, loss train:2.4124, loss val:2.4526\n",
      "step:7500, loss train:2.4025, loss val:2.4565\n",
      "step:7750, loss train:2.4013, loss val:2.4565\n",
      "step:8000, loss train:2.4239, loss val:2.4492\n",
      "step:8250, loss train:2.4058, loss val:2.4469\n",
      "step:8500, loss train:2.4228, loss val:2.4334\n",
      "step:8750, loss train:2.3986, loss val:2.4397\n",
      "step:9000, loss train:2.4141, loss val:2.4330\n",
      "step:9250, loss train:2.4020, loss val:2.4447\n",
      "step:9500, loss train:2.4086, loss val:2.4503\n",
      "step:9750, loss train:2.4032, loss val:2.4482\n",
      "batch:134\n",
      "step:0, loss train:2.4024, loss val:2.4339\n",
      "step:250, loss train:2.4135, loss val:2.4393\n",
      "step:500, loss train:2.4161, loss val:2.4528\n",
      "step:750, loss train:2.4144, loss val:2.4361\n",
      "step:1000, loss train:2.4380, loss val:2.4495\n",
      "step:1250, loss train:2.4154, loss val:2.4309\n",
      "step:1500, loss train:2.3987, loss val:2.4402\n",
      "step:1750, loss train:2.4061, loss val:2.4490\n",
      "step:2000, loss train:2.4326, loss val:2.4666\n",
      "step:2250, loss train:2.4071, loss val:2.4608\n",
      "step:2500, loss train:2.4052, loss val:2.4683\n",
      "step:2750, loss train:2.4091, loss val:2.4518\n",
      "step:3000, loss train:2.3975, loss val:2.4407\n",
      "step:3250, loss train:2.4161, loss val:2.4553\n",
      "step:3500, loss train:2.4051, loss val:2.4266\n",
      "step:3750, loss train:2.4146, loss val:2.4384\n",
      "step:4000, loss train:2.4237, loss val:2.4568\n",
      "step:4250, loss train:2.3984, loss val:2.4380\n",
      "step:4500, loss train:2.4195, loss val:2.4289\n",
      "step:4750, loss train:2.4161, loss val:2.4663\n",
      "step:5000, loss train:2.4204, loss val:2.4495\n",
      "step:5250, loss train:2.4268, loss val:2.4378\n",
      "step:5500, loss train:2.4191, loss val:2.4589\n",
      "step:5750, loss train:2.4001, loss val:2.4476\n",
      "step:6000, loss train:2.4338, loss val:2.4603\n",
      "step:6250, loss train:2.4123, loss val:2.4583\n",
      "step:6500, loss train:2.4359, loss val:2.4655\n",
      "step:6750, loss train:2.4088, loss val:2.4270\n",
      "step:7000, loss train:2.4161, loss val:2.4659\n",
      "step:7250, loss train:2.3871, loss val:2.4612\n",
      "step:7500, loss train:2.4244, loss val:2.4456\n",
      "step:7750, loss train:2.4249, loss val:2.4413\n",
      "step:8000, loss train:2.4067, loss val:2.4557\n",
      "step:8250, loss train:2.4222, loss val:2.4374\n",
      "step:8500, loss train:2.4234, loss val:2.4669\n",
      "step:8750, loss train:2.4232, loss val:2.4338\n",
      "step:9000, loss train:2.4184, loss val:2.4406\n",
      "step:9250, loss train:2.4120, loss val:2.4542\n",
      "step:9500, loss train:2.4159, loss val:2.4355\n",
      "step:9750, loss train:2.4155, loss val:2.4489\n",
      "batch:135\n",
      "step:0, loss train:2.4059, loss val:2.4288\n",
      "step:250, loss train:2.4067, loss val:2.4448\n",
      "step:500, loss train:2.4128, loss val:2.4368\n",
      "step:750, loss train:2.4033, loss val:2.4643\n",
      "step:1000, loss train:2.4296, loss val:2.4429\n",
      "step:1250, loss train:2.4080, loss val:2.4318\n",
      "step:1500, loss train:2.4024, loss val:2.4617\n",
      "step:1750, loss train:2.4093, loss val:2.4593\n",
      "step:2000, loss train:2.4215, loss val:2.4496\n",
      "step:2250, loss train:2.4087, loss val:2.4502\n",
      "step:2500, loss train:2.4137, loss val:2.4490\n",
      "step:2750, loss train:2.4179, loss val:2.4492\n",
      "step:3000, loss train:2.3893, loss val:2.4412\n",
      "step:3250, loss train:2.4194, loss val:2.4425\n",
      "step:3500, loss train:2.4225, loss val:2.4108\n",
      "step:3750, loss train:2.4232, loss val:2.4405\n",
      "step:4000, loss train:2.3980, loss val:2.4431\n",
      "step:4250, loss train:2.4248, loss val:2.4429\n",
      "step:4500, loss train:2.4180, loss val:2.4395\n",
      "step:4750, loss train:2.4206, loss val:2.4428\n",
      "step:5000, loss train:2.4196, loss val:2.4572\n",
      "step:5250, loss train:2.3957, loss val:2.4452\n",
      "step:5500, loss train:2.4193, loss val:2.4439\n",
      "step:5750, loss train:2.4121, loss val:2.4514\n",
      "step:6000, loss train:2.4065, loss val:2.4370\n",
      "step:6250, loss train:2.4163, loss val:2.4703\n",
      "step:6500, loss train:2.3947, loss val:2.4392\n",
      "step:6750, loss train:2.4012, loss val:2.4357\n",
      "step:7000, loss train:2.4088, loss val:2.4451\n",
      "step:7250, loss train:2.4094, loss val:2.4690\n",
      "step:7500, loss train:2.4207, loss val:2.4506\n",
      "step:7750, loss train:2.4249, loss val:2.4369\n",
      "step:8000, loss train:2.4309, loss val:2.4638\n",
      "step:8250, loss train:2.4023, loss val:2.4467\n",
      "step:8500, loss train:2.3856, loss val:2.4554\n",
      "step:8750, loss train:2.4020, loss val:2.4652\n",
      "step:9000, loss train:2.4066, loss val:2.4285\n",
      "step:9250, loss train:2.4390, loss val:2.4493\n",
      "step:9500, loss train:2.4203, loss val:2.4430\n",
      "step:9750, loss train:2.4267, loss val:2.4440\n",
      "batch:136\n",
      "step:0, loss train:2.4081, loss val:2.4480\n",
      "step:250, loss train:2.4187, loss val:2.4413\n",
      "step:500, loss train:2.3996, loss val:2.4191\n",
      "step:750, loss train:2.4082, loss val:2.4448\n",
      "step:1000, loss train:2.4149, loss val:2.4223\n",
      "step:1250, loss train:2.4097, loss val:2.4536\n",
      "step:1500, loss train:2.4049, loss val:2.4626\n",
      "step:1750, loss train:2.4077, loss val:2.4560\n",
      "step:2000, loss train:2.4050, loss val:2.4584\n",
      "step:2250, loss train:2.4179, loss val:2.4469\n",
      "step:2500, loss train:2.4057, loss val:2.4371\n",
      "step:2750, loss train:2.4158, loss val:2.4468\n",
      "step:3000, loss train:2.3981, loss val:2.4349\n",
      "step:3250, loss train:2.4203, loss val:2.4788\n",
      "step:3500, loss train:2.4104, loss val:2.4175\n",
      "step:3750, loss train:2.4171, loss val:2.4536\n",
      "step:4000, loss train:2.4241, loss val:2.4444\n",
      "step:4250, loss train:2.4071, loss val:2.4420\n",
      "step:4500, loss train:2.4213, loss val:2.4509\n",
      "step:4750, loss train:2.4073, loss val:2.4424\n",
      "step:5000, loss train:2.4112, loss val:2.4416\n",
      "step:5250, loss train:2.4089, loss val:2.4567\n",
      "step:5500, loss train:2.4161, loss val:2.4341\n",
      "step:5750, loss train:2.4182, loss val:2.4440\n",
      "step:6000, loss train:2.4111, loss val:2.4230\n",
      "step:6250, loss train:2.4098, loss val:2.4363\n",
      "step:6500, loss train:2.3962, loss val:2.4413\n",
      "step:6750, loss train:2.3953, loss val:2.4687\n",
      "step:7000, loss train:2.4328, loss val:2.4567\n",
      "step:7250, loss train:2.4369, loss val:2.4458\n",
      "step:7500, loss train:2.3928, loss val:2.4491\n",
      "step:7750, loss train:2.4192, loss val:2.4320\n",
      "step:8000, loss train:2.4288, loss val:2.4385\n",
      "step:8250, loss train:2.4062, loss val:2.4476\n",
      "step:8500, loss train:2.4096, loss val:2.4382\n",
      "step:8750, loss train:2.4243, loss val:2.4540\n",
      "step:9000, loss train:2.4279, loss val:2.4627\n",
      "step:9250, loss train:2.4135, loss val:2.4587\n",
      "step:9500, loss train:2.4134, loss val:2.4540\n",
      "step:9750, loss train:2.4188, loss val:2.4401\n",
      "batch:137\n",
      "step:0, loss train:2.4088, loss val:2.4540\n",
      "step:250, loss train:2.4184, loss val:2.4721\n",
      "step:500, loss train:2.4018, loss val:2.4438\n",
      "step:750, loss train:2.4144, loss val:2.4630\n",
      "step:1000, loss train:2.4054, loss val:2.4673\n",
      "step:1250, loss train:2.4152, loss val:2.4554\n",
      "step:1500, loss train:2.4014, loss val:2.4554\n",
      "step:1750, loss train:2.3998, loss val:2.4667\n",
      "step:2000, loss train:2.4178, loss val:2.4583\n",
      "step:2250, loss train:2.4161, loss val:2.4401\n",
      "step:2500, loss train:2.4022, loss val:2.4371\n",
      "step:2750, loss train:2.4148, loss val:2.4130\n",
      "step:3000, loss train:2.4246, loss val:2.4527\n",
      "step:3250, loss train:2.4149, loss val:2.4456\n",
      "step:3500, loss train:2.3995, loss val:2.4549\n",
      "step:3750, loss train:2.4226, loss val:2.4446\n",
      "step:4000, loss train:2.4218, loss val:2.4560\n",
      "step:4250, loss train:2.4057, loss val:2.4578\n",
      "step:4500, loss train:2.4382, loss val:2.4384\n",
      "step:4750, loss train:2.3975, loss val:2.4596\n",
      "step:5000, loss train:2.4092, loss val:2.4739\n",
      "step:5250, loss train:2.4224, loss val:2.4566\n",
      "step:5500, loss train:2.4029, loss val:2.4443\n",
      "step:5750, loss train:2.4077, loss val:2.4472\n",
      "step:6000, loss train:2.4142, loss val:2.4081\n",
      "step:6250, loss train:2.4136, loss val:2.4210\n",
      "step:6500, loss train:2.4153, loss val:2.4535\n",
      "step:6750, loss train:2.4190, loss val:2.4327\n",
      "step:7000, loss train:2.4181, loss val:2.4420\n",
      "step:7250, loss train:2.4233, loss val:2.4477\n",
      "step:7500, loss train:2.4229, loss val:2.4466\n",
      "step:7750, loss train:2.4151, loss val:2.4592\n",
      "step:8000, loss train:2.4199, loss val:2.4556\n",
      "step:8250, loss train:2.4075, loss val:2.4703\n",
      "step:8500, loss train:2.3988, loss val:2.4338\n",
      "step:8750, loss train:2.4072, loss val:2.4446\n",
      "step:9000, loss train:2.4139, loss val:2.4495\n",
      "step:9250, loss train:2.4082, loss val:2.4416\n",
      "step:9500, loss train:2.4142, loss val:2.4344\n",
      "step:9750, loss train:2.4219, loss val:2.4436\n",
      "batch:138\n",
      "step:0, loss train:2.4248, loss val:2.4569\n",
      "step:250, loss train:2.4104, loss val:2.4421\n",
      "step:500, loss train:2.3957, loss val:2.4576\n",
      "step:750, loss train:2.4110, loss val:2.4449\n",
      "step:1000, loss train:2.4103, loss val:2.4525\n",
      "step:1250, loss train:2.4040, loss val:2.4345\n",
      "step:1500, loss train:2.4086, loss val:2.4461\n",
      "step:1750, loss train:2.4101, loss val:2.4373\n",
      "step:2000, loss train:2.4072, loss val:2.4681\n",
      "step:2250, loss train:2.4183, loss val:2.4705\n",
      "step:2500, loss train:2.4028, loss val:2.4422\n",
      "step:2750, loss train:2.4204, loss val:2.4435\n",
      "step:3000, loss train:2.4027, loss val:2.4724\n",
      "step:3250, loss train:2.4151, loss val:2.4464\n",
      "step:3500, loss train:2.4128, loss val:2.4695\n",
      "step:3750, loss train:2.4000, loss val:2.4251\n",
      "step:4000, loss train:2.4114, loss val:2.4594\n",
      "step:4250, loss train:2.4143, loss val:2.4733\n",
      "step:4500, loss train:2.4209, loss val:2.4632\n",
      "step:4750, loss train:2.4093, loss val:2.4583\n",
      "step:5000, loss train:2.4110, loss val:2.4553\n",
      "step:5250, loss train:2.3935, loss val:2.4439\n",
      "step:5500, loss train:2.4095, loss val:2.4379\n",
      "step:5750, loss train:2.4163, loss val:2.4712\n",
      "step:6000, loss train:2.4028, loss val:2.4552\n",
      "step:6250, loss train:2.4350, loss val:2.4506\n",
      "step:6500, loss train:2.4187, loss val:2.4457\n",
      "step:6750, loss train:2.3993, loss val:2.4265\n",
      "step:7000, loss train:2.3994, loss val:2.4459\n",
      "step:7250, loss train:2.4165, loss val:2.4097\n",
      "step:7500, loss train:2.4115, loss val:2.4509\n",
      "step:7750, loss train:2.4167, loss val:2.4288\n",
      "step:8000, loss train:2.4107, loss val:2.4502\n",
      "step:8250, loss train:2.4238, loss val:2.4282\n",
      "step:8500, loss train:2.4227, loss val:2.4641\n",
      "step:8750, loss train:2.4111, loss val:2.4504\n",
      "step:9000, loss train:2.4149, loss val:2.4506\n",
      "step:9250, loss train:2.3958, loss val:2.4464\n",
      "step:9500, loss train:2.4035, loss val:2.4649\n",
      "step:9750, loss train:2.4169, loss val:2.4511\n",
      "batch:139\n",
      "step:0, loss train:2.4252, loss val:2.4588\n",
      "step:250, loss train:2.3900, loss val:2.4499\n",
      "step:500, loss train:2.4135, loss val:2.4661\n",
      "step:750, loss train:2.4275, loss val:2.4720\n",
      "step:1000, loss train:2.4359, loss val:2.4277\n",
      "step:1250, loss train:2.4104, loss val:2.4461\n",
      "step:1500, loss train:2.4131, loss val:2.4498\n",
      "step:1750, loss train:2.4133, loss val:2.4326\n",
      "step:2000, loss train:2.4300, loss val:2.4571\n",
      "step:2250, loss train:2.4193, loss val:2.4459\n",
      "step:2500, loss train:2.4269, loss val:2.4431\n",
      "step:2750, loss train:2.4112, loss val:2.4269\n",
      "step:3000, loss train:2.4072, loss val:2.4351\n",
      "step:3250, loss train:2.4043, loss val:2.4491\n",
      "step:3500, loss train:2.4158, loss val:2.4677\n",
      "step:3750, loss train:2.4351, loss val:2.4410\n",
      "step:4000, loss train:2.4070, loss val:2.4429\n",
      "step:4250, loss train:2.4044, loss val:2.4379\n",
      "step:4500, loss train:2.4172, loss val:2.4184\n",
      "step:4750, loss train:2.4168, loss val:2.4533\n",
      "step:5000, loss train:2.4116, loss val:2.4602\n",
      "step:5250, loss train:2.4286, loss val:2.4577\n",
      "step:5500, loss train:2.4103, loss val:2.4542\n",
      "step:5750, loss train:2.4176, loss val:2.4307\n",
      "step:6000, loss train:2.4141, loss val:2.4440\n",
      "step:6250, loss train:2.4095, loss val:2.4663\n",
      "step:6500, loss train:2.4139, loss val:2.4315\n",
      "step:6750, loss train:2.4110, loss val:2.4415\n",
      "step:7000, loss train:2.4113, loss val:2.4545\n",
      "step:7250, loss train:2.4036, loss val:2.4300\n",
      "step:7500, loss train:2.4210, loss val:2.4472\n",
      "step:7750, loss train:2.4045, loss val:2.4612\n",
      "step:8000, loss train:2.3993, loss val:2.4399\n",
      "step:8250, loss train:2.4153, loss val:2.4553\n",
      "step:8500, loss train:2.4030, loss val:2.4517\n",
      "step:8750, loss train:2.4111, loss val:2.4300\n",
      "step:9000, loss train:2.4125, loss val:2.4425\n",
      "step:9250, loss train:2.4352, loss val:2.4559\n",
      "step:9500, loss train:2.4206, loss val:2.4349\n",
      "step:9750, loss train:2.4171, loss val:2.4451\n",
      "batch:140\n",
      "step:0, loss train:2.4178, loss val:2.4514\n",
      "step:250, loss train:2.3986, loss val:2.4617\n",
      "step:500, loss train:2.4155, loss val:2.4397\n",
      "step:750, loss train:2.4081, loss val:2.4610\n",
      "step:1000, loss train:2.4223, loss val:2.4624\n",
      "step:1250, loss train:2.4159, loss val:2.4824\n",
      "step:1500, loss train:2.4050, loss val:2.4574\n",
      "step:1750, loss train:2.4203, loss val:2.4414\n",
      "step:2000, loss train:2.4154, loss val:2.4412\n",
      "step:2250, loss train:2.4200, loss val:2.4284\n",
      "step:2500, loss train:2.4120, loss val:2.4724\n",
      "step:2750, loss train:2.3917, loss val:2.4529\n",
      "step:3000, loss train:2.4209, loss val:2.4241\n",
      "step:3250, loss train:2.4163, loss val:2.4397\n",
      "step:3500, loss train:2.4276, loss val:2.4376\n",
      "step:3750, loss train:2.4257, loss val:2.4627\n",
      "step:4000, loss train:2.4107, loss val:2.4623\n",
      "step:4250, loss train:2.4262, loss val:2.4472\n",
      "step:4500, loss train:2.4099, loss val:2.4594\n",
      "step:4750, loss train:2.4207, loss val:2.4655\n",
      "step:5000, loss train:2.3927, loss val:2.4640\n",
      "step:5250, loss train:2.4200, loss val:2.4495\n",
      "step:5500, loss train:2.4088, loss val:2.4607\n",
      "step:5750, loss train:2.4023, loss val:2.4440\n",
      "step:6000, loss train:2.4209, loss val:2.4413\n",
      "step:6250, loss train:2.4150, loss val:2.4568\n",
      "step:6500, loss train:2.4151, loss val:2.4627\n",
      "step:6750, loss train:2.4129, loss val:2.4549\n",
      "step:7000, loss train:2.4208, loss val:2.4623\n",
      "step:7250, loss train:2.3945, loss val:2.4376\n",
      "step:7500, loss train:2.4315, loss val:2.4652\n",
      "step:7750, loss train:2.4365, loss val:2.4422\n",
      "step:8000, loss train:2.4086, loss val:2.4496\n",
      "step:8250, loss train:2.4065, loss val:2.4325\n",
      "step:8500, loss train:2.4173, loss val:2.4477\n",
      "step:8750, loss train:2.4119, loss val:2.4448\n",
      "step:9000, loss train:2.3981, loss val:2.4512\n",
      "step:9250, loss train:2.4116, loss val:2.4243\n",
      "step:9500, loss train:2.4121, loss val:2.4214\n",
      "step:9750, loss train:2.4089, loss val:2.4494\n",
      "batch:141\n",
      "step:0, loss train:2.4271, loss val:2.4431\n",
      "step:250, loss train:2.4195, loss val:2.4642\n",
      "step:500, loss train:2.4107, loss val:2.4447\n",
      "step:750, loss train:2.4152, loss val:2.4539\n",
      "step:1000, loss train:2.4282, loss val:2.4646\n",
      "step:1250, loss train:2.4086, loss val:2.4425\n",
      "step:1500, loss train:2.4065, loss val:2.4374\n",
      "step:1750, loss train:2.3993, loss val:2.4424\n",
      "step:2000, loss train:2.4208, loss val:2.4312\n",
      "step:2250, loss train:2.3852, loss val:2.4495\n",
      "step:2500, loss train:2.4225, loss val:2.4572\n",
      "step:2750, loss train:2.4246, loss val:2.4402\n",
      "step:3000, loss train:2.4200, loss val:2.4581\n",
      "step:3250, loss train:2.4107, loss val:2.4505\n",
      "step:3500, loss train:2.4184, loss val:2.4463\n",
      "step:3750, loss train:2.4083, loss val:2.4396\n",
      "step:4000, loss train:2.4209, loss val:2.4394\n",
      "step:4250, loss train:2.4262, loss val:2.4647\n",
      "step:4500, loss train:2.4273, loss val:2.4312\n",
      "step:4750, loss train:2.4103, loss val:2.4403\n",
      "step:5000, loss train:2.4324, loss val:2.4536\n",
      "step:5250, loss train:2.4290, loss val:2.4340\n",
      "step:5500, loss train:2.4156, loss val:2.4616\n",
      "step:5750, loss train:2.4007, loss val:2.4601\n",
      "step:6000, loss train:2.4278, loss val:2.4387\n",
      "step:6250, loss train:2.4210, loss val:2.4414\n",
      "step:6500, loss train:2.4099, loss val:2.4560\n",
      "step:6750, loss train:2.4134, loss val:2.4664\n",
      "step:7000, loss train:2.4097, loss val:2.4520\n",
      "step:7250, loss train:2.4055, loss val:2.4476\n",
      "step:7500, loss train:2.4121, loss val:2.4332\n",
      "step:7750, loss train:2.4174, loss val:2.4629\n",
      "step:8000, loss train:2.4115, loss val:2.4480\n",
      "step:8250, loss train:2.4134, loss val:2.4362\n",
      "step:8500, loss train:2.4065, loss val:2.4440\n",
      "step:8750, loss train:2.4018, loss val:2.4741\n",
      "step:9000, loss train:2.4167, loss val:2.4557\n",
      "step:9250, loss train:2.3911, loss val:2.4416\n",
      "step:9500, loss train:2.4201, loss val:2.4264\n",
      "step:9750, loss train:2.3994, loss val:2.4528\n",
      "batch:142\n",
      "step:0, loss train:2.4239, loss val:2.4550\n",
      "step:250, loss train:2.4113, loss val:2.4351\n",
      "step:500, loss train:2.4293, loss val:2.4522\n",
      "step:750, loss train:2.4202, loss val:2.4699\n",
      "step:1000, loss train:2.4064, loss val:2.4420\n",
      "step:1250, loss train:2.4008, loss val:2.4303\n",
      "step:1500, loss train:2.4145, loss val:2.4538\n",
      "step:1750, loss train:2.4213, loss val:2.4464\n",
      "step:2000, loss train:2.4137, loss val:2.4105\n",
      "step:2250, loss train:2.4050, loss val:2.4511\n",
      "step:2500, loss train:2.3962, loss val:2.4661\n",
      "step:2750, loss train:2.4101, loss val:2.4586\n",
      "step:3000, loss train:2.4172, loss val:2.4630\n",
      "step:3250, loss train:2.4053, loss val:2.4702\n",
      "step:3500, loss train:2.4330, loss val:2.4629\n",
      "step:3750, loss train:2.3970, loss val:2.4331\n",
      "step:4000, loss train:2.4200, loss val:2.4548\n",
      "step:4250, loss train:2.4075, loss val:2.4444\n",
      "step:4500, loss train:2.3982, loss val:2.4421\n",
      "step:4750, loss train:2.4039, loss val:2.4325\n",
      "step:5000, loss train:2.4002, loss val:2.4556\n",
      "step:5250, loss train:2.4139, loss val:2.4365\n",
      "step:5500, loss train:2.4111, loss val:2.4420\n",
      "step:5750, loss train:2.4087, loss val:2.4355\n",
      "step:6000, loss train:2.4160, loss val:2.4420\n",
      "step:6250, loss train:2.3910, loss val:2.4311\n",
      "step:6500, loss train:2.4217, loss val:2.4445\n",
      "step:6750, loss train:2.4211, loss val:2.4620\n",
      "step:7000, loss train:2.4241, loss val:2.4396\n",
      "step:7250, loss train:2.4066, loss val:2.4464\n",
      "step:7500, loss train:2.4065, loss val:2.4393\n",
      "step:7750, loss train:2.4172, loss val:2.4541\n",
      "step:8000, loss train:2.4213, loss val:2.4383\n",
      "step:8250, loss train:2.4220, loss val:2.4534\n",
      "step:8500, loss train:2.4054, loss val:2.4403\n",
      "step:8750, loss train:2.4154, loss val:2.4569\n",
      "step:9000, loss train:2.4180, loss val:2.4416\n",
      "step:9250, loss train:2.4169, loss val:2.4385\n",
      "step:9500, loss train:2.3982, loss val:2.4372\n",
      "step:9750, loss train:2.4214, loss val:2.4494\n",
      "batch:143\n",
      "step:0, loss train:2.4263, loss val:2.4480\n",
      "step:250, loss train:2.4139, loss val:2.4551\n",
      "step:500, loss train:2.4078, loss val:2.4577\n",
      "step:750, loss train:2.4232, loss val:2.4700\n",
      "step:1000, loss train:2.4146, loss val:2.4314\n",
      "step:1250, loss train:2.3973, loss val:2.4371\n",
      "step:1500, loss train:2.4156, loss val:2.4603\n",
      "step:1750, loss train:2.4275, loss val:2.4648\n",
      "step:2000, loss train:2.4276, loss val:2.4356\n",
      "step:2250, loss train:2.4096, loss val:2.4450\n",
      "step:2500, loss train:2.4076, loss val:2.4506\n",
      "step:2750, loss train:2.4116, loss val:2.4551\n",
      "step:3000, loss train:2.3996, loss val:2.4428\n",
      "step:3250, loss train:2.4203, loss val:2.4443\n",
      "step:3500, loss train:2.3963, loss val:2.4520\n",
      "step:3750, loss train:2.4349, loss val:2.4488\n",
      "step:4000, loss train:2.4024, loss val:2.4398\n",
      "step:4250, loss train:2.4030, loss val:2.4468\n",
      "step:4500, loss train:2.4077, loss val:2.4577\n",
      "step:4750, loss train:2.4197, loss val:2.4286\n",
      "step:5000, loss train:2.4264, loss val:2.4608\n",
      "step:5250, loss train:2.4019, loss val:2.4407\n",
      "step:5500, loss train:2.3994, loss val:2.4572\n",
      "step:5750, loss train:2.4173, loss val:2.4476\n",
      "step:6000, loss train:2.4035, loss val:2.4309\n",
      "step:6250, loss train:2.4206, loss val:2.4272\n",
      "step:6500, loss train:2.4323, loss val:2.4534\n",
      "step:6750, loss train:2.4060, loss val:2.4341\n",
      "step:7000, loss train:2.4097, loss val:2.4519\n",
      "step:7250, loss train:2.3879, loss val:2.4481\n",
      "step:7500, loss train:2.3961, loss val:2.4583\n",
      "step:7750, loss train:2.4092, loss val:2.4457\n",
      "step:8000, loss train:2.4326, loss val:2.4419\n",
      "step:8250, loss train:2.3960, loss val:2.4848\n",
      "step:8500, loss train:2.4127, loss val:2.4389\n",
      "step:8750, loss train:2.4143, loss val:2.4280\n",
      "step:9000, loss train:2.4105, loss val:2.4512\n",
      "step:9250, loss train:2.4004, loss val:2.4600\n",
      "step:9500, loss train:2.4220, loss val:2.4439\n",
      "step:9750, loss train:2.4127, loss val:2.4309\n",
      "batch:144\n",
      "step:0, loss train:2.4231, loss val:2.4639\n",
      "step:250, loss train:2.4105, loss val:2.4576\n",
      "step:500, loss train:2.4273, loss val:2.4461\n",
      "step:750, loss train:2.3985, loss val:2.4399\n",
      "step:1000, loss train:2.4152, loss val:2.4480\n",
      "step:1250, loss train:2.4176, loss val:2.4394\n",
      "step:1500, loss train:2.4037, loss val:2.4368\n",
      "step:1750, loss train:2.4185, loss val:2.4351\n",
      "step:2000, loss train:2.3901, loss val:2.4338\n",
      "step:2250, loss train:2.4098, loss val:2.4351\n",
      "step:2500, loss train:2.4157, loss val:2.4282\n",
      "step:2750, loss train:2.4021, loss val:2.4474\n",
      "step:3000, loss train:2.4305, loss val:2.4418\n",
      "step:3250, loss train:2.4057, loss val:2.4564\n",
      "step:3500, loss train:2.4281, loss val:2.4451\n",
      "step:3750, loss train:2.3897, loss val:2.4264\n",
      "step:4000, loss train:2.4099, loss val:2.4482\n",
      "step:4250, loss train:2.4125, loss val:2.4467\n",
      "step:4500, loss train:2.4013, loss val:2.4507\n",
      "step:4750, loss train:2.4136, loss val:2.4489\n",
      "step:5000, loss train:2.4104, loss val:2.4473\n",
      "step:5250, loss train:2.4032, loss val:2.4486\n",
      "step:5500, loss train:2.4162, loss val:2.4443\n",
      "step:5750, loss train:2.4191, loss val:2.4499\n",
      "step:6000, loss train:2.4148, loss val:2.4534\n",
      "step:6250, loss train:2.3998, loss val:2.4472\n",
      "step:6500, loss train:2.4255, loss val:2.4571\n",
      "step:6750, loss train:2.4209, loss val:2.4412\n",
      "step:7000, loss train:2.4209, loss val:2.4311\n",
      "step:7250, loss train:2.4250, loss val:2.4499\n",
      "step:7500, loss train:2.4117, loss val:2.4821\n",
      "step:7750, loss train:2.4112, loss val:2.4263\n",
      "step:8000, loss train:2.4167, loss val:2.4557\n",
      "step:8250, loss train:2.4132, loss val:2.4639\n",
      "step:8500, loss train:2.4113, loss val:2.4575\n",
      "step:8750, loss train:2.4290, loss val:2.4387\n",
      "step:9000, loss train:2.3962, loss val:2.4360\n",
      "step:9250, loss train:2.4106, loss val:2.4370\n",
      "step:9500, loss train:2.4129, loss val:2.4425\n",
      "step:9750, loss train:2.4043, loss val:2.4329\n",
      "batch:145\n",
      "step:0, loss train:2.4043, loss val:2.4319\n",
      "step:250, loss train:2.4121, loss val:2.4570\n",
      "step:500, loss train:2.4038, loss val:2.4659\n",
      "step:750, loss train:2.4141, loss val:2.4370\n",
      "step:1000, loss train:2.4017, loss val:2.4475\n",
      "step:1250, loss train:2.4080, loss val:2.4586\n",
      "step:1500, loss train:2.3987, loss val:2.4562\n",
      "step:1750, loss train:2.4024, loss val:2.4450\n",
      "step:2000, loss train:2.4146, loss val:2.4408\n",
      "step:2250, loss train:2.4227, loss val:2.4589\n",
      "step:2500, loss train:2.4141, loss val:2.4353\n",
      "step:2750, loss train:2.4213, loss val:2.4227\n",
      "step:3000, loss train:2.4211, loss val:2.4181\n",
      "step:3250, loss train:2.4168, loss val:2.4649\n",
      "step:3500, loss train:2.4085, loss val:2.4527\n",
      "step:3750, loss train:2.4171, loss val:2.4508\n",
      "step:4000, loss train:2.4309, loss val:2.4522\n",
      "step:4250, loss train:2.3988, loss val:2.4499\n",
      "step:4500, loss train:2.4006, loss val:2.4235\n",
      "step:4750, loss train:2.4068, loss val:2.4563\n",
      "step:5000, loss train:2.4168, loss val:2.4531\n",
      "step:5250, loss train:2.3941, loss val:2.4426\n",
      "step:5500, loss train:2.4064, loss val:2.4538\n",
      "step:5750, loss train:2.4175, loss val:2.4507\n",
      "step:6000, loss train:2.4287, loss val:2.4573\n",
      "step:6250, loss train:2.4148, loss val:2.4695\n",
      "step:6500, loss train:2.3995, loss val:2.4668\n",
      "step:6750, loss train:2.4080, loss val:2.4451\n",
      "step:7000, loss train:2.3904, loss val:2.4336\n",
      "step:7250, loss train:2.4060, loss val:2.4357\n",
      "step:7500, loss train:2.4200, loss val:2.4508\n",
      "step:7750, loss train:2.4101, loss val:2.4304\n",
      "step:8000, loss train:2.4255, loss val:2.4475\n",
      "step:8250, loss train:2.4153, loss val:2.4514\n",
      "step:8500, loss train:2.4040, loss val:2.4292\n",
      "step:8750, loss train:2.4294, loss val:2.4453\n",
      "step:9000, loss train:2.4132, loss val:2.4404\n",
      "step:9250, loss train:2.4062, loss val:2.4497\n",
      "step:9500, loss train:2.4159, loss val:2.4359\n",
      "step:9750, loss train:2.4080, loss val:2.4620\n",
      "batch:146\n",
      "step:0, loss train:2.4040, loss val:2.4616\n",
      "step:250, loss train:2.4137, loss val:2.4388\n",
      "step:500, loss train:2.4147, loss val:2.4449\n",
      "step:750, loss train:2.4059, loss val:2.4623\n",
      "step:1000, loss train:2.4186, loss val:2.4444\n",
      "step:1250, loss train:2.4203, loss val:2.4405\n",
      "step:1500, loss train:2.3984, loss val:2.4348\n",
      "step:1750, loss train:2.4057, loss val:2.4626\n",
      "step:2000, loss train:2.3992, loss val:2.4256\n",
      "step:2250, loss train:2.4199, loss val:2.4629\n",
      "step:2500, loss train:2.4013, loss val:2.4361\n",
      "step:2750, loss train:2.4289, loss val:2.4441\n",
      "step:3000, loss train:2.3998, loss val:2.4562\n",
      "step:3250, loss train:2.4108, loss val:2.4584\n",
      "step:3500, loss train:2.4101, loss val:2.4374\n",
      "step:3750, loss train:2.4121, loss val:2.4277\n",
      "step:4000, loss train:2.4063, loss val:2.4497\n",
      "step:4250, loss train:2.4037, loss val:2.4437\n",
      "step:4500, loss train:2.4137, loss val:2.4626\n",
      "step:4750, loss train:2.4148, loss val:2.4415\n",
      "step:5000, loss train:2.4048, loss val:2.4648\n",
      "step:5250, loss train:2.4175, loss val:2.4560\n",
      "step:5500, loss train:2.4239, loss val:2.4620\n",
      "step:5750, loss train:2.4301, loss val:2.4525\n",
      "step:6000, loss train:2.3990, loss val:2.4547\n",
      "step:6250, loss train:2.4096, loss val:2.4453\n",
      "step:6500, loss train:2.4191, loss val:2.4381\n",
      "step:6750, loss train:2.4157, loss val:2.4347\n",
      "step:7000, loss train:2.4174, loss val:2.4269\n",
      "step:7250, loss train:2.4176, loss val:2.4459\n",
      "step:7500, loss train:2.4056, loss val:2.4403\n",
      "step:7750, loss train:2.4174, loss val:2.4468\n",
      "step:8000, loss train:2.4165, loss val:2.4573\n",
      "step:8250, loss train:2.4266, loss val:2.4419\n",
      "step:8500, loss train:2.4074, loss val:2.4531\n",
      "step:8750, loss train:2.4169, loss val:2.4269\n",
      "step:9000, loss train:2.3922, loss val:2.4573\n",
      "step:9250, loss train:2.4036, loss val:2.4605\n",
      "step:9500, loss train:2.4183, loss val:2.4308\n",
      "step:9750, loss train:2.4155, loss val:2.4537\n",
      "batch:147\n",
      "step:0, loss train:2.4203, loss val:2.4659\n",
      "step:250, loss train:2.4212, loss val:2.4478\n",
      "step:500, loss train:2.4034, loss val:2.4343\n",
      "step:750, loss train:2.4219, loss val:2.4436\n",
      "step:1000, loss train:2.4076, loss val:2.4426\n",
      "step:1250, loss train:2.4053, loss val:2.4487\n",
      "step:1500, loss train:2.4196, loss val:2.4439\n",
      "step:1750, loss train:2.4141, loss val:2.4602\n",
      "step:2000, loss train:2.4266, loss val:2.4429\n",
      "step:2250, loss train:2.4113, loss val:2.4715\n",
      "step:2500, loss train:2.4109, loss val:2.4456\n",
      "step:2750, loss train:2.4204, loss val:2.4371\n",
      "step:3000, loss train:2.4047, loss val:2.4549\n",
      "step:3250, loss train:2.4309, loss val:2.4427\n",
      "step:3500, loss train:2.4044, loss val:2.4498\n",
      "step:3750, loss train:2.4073, loss val:2.4507\n",
      "step:4000, loss train:2.4206, loss val:2.4521\n",
      "step:4250, loss train:2.3993, loss val:2.4500\n",
      "step:4500, loss train:2.4002, loss val:2.4263\n",
      "step:4750, loss train:2.4074, loss val:2.4349\n",
      "step:5000, loss train:2.4044, loss val:2.4596\n",
      "step:5250, loss train:2.4227, loss val:2.4302\n",
      "step:5500, loss train:2.4188, loss val:2.4470\n",
      "step:5750, loss train:2.4198, loss val:2.4345\n",
      "step:6000, loss train:2.4027, loss val:2.4578\n",
      "step:6250, loss train:2.4072, loss val:2.4312\n",
      "step:6500, loss train:2.4052, loss val:2.4296\n",
      "step:6750, loss train:2.4056, loss val:2.4556\n",
      "step:7000, loss train:2.4168, loss val:2.4453\n",
      "step:7250, loss train:2.4091, loss val:2.4491\n",
      "step:7500, loss train:2.4297, loss val:2.4458\n",
      "step:7750, loss train:2.4148, loss val:2.4366\n",
      "step:8000, loss train:2.3938, loss val:2.4670\n",
      "step:8250, loss train:2.4186, loss val:2.4681\n",
      "step:8500, loss train:2.4029, loss val:2.4452\n",
      "step:8750, loss train:2.4166, loss val:2.4471\n",
      "step:9000, loss train:2.4337, loss val:2.4563\n",
      "step:9250, loss train:2.4046, loss val:2.4570\n",
      "step:9500, loss train:2.4079, loss val:2.4465\n",
      "step:9750, loss train:2.4322, loss val:2.4428\n",
      "batch:148\n",
      "step:0, loss train:2.4264, loss val:2.4655\n",
      "step:250, loss train:2.4140, loss val:2.4585\n",
      "step:500, loss train:2.4281, loss val:2.4521\n",
      "step:750, loss train:2.4039, loss val:2.4408\n",
      "step:1000, loss train:2.4022, loss val:2.4390\n",
      "step:1250, loss train:2.4132, loss val:2.4660\n",
      "step:1500, loss train:2.4151, loss val:2.4550\n",
      "step:1750, loss train:2.3969, loss val:2.4515\n",
      "step:2000, loss train:2.4348, loss val:2.4280\n",
      "step:2250, loss train:2.4070, loss val:2.4458\n",
      "step:2500, loss train:2.4088, loss val:2.4520\n",
      "step:2750, loss train:2.4084, loss val:2.4508\n",
      "step:3000, loss train:2.3913, loss val:2.4345\n",
      "step:3250, loss train:2.4307, loss val:2.4366\n",
      "step:3500, loss train:2.4230, loss val:2.4544\n",
      "step:3750, loss train:2.4062, loss val:2.4497\n",
      "step:4000, loss train:2.3919, loss val:2.4760\n",
      "step:4250, loss train:2.4310, loss val:2.4561\n",
      "step:4500, loss train:2.4109, loss val:2.4552\n",
      "step:4750, loss train:2.4131, loss val:2.4247\n",
      "step:5000, loss train:2.4108, loss val:2.4446\n",
      "step:5250, loss train:2.4197, loss val:2.4401\n",
      "step:5500, loss train:2.4162, loss val:2.4405\n",
      "step:5750, loss train:2.4177, loss val:2.4413\n",
      "step:6000, loss train:2.4223, loss val:2.4626\n",
      "step:6250, loss train:2.4091, loss val:2.4514\n",
      "step:6500, loss train:2.3912, loss val:2.4394\n",
      "step:6750, loss train:2.4108, loss val:2.4473\n",
      "step:7000, loss train:2.4058, loss val:2.4265\n",
      "step:7250, loss train:2.4077, loss val:2.4281\n",
      "step:7500, loss train:2.4179, loss val:2.4510\n",
      "step:7750, loss train:2.4090, loss val:2.4467\n",
      "step:8000, loss train:2.3987, loss val:2.4396\n",
      "step:8250, loss train:2.4064, loss val:2.4736\n",
      "step:8500, loss train:2.4118, loss val:2.4528\n",
      "step:8750, loss train:2.4265, loss val:2.4288\n",
      "step:9000, loss train:2.4053, loss val:2.4425\n",
      "step:9250, loss train:2.4059, loss val:2.4568\n",
      "step:9500, loss train:2.4132, loss val:2.4583\n",
      "step:9750, loss train:2.4255, loss val:2.4350\n",
      "batch:149\n",
      "step:0, loss train:2.4186, loss val:2.4647\n",
      "step:250, loss train:2.4118, loss val:2.4367\n",
      "step:500, loss train:2.4070, loss val:2.4406\n",
      "step:750, loss train:2.4248, loss val:2.4584\n",
      "step:1000, loss train:2.4130, loss val:2.4407\n",
      "step:1250, loss train:2.4193, loss val:2.4448\n",
      "step:1500, loss train:2.4289, loss val:2.4431\n",
      "step:1750, loss train:2.4135, loss val:2.4460\n",
      "step:2000, loss train:2.4051, loss val:2.4427\n",
      "step:2250, loss train:2.4104, loss val:2.4499\n",
      "step:2500, loss train:2.3989, loss val:2.4394\n",
      "step:2750, loss train:2.3981, loss val:2.4439\n",
      "step:3000, loss train:2.4099, loss val:2.4476\n",
      "step:3250, loss train:2.4142, loss val:2.4289\n",
      "step:3500, loss train:2.4154, loss val:2.4529\n",
      "step:3750, loss train:2.4101, loss val:2.4589\n",
      "step:4000, loss train:2.4113, loss val:2.4608\n",
      "step:4250, loss train:2.4188, loss val:2.4450\n",
      "step:4500, loss train:2.4198, loss val:2.4447\n",
      "step:4750, loss train:2.4061, loss val:2.4531\n",
      "step:5000, loss train:2.4134, loss val:2.4582\n",
      "step:5250, loss train:2.4280, loss val:2.4385\n",
      "step:5500, loss train:2.4063, loss val:2.4435\n",
      "step:5750, loss train:2.4394, loss val:2.4429\n",
      "step:6000, loss train:2.4128, loss val:2.4487\n",
      "step:6250, loss train:2.4196, loss val:2.4391\n",
      "step:6500, loss train:2.4133, loss val:2.4322\n",
      "step:6750, loss train:2.3926, loss val:2.4409\n",
      "step:7000, loss train:2.4113, loss val:2.4564\n",
      "step:7250, loss train:2.4115, loss val:2.4621\n",
      "step:7500, loss train:2.4161, loss val:2.4500\n",
      "step:7750, loss train:2.4017, loss val:2.4498\n",
      "step:8000, loss train:2.4129, loss val:2.4251\n",
      "step:8250, loss train:2.4266, loss val:2.4545\n",
      "step:8500, loss train:2.4141, loss val:2.4345\n",
      "step:8750, loss train:2.4182, loss val:2.4484\n",
      "step:9000, loss train:2.4003, loss val:2.4569\n",
      "step:9250, loss train:2.4276, loss val:2.4474\n",
      "step:9500, loss train:2.4205, loss val:2.4336\n",
      "step:9750, loss train:2.3950, loss val:2.4450\n",
      "batch:150\n",
      "step:0, loss train:2.4132, loss val:2.4523\n",
      "step:250, loss train:2.4312, loss val:2.4552\n",
      "step:500, loss train:2.4025, loss val:2.4530\n",
      "step:750, loss train:2.4002, loss val:2.4518\n",
      "step:1000, loss train:2.4194, loss val:2.4361\n",
      "step:1250, loss train:2.4058, loss val:2.4789\n",
      "step:1500, loss train:2.4098, loss val:2.4394\n",
      "step:1750, loss train:2.4280, loss val:2.4288\n",
      "step:2000, loss train:2.3998, loss val:2.4399\n",
      "step:2250, loss train:2.4195, loss val:2.4649\n",
      "step:2500, loss train:2.4187, loss val:2.4787\n",
      "step:2750, loss train:2.4251, loss val:2.4420\n",
      "step:3000, loss train:2.4120, loss val:2.4466\n",
      "step:3250, loss train:2.4187, loss val:2.4622\n",
      "step:3500, loss train:2.4079, loss val:2.4457\n",
      "step:3750, loss train:2.4172, loss val:2.4119\n",
      "step:4000, loss train:2.4293, loss val:2.4360\n",
      "step:4250, loss train:2.4135, loss val:2.4363\n",
      "step:4500, loss train:2.4179, loss val:2.4490\n",
      "step:4750, loss train:2.4049, loss val:2.4528\n",
      "step:5000, loss train:2.4247, loss val:2.4388\n",
      "step:5250, loss train:2.4168, loss val:2.4617\n",
      "step:5500, loss train:2.4075, loss val:2.4537\n",
      "step:5750, loss train:2.4009, loss val:2.4283\n",
      "step:6000, loss train:2.4021, loss val:2.4331\n",
      "step:6250, loss train:2.4119, loss val:2.4452\n",
      "step:6500, loss train:2.4229, loss val:2.4549\n",
      "step:6750, loss train:2.4012, loss val:2.4582\n",
      "step:7000, loss train:2.4081, loss val:2.4610\n",
      "step:7250, loss train:2.4142, loss val:2.4359\n",
      "step:7500, loss train:2.4077, loss val:2.4542\n",
      "step:7750, loss train:2.4090, loss val:2.4543\n",
      "step:8000, loss train:2.4031, loss val:2.4493\n",
      "step:8250, loss train:2.4112, loss val:2.4576\n",
      "step:8500, loss train:2.4184, loss val:2.4571\n",
      "step:8750, loss train:2.4007, loss val:2.4438\n",
      "step:9000, loss train:2.4012, loss val:2.4510\n",
      "step:9250, loss train:2.4112, loss val:2.4415\n",
      "step:9500, loss train:2.4091, loss val:2.4351\n",
      "step:9750, loss train:2.4052, loss val:2.4355\n",
      "batch:151\n",
      "step:0, loss train:2.4123, loss val:2.4552\n",
      "step:250, loss train:2.4174, loss val:2.4562\n",
      "step:500, loss train:2.4193, loss val:2.4466\n",
      "step:750, loss train:2.4175, loss val:2.4568\n",
      "step:1000, loss train:2.4370, loss val:2.4248\n",
      "step:1250, loss train:2.4062, loss val:2.4379\n",
      "step:1500, loss train:2.4118, loss val:2.4593\n",
      "step:1750, loss train:2.4334, loss val:2.4577\n",
      "step:2000, loss train:2.4289, loss val:2.4367\n",
      "step:2250, loss train:2.4010, loss val:2.4548\n",
      "step:2500, loss train:2.4291, loss val:2.4399\n",
      "step:2750, loss train:2.4082, loss val:2.4545\n",
      "step:3000, loss train:2.4180, loss val:2.4653\n",
      "step:3250, loss train:2.4221, loss val:2.4491\n",
      "step:3500, loss train:2.3929, loss val:2.4482\n",
      "step:3750, loss train:2.4468, loss val:2.4354\n",
      "step:4000, loss train:2.4152, loss val:2.4252\n",
      "step:4250, loss train:2.4021, loss val:2.4306\n",
      "step:4500, loss train:2.4070, loss val:2.4336\n",
      "step:4750, loss train:2.4271, loss val:2.4482\n",
      "step:5000, loss train:2.4261, loss val:2.4535\n",
      "step:5250, loss train:2.4195, loss val:2.4286\n",
      "step:5500, loss train:2.4287, loss val:2.4597\n",
      "step:5750, loss train:2.4122, loss val:2.4324\n",
      "step:6000, loss train:2.4177, loss val:2.4376\n",
      "step:6250, loss train:2.4166, loss val:2.4275\n",
      "step:6500, loss train:2.4202, loss val:2.4558\n",
      "step:6750, loss train:2.4068, loss val:2.4384\n",
      "step:7000, loss train:2.4231, loss val:2.4495\n",
      "step:7250, loss train:2.4191, loss val:2.4347\n",
      "step:7500, loss train:2.4149, loss val:2.4351\n",
      "step:7750, loss train:2.4166, loss val:2.4626\n",
      "step:8000, loss train:2.4005, loss val:2.4484\n",
      "step:8250, loss train:2.4316, loss val:2.4411\n",
      "step:8500, loss train:2.4179, loss val:2.4427\n",
      "step:8750, loss train:2.4161, loss val:2.4261\n",
      "step:9000, loss train:2.4043, loss val:2.4404\n",
      "step:9250, loss train:2.4049, loss val:2.4450\n",
      "step:9500, loss train:2.4172, loss val:2.4488\n",
      "step:9750, loss train:2.4064, loss val:2.4434\n",
      "batch:152\n",
      "step:0, loss train:2.4140, loss val:2.4379\n",
      "step:250, loss train:2.3972, loss val:2.4587\n",
      "step:500, loss train:2.4227, loss val:2.4666\n",
      "step:750, loss train:2.4144, loss val:2.4786\n",
      "step:1000, loss train:2.4068, loss val:2.4388\n",
      "step:1250, loss train:2.4049, loss val:2.4520\n",
      "step:1500, loss train:2.4171, loss val:2.4492\n",
      "step:1750, loss train:2.4078, loss val:2.4441\n",
      "step:2000, loss train:2.4104, loss val:2.4400\n",
      "step:2250, loss train:2.4093, loss val:2.4579\n",
      "step:2500, loss train:2.3977, loss val:2.4504\n",
      "step:2750, loss train:2.4052, loss val:2.4393\n",
      "step:3000, loss train:2.4096, loss val:2.4666\n",
      "step:3250, loss train:2.4162, loss val:2.4332\n",
      "step:3500, loss train:2.4158, loss val:2.4352\n",
      "step:3750, loss train:2.4117, loss val:2.4506\n",
      "step:4000, loss train:2.3984, loss val:2.4486\n",
      "step:4250, loss train:2.3994, loss val:2.4410\n",
      "step:4500, loss train:2.4111, loss val:2.4446\n",
      "step:4750, loss train:2.4264, loss val:2.4498\n",
      "step:5000, loss train:2.4006, loss val:2.4389\n",
      "step:5250, loss train:2.4209, loss val:2.4407\n",
      "step:5500, loss train:2.3995, loss val:2.4406\n",
      "step:5750, loss train:2.4002, loss val:2.4889\n",
      "step:6000, loss train:2.4216, loss val:2.4556\n",
      "step:6250, loss train:2.3998, loss val:2.4368\n",
      "step:6500, loss train:2.3949, loss val:2.4471\n",
      "step:6750, loss train:2.4132, loss val:2.4578\n",
      "step:7000, loss train:2.4117, loss val:2.4468\n",
      "step:7250, loss train:2.3939, loss val:2.4500\n",
      "step:7500, loss train:2.4037, loss val:2.4363\n",
      "step:7750, loss train:2.4181, loss val:2.4424\n",
      "step:8000, loss train:2.4337, loss val:2.4257\n",
      "step:8250, loss train:2.4265, loss val:2.4288\n",
      "step:8500, loss train:2.4107, loss val:2.4415\n",
      "step:8750, loss train:2.4008, loss val:2.4541\n",
      "step:9000, loss train:2.4043, loss val:2.4729\n",
      "step:9250, loss train:2.4165, loss val:2.4441\n",
      "step:9500, loss train:2.4117, loss val:2.4513\n",
      "step:9750, loss train:2.4145, loss val:2.4389\n",
      "batch:153\n",
      "step:0, loss train:2.4152, loss val:2.4525\n",
      "step:250, loss train:2.4145, loss val:2.4299\n",
      "step:500, loss train:2.4280, loss val:2.4560\n",
      "step:750, loss train:2.4310, loss val:2.4378\n",
      "step:1000, loss train:2.4128, loss val:2.4505\n",
      "step:1250, loss train:2.4316, loss val:2.4410\n",
      "step:1500, loss train:2.4116, loss val:2.4720\n",
      "step:1750, loss train:2.4149, loss val:2.4303\n",
      "step:2000, loss train:2.4310, loss val:2.4500\n",
      "step:2250, loss train:2.4160, loss val:2.4419\n",
      "step:2500, loss train:2.3978, loss val:2.4547\n",
      "step:2750, loss train:2.4038, loss val:2.4451\n",
      "step:3000, loss train:2.3983, loss val:2.4346\n",
      "step:3250, loss train:2.4116, loss val:2.4526\n",
      "step:3500, loss train:2.4274, loss val:2.4353\n",
      "step:3750, loss train:2.4244, loss val:2.4419\n",
      "step:4000, loss train:2.4058, loss val:2.4617\n",
      "step:4250, loss train:2.4119, loss val:2.4656\n",
      "step:4500, loss train:2.4102, loss val:2.4387\n",
      "step:4750, loss train:2.4090, loss val:2.4355\n",
      "step:5000, loss train:2.4066, loss val:2.4445\n",
      "step:5250, loss train:2.4155, loss val:2.4561\n",
      "step:5500, loss train:2.4031, loss val:2.4485\n",
      "step:5750, loss train:2.4255, loss val:2.4146\n",
      "step:6000, loss train:2.4314, loss val:2.4402\n",
      "step:6250, loss train:2.4028, loss val:2.4417\n",
      "step:6500, loss train:2.4249, loss val:2.4488\n",
      "step:6750, loss train:2.4163, loss val:2.4194\n",
      "step:7000, loss train:2.4077, loss val:2.4499\n",
      "step:7250, loss train:2.4160, loss val:2.4363\n",
      "step:7500, loss train:2.4137, loss val:2.4598\n",
      "step:7750, loss train:2.4250, loss val:2.4428\n",
      "step:8000, loss train:2.4173, loss val:2.4374\n",
      "step:8250, loss train:2.3794, loss val:2.4477\n",
      "step:8500, loss train:2.4278, loss val:2.4415\n",
      "step:8750, loss train:2.3995, loss val:2.4428\n",
      "step:9000, loss train:2.4086, loss val:2.4388\n",
      "step:9250, loss train:2.4100, loss val:2.4510\n",
      "step:9500, loss train:2.4113, loss val:2.4543\n",
      "step:9750, loss train:2.4286, loss val:2.4414\n",
      "batch:154\n",
      "step:0, loss train:2.4039, loss val:2.4287\n",
      "step:250, loss train:2.3926, loss val:2.4676\n",
      "step:500, loss train:2.4185, loss val:2.4466\n",
      "step:750, loss train:2.4010, loss val:2.4572\n",
      "step:1000, loss train:2.4110, loss val:2.4588\n",
      "step:1250, loss train:2.4055, loss val:2.4426\n",
      "step:1500, loss train:2.4237, loss val:2.4247\n",
      "step:1750, loss train:2.4175, loss val:2.4644\n",
      "step:2000, loss train:2.4121, loss val:2.4496\n",
      "step:2250, loss train:2.4050, loss val:2.4500\n",
      "step:2500, loss train:2.4185, loss val:2.4736\n",
      "step:2750, loss train:2.4021, loss val:2.4395\n",
      "step:3000, loss train:2.4142, loss val:2.4547\n",
      "step:3250, loss train:2.4234, loss val:2.4206\n",
      "step:3500, loss train:2.4056, loss val:2.4491\n",
      "step:3750, loss train:2.3950, loss val:2.4819\n",
      "step:4000, loss train:2.4032, loss val:2.4500\n",
      "step:4250, loss train:2.4243, loss val:2.4346\n",
      "step:4500, loss train:2.4273, loss val:2.4582\n",
      "step:4750, loss train:2.4327, loss val:2.4348\n",
      "step:5000, loss train:2.4115, loss val:2.4481\n",
      "step:5250, loss train:2.4079, loss val:2.4384\n",
      "step:5500, loss train:2.4035, loss val:2.4411\n",
      "step:5750, loss train:2.4050, loss val:2.4368\n",
      "step:6000, loss train:2.4052, loss val:2.4574\n",
      "step:6250, loss train:2.4124, loss val:2.4671\n",
      "step:6500, loss train:2.4184, loss val:2.4454\n",
      "step:6750, loss train:2.4305, loss val:2.4405\n",
      "step:7000, loss train:2.4274, loss val:2.4535\n",
      "step:7250, loss train:2.3949, loss val:2.4476\n",
      "step:7500, loss train:2.4057, loss val:2.4547\n",
      "step:7750, loss train:2.4059, loss val:2.4428\n",
      "step:8000, loss train:2.4222, loss val:2.4430\n",
      "step:8250, loss train:2.4331, loss val:2.4394\n",
      "step:8500, loss train:2.3878, loss val:2.4452\n",
      "step:8750, loss train:2.4092, loss val:2.4446\n",
      "step:9000, loss train:2.3971, loss val:2.4317\n",
      "step:9250, loss train:2.4219, loss val:2.4467\n",
      "step:9500, loss train:2.4210, loss val:2.4545\n",
      "step:9750, loss train:2.4107, loss val:2.4487\n",
      "batch:155\n",
      "step:0, loss train:2.4106, loss val:2.4488\n",
      "step:250, loss train:2.4209, loss val:2.4705\n",
      "step:500, loss train:2.4018, loss val:2.4431\n",
      "step:750, loss train:2.3983, loss val:2.4565\n",
      "step:1000, loss train:2.4165, loss val:2.4582\n",
      "step:1250, loss train:2.4059, loss val:2.4494\n",
      "step:1500, loss train:2.4300, loss val:2.4460\n",
      "step:1750, loss train:2.4115, loss val:2.4649\n",
      "step:2000, loss train:2.4002, loss val:2.4418\n",
      "step:2250, loss train:2.4044, loss val:2.4533\n",
      "step:2500, loss train:2.4201, loss val:2.4430\n",
      "step:2750, loss train:2.4095, loss val:2.4642\n",
      "step:3000, loss train:2.4237, loss val:2.4560\n",
      "step:3250, loss train:2.3950, loss val:2.4114\n",
      "step:3500, loss train:2.3990, loss val:2.4422\n",
      "step:3750, loss train:2.3958, loss val:2.4460\n",
      "step:4000, loss train:2.4160, loss val:2.4510\n",
      "step:4250, loss train:2.4218, loss val:2.4356\n",
      "step:4500, loss train:2.4055, loss val:2.4292\n",
      "step:4750, loss train:2.4221, loss val:2.4618\n",
      "step:5000, loss train:2.4129, loss val:2.4407\n",
      "step:5250, loss train:2.4068, loss val:2.4403\n",
      "step:5500, loss train:2.4109, loss val:2.4651\n",
      "step:5750, loss train:2.4111, loss val:2.4515\n",
      "step:6000, loss train:2.4203, loss val:2.4647\n",
      "step:6250, loss train:2.4208, loss val:2.4632\n",
      "step:6500, loss train:2.4227, loss val:2.4680\n",
      "step:6750, loss train:2.4115, loss val:2.4525\n",
      "step:7000, loss train:2.4228, loss val:2.4242\n",
      "step:7250, loss train:2.4257, loss val:2.4463\n",
      "step:7500, loss train:2.4117, loss val:2.4277\n",
      "step:7750, loss train:2.3988, loss val:2.4530\n",
      "step:8000, loss train:2.3983, loss val:2.4445\n",
      "step:8250, loss train:2.3993, loss val:2.4390\n",
      "step:8500, loss train:2.4135, loss val:2.4546\n",
      "step:8750, loss train:2.4179, loss val:2.4567\n",
      "step:9000, loss train:2.4109, loss val:2.4506\n",
      "step:9250, loss train:2.4095, loss val:2.4333\n",
      "step:9500, loss train:2.3989, loss val:2.4629\n",
      "step:9750, loss train:2.4095, loss val:2.4553\n",
      "batch:156\n",
      "step:0, loss train:2.3957, loss val:2.4517\n",
      "step:250, loss train:2.4081, loss val:2.4540\n",
      "step:500, loss train:2.3890, loss val:2.4334\n",
      "step:750, loss train:2.4101, loss val:2.4602\n",
      "step:1000, loss train:2.4196, loss val:2.4498\n",
      "step:1250, loss train:2.4072, loss val:2.4230\n",
      "step:1500, loss train:2.4307, loss val:2.4572\n",
      "step:1750, loss train:2.3924, loss val:2.4477\n",
      "step:2000, loss train:2.4216, loss val:2.4478\n",
      "step:2250, loss train:2.4406, loss val:2.4321\n",
      "step:2500, loss train:2.4190, loss val:2.4331\n",
      "step:2750, loss train:2.4040, loss val:2.4420\n",
      "step:3000, loss train:2.4292, loss val:2.4532\n",
      "step:3250, loss train:2.4306, loss val:2.4420\n",
      "step:3500, loss train:2.4064, loss val:2.4440\n",
      "step:3750, loss train:2.3775, loss val:2.4403\n",
      "step:4000, loss train:2.4008, loss val:2.4612\n",
      "step:4250, loss train:2.4007, loss val:2.4338\n",
      "step:4500, loss train:2.4049, loss val:2.4278\n",
      "step:4750, loss train:2.4049, loss val:2.4411\n",
      "step:5000, loss train:2.4085, loss val:2.4520\n",
      "step:5250, loss train:2.4083, loss val:2.4422\n",
      "step:5500, loss train:2.4162, loss val:2.4344\n",
      "step:5750, loss train:2.4226, loss val:2.4611\n",
      "step:6000, loss train:2.3925, loss val:2.4796\n",
      "step:6250, loss train:2.3986, loss val:2.4535\n",
      "step:6500, loss train:2.4094, loss val:2.4403\n",
      "step:6750, loss train:2.4102, loss val:2.4329\n",
      "step:7000, loss train:2.4040, loss val:2.4404\n",
      "step:7250, loss train:2.4039, loss val:2.4376\n",
      "step:7500, loss train:2.3968, loss val:2.4650\n",
      "step:7750, loss train:2.4300, loss val:2.4514\n",
      "step:8000, loss train:2.3890, loss val:2.4667\n",
      "step:8250, loss train:2.3886, loss val:2.4388\n",
      "step:8500, loss train:2.4131, loss val:2.4473\n",
      "step:8750, loss train:2.4213, loss val:2.4428\n",
      "step:9000, loss train:2.4022, loss val:2.4582\n",
      "step:9250, loss train:2.4088, loss val:2.4509\n",
      "step:9500, loss train:2.4181, loss val:2.4304\n",
      "step:9750, loss train:2.3995, loss val:2.4502\n",
      "batch:157\n",
      "step:0, loss train:2.4045, loss val:2.4646\n",
      "step:250, loss train:2.4285, loss val:2.4360\n",
      "step:500, loss train:2.4086, loss val:2.4345\n",
      "step:750, loss train:2.4010, loss val:2.4176\n",
      "step:1000, loss train:2.4167, loss val:2.4345\n",
      "step:1250, loss train:2.4306, loss val:2.4478\n",
      "step:1500, loss train:2.4217, loss val:2.4541\n",
      "step:1750, loss train:2.4050, loss val:2.4489\n",
      "step:2000, loss train:2.3964, loss val:2.4411\n",
      "step:2250, loss train:2.4211, loss val:2.4608\n",
      "step:2500, loss train:2.3960, loss val:2.4545\n",
      "step:2750, loss train:2.4209, loss val:2.4445\n",
      "step:3000, loss train:2.4326, loss val:2.4401\n",
      "step:3250, loss train:2.4074, loss val:2.4198\n",
      "step:3500, loss train:2.3943, loss val:2.4438\n",
      "step:3750, loss train:2.4194, loss val:2.4571\n",
      "step:4000, loss train:2.4142, loss val:2.4233\n",
      "step:4250, loss train:2.3955, loss val:2.4425\n",
      "step:4500, loss train:2.4128, loss val:2.4385\n",
      "step:4750, loss train:2.4256, loss val:2.4451\n",
      "step:5000, loss train:2.3845, loss val:2.4292\n",
      "step:5250, loss train:2.4171, loss val:2.4507\n",
      "step:5500, loss train:2.4256, loss val:2.4729\n",
      "step:5750, loss train:2.3977, loss val:2.4463\n",
      "step:6000, loss train:2.3999, loss val:2.4389\n",
      "step:6250, loss train:2.4174, loss val:2.4427\n",
      "step:6500, loss train:2.4198, loss val:2.4374\n",
      "step:6750, loss train:2.4279, loss val:2.4385\n",
      "step:7000, loss train:2.3903, loss val:2.4388\n",
      "step:7250, loss train:2.4043, loss val:2.4615\n",
      "step:7500, loss train:2.4256, loss val:2.4650\n",
      "step:7750, loss train:2.4258, loss val:2.4594\n",
      "step:8000, loss train:2.4025, loss val:2.4412\n",
      "step:8250, loss train:2.4224, loss val:2.4585\n",
      "step:8500, loss train:2.4216, loss val:2.4350\n",
      "step:8750, loss train:2.4119, loss val:2.4444\n",
      "step:9000, loss train:2.3909, loss val:2.4935\n",
      "step:9250, loss train:2.4134, loss val:2.4558\n",
      "step:9500, loss train:2.4336, loss val:2.4536\n",
      "step:9750, loss train:2.4057, loss val:2.4376\n",
      "batch:158\n",
      "step:0, loss train:2.4213, loss val:2.4202\n",
      "step:250, loss train:2.4149, loss val:2.4271\n",
      "step:500, loss train:2.4049, loss val:2.4653\n",
      "step:750, loss train:2.4091, loss val:2.4338\n",
      "step:1000, loss train:2.4215, loss val:2.4380\n",
      "step:1250, loss train:2.4119, loss val:2.4343\n",
      "step:1500, loss train:2.4293, loss val:2.4565\n",
      "step:1750, loss train:2.3900, loss val:2.4293\n",
      "step:2000, loss train:2.4118, loss val:2.4468\n",
      "step:2250, loss train:2.4251, loss val:2.4338\n",
      "step:2500, loss train:2.4092, loss val:2.4529\n",
      "step:2750, loss train:2.4112, loss val:2.4189\n",
      "step:3000, loss train:2.4164, loss val:2.4481\n",
      "step:3250, loss train:2.4083, loss val:2.4216\n",
      "step:3500, loss train:2.4133, loss val:2.4352\n",
      "step:3750, loss train:2.4077, loss val:2.4497\n",
      "step:4000, loss train:2.3991, loss val:2.4558\n",
      "step:4250, loss train:2.4062, loss val:2.4337\n",
      "step:4500, loss train:2.4032, loss val:2.4532\n",
      "step:4750, loss train:2.3939, loss val:2.4200\n",
      "step:5000, loss train:2.4245, loss val:2.4629\n",
      "step:5250, loss train:2.3973, loss val:2.4436\n",
      "step:5500, loss train:2.4095, loss val:2.4414\n",
      "step:5750, loss train:2.4248, loss val:2.4525\n",
      "step:6000, loss train:2.4250, loss val:2.4450\n",
      "step:6250, loss train:2.4196, loss val:2.4688\n",
      "step:6500, loss train:2.4133, loss val:2.4553\n",
      "step:6750, loss train:2.4244, loss val:2.4385\n",
      "step:7000, loss train:2.4139, loss val:2.4684\n",
      "step:7250, loss train:2.4015, loss val:2.4556\n",
      "step:7500, loss train:2.4180, loss val:2.4265\n",
      "step:7750, loss train:2.4311, loss val:2.4437\n",
      "step:8000, loss train:2.4320, loss val:2.4397\n",
      "step:8250, loss train:2.4200, loss val:2.4472\n",
      "step:8500, loss train:2.4202, loss val:2.4492\n",
      "step:8750, loss train:2.4103, loss val:2.4427\n",
      "step:9000, loss train:2.4246, loss val:2.4582\n",
      "step:9250, loss train:2.4147, loss val:2.4567\n",
      "step:9500, loss train:2.4197, loss val:2.4415\n",
      "step:9750, loss train:2.3961, loss val:2.4463\n",
      "batch:159\n",
      "step:0, loss train:2.4188, loss val:2.4478\n",
      "step:250, loss train:2.4174, loss val:2.4593\n",
      "step:500, loss train:2.4040, loss val:2.4521\n",
      "step:750, loss train:2.4241, loss val:2.4476\n",
      "step:1000, loss train:2.4139, loss val:2.4220\n",
      "step:1250, loss train:2.4106, loss val:2.4586\n",
      "step:1500, loss train:2.4090, loss val:2.4412\n",
      "step:1750, loss train:2.4070, loss val:2.4702\n",
      "step:2000, loss train:2.4054, loss val:2.4511\n",
      "step:2250, loss train:2.4057, loss val:2.4194\n",
      "step:2500, loss train:2.4180, loss val:2.4661\n",
      "step:2750, loss train:2.4064, loss val:2.4328\n",
      "step:3000, loss train:2.3961, loss val:2.4750\n",
      "step:3250, loss train:2.4243, loss val:2.4402\n",
      "step:3500, loss train:2.4062, loss val:2.4446\n",
      "step:3750, loss train:2.4053, loss val:2.4365\n",
      "step:4000, loss train:2.4088, loss val:2.4659\n",
      "step:4250, loss train:2.4047, loss val:2.4473\n",
      "step:4500, loss train:2.4167, loss val:2.4512\n",
      "step:4750, loss train:2.3847, loss val:2.4675\n",
      "step:5000, loss train:2.4038, loss val:2.4630\n",
      "step:5250, loss train:2.4044, loss val:2.4599\n",
      "step:5500, loss train:2.4110, loss val:2.4571\n",
      "step:5750, loss train:2.4086, loss val:2.4401\n",
      "step:6000, loss train:2.4132, loss val:2.4434\n",
      "step:6250, loss train:2.4393, loss val:2.4517\n",
      "step:6500, loss train:2.4249, loss val:2.4283\n",
      "step:6750, loss train:2.3941, loss val:2.4495\n",
      "step:7000, loss train:2.4080, loss val:2.4513\n",
      "step:7250, loss train:2.4058, loss val:2.4297\n",
      "step:7500, loss train:2.3949, loss val:2.4450\n",
      "step:7750, loss train:2.4150, loss val:2.4439\n",
      "step:8000, loss train:2.4174, loss val:2.4503\n",
      "step:8250, loss train:2.4193, loss val:2.4682\n",
      "step:8500, loss train:2.3937, loss val:2.4595\n",
      "step:8750, loss train:2.4203, loss val:2.4331\n",
      "step:9000, loss train:2.4070, loss val:2.4738\n",
      "step:9250, loss train:2.4001, loss val:2.4480\n",
      "step:9500, loss train:2.4183, loss val:2.4291\n",
      "step:9750, loss train:2.4013, loss val:2.4543\n",
      "batch:160\n",
      "step:0, loss train:2.4126, loss val:2.4292\n",
      "step:250, loss train:2.4199, loss val:2.4581\n",
      "step:500, loss train:2.4163, loss val:2.4494\n",
      "step:750, loss train:2.4292, loss val:2.4526\n",
      "step:1000, loss train:2.4162, loss val:2.4546\n",
      "step:1250, loss train:2.4038, loss val:2.4583\n",
      "step:1500, loss train:2.4119, loss val:2.4434\n",
      "step:1750, loss train:2.4143, loss val:2.4438\n",
      "step:2000, loss train:2.4009, loss val:2.4423\n",
      "step:2250, loss train:2.4120, loss val:2.4487\n",
      "step:2500, loss train:2.4132, loss val:2.4631\n",
      "step:2750, loss train:2.4191, loss val:2.4391\n",
      "step:3000, loss train:2.4005, loss val:2.4412\n",
      "step:3250, loss train:2.4246, loss val:2.4555\n",
      "step:3500, loss train:2.4223, loss val:2.4482\n",
      "step:3750, loss train:2.4060, loss val:2.4470\n",
      "step:4000, loss train:2.4155, loss val:2.4325\n",
      "step:4250, loss train:2.4038, loss val:2.4353\n",
      "step:4500, loss train:2.4230, loss val:2.4477\n",
      "step:4750, loss train:2.4023, loss val:2.4498\n",
      "step:5000, loss train:2.4174, loss val:2.4243\n",
      "step:5250, loss train:2.4213, loss val:2.4674\n",
      "step:5500, loss train:2.4205, loss val:2.4755\n",
      "step:5750, loss train:2.4081, loss val:2.4542\n",
      "step:6000, loss train:2.4077, loss val:2.4612\n",
      "step:6250, loss train:2.3976, loss val:2.4422\n",
      "step:6500, loss train:2.4087, loss val:2.4608\n",
      "step:6750, loss train:2.4224, loss val:2.4493\n",
      "step:7000, loss train:2.3987, loss val:2.4474\n",
      "step:7250, loss train:2.4000, loss val:2.4368\n",
      "step:7500, loss train:2.4257, loss val:2.4382\n",
      "step:7750, loss train:2.3984, loss val:2.4412\n",
      "step:8000, loss train:2.3962, loss val:2.4532\n",
      "step:8250, loss train:2.4159, loss val:2.4694\n",
      "step:8500, loss train:2.3968, loss val:2.4461\n",
      "step:8750, loss train:2.4294, loss val:2.4575\n",
      "step:9000, loss train:2.3984, loss val:2.4539\n",
      "step:9250, loss train:2.4263, loss val:2.4183\n",
      "step:9500, loss train:2.4039, loss val:2.4497\n",
      "step:9750, loss train:2.4066, loss val:2.4171\n",
      "batch:161\n",
      "step:0, loss train:2.3942, loss val:2.4335\n",
      "step:250, loss train:2.4302, loss val:2.4418\n",
      "step:500, loss train:2.4072, loss val:2.4455\n",
      "step:750, loss train:2.4192, loss val:2.4467\n",
      "step:1000, loss train:2.4059, loss val:2.4379\n",
      "step:1250, loss train:2.4066, loss val:2.4490\n",
      "step:1500, loss train:2.4193, loss val:2.4676\n",
      "step:1750, loss train:2.4217, loss val:2.4236\n",
      "step:2000, loss train:2.4067, loss val:2.4652\n",
      "step:2250, loss train:2.4263, loss val:2.4354\n",
      "step:2500, loss train:2.4216, loss val:2.4542\n",
      "step:2750, loss train:2.4090, loss val:2.4287\n",
      "step:3000, loss train:2.4349, loss val:2.4683\n",
      "step:3250, loss train:2.3946, loss val:2.4336\n",
      "step:3500, loss train:2.4267, loss val:2.4599\n",
      "step:3750, loss train:2.4138, loss val:2.4497\n",
      "step:4000, loss train:2.4130, loss val:2.4534\n",
      "step:4250, loss train:2.4174, loss val:2.4441\n",
      "step:4500, loss train:2.4170, loss val:2.4397\n",
      "step:4750, loss train:2.3920, loss val:2.4537\n",
      "step:5000, loss train:2.4151, loss val:2.4590\n",
      "step:5250, loss train:2.4132, loss val:2.4507\n",
      "step:5500, loss train:2.4176, loss val:2.4328\n",
      "step:5750, loss train:2.4179, loss val:2.4526\n",
      "step:6000, loss train:2.4151, loss val:2.4487\n",
      "step:6250, loss train:2.4095, loss val:2.4460\n",
      "step:6500, loss train:2.4123, loss val:2.4407\n",
      "step:6750, loss train:2.4108, loss val:2.4263\n",
      "step:7000, loss train:2.4378, loss val:2.4511\n",
      "step:7250, loss train:2.4340, loss val:2.4561\n",
      "step:7500, loss train:2.4048, loss val:2.4599\n",
      "step:7750, loss train:2.4061, loss val:2.4377\n",
      "step:8000, loss train:2.4163, loss val:2.4501\n",
      "step:8250, loss train:2.4107, loss val:2.4479\n",
      "step:8500, loss train:2.4020, loss val:2.4297\n",
      "step:8750, loss train:2.4061, loss val:2.4256\n",
      "step:9000, loss train:2.4017, loss val:2.4452\n",
      "step:9250, loss train:2.4173, loss val:2.4505\n",
      "step:9500, loss train:2.4101, loss val:2.4535\n",
      "step:9750, loss train:2.3845, loss val:2.4500\n",
      "batch:162\n",
      "step:0, loss train:2.4189, loss val:2.4688\n",
      "step:250, loss train:2.4247, loss val:2.4687\n",
      "step:500, loss train:2.4180, loss val:2.4415\n",
      "step:750, loss train:2.4184, loss val:2.4679\n",
      "step:1000, loss train:2.3914, loss val:2.4469\n",
      "step:1250, loss train:2.3928, loss val:2.4700\n",
      "step:1500, loss train:2.4141, loss val:2.4496\n",
      "step:1750, loss train:2.4198, loss val:2.4502\n",
      "step:2000, loss train:2.4142, loss val:2.4717\n",
      "step:2250, loss train:2.4084, loss val:2.4552\n",
      "step:2500, loss train:2.4118, loss val:2.4449\n",
      "step:2750, loss train:2.4294, loss val:2.4300\n",
      "step:3000, loss train:2.3970, loss val:2.4555\n",
      "step:3250, loss train:2.4094, loss val:2.4534\n",
      "step:3500, loss train:2.3978, loss val:2.4374\n",
      "step:3750, loss train:2.3970, loss val:2.4456\n",
      "step:4000, loss train:2.4197, loss val:2.4364\n",
      "step:4250, loss train:2.4247, loss val:2.4689\n",
      "step:4500, loss train:2.4317, loss val:2.4195\n",
      "step:4750, loss train:2.4044, loss val:2.4473\n",
      "step:5000, loss train:2.4251, loss val:2.4444\n",
      "step:5250, loss train:2.4044, loss val:2.4740\n",
      "step:5500, loss train:2.4116, loss val:2.4438\n",
      "step:5750, loss train:2.4174, loss val:2.4797\n",
      "step:6000, loss train:2.4013, loss val:2.4533\n",
      "step:6250, loss train:2.4198, loss val:2.4695\n",
      "step:6500, loss train:2.4102, loss val:2.4283\n",
      "step:6750, loss train:2.4165, loss val:2.4320\n",
      "step:7000, loss train:2.4076, loss val:2.4443\n",
      "step:7250, loss train:2.4104, loss val:2.4433\n",
      "step:7500, loss train:2.4014, loss val:2.4482\n",
      "step:7750, loss train:2.4203, loss val:2.4329\n",
      "step:8000, loss train:2.4350, loss val:2.4340\n",
      "step:8250, loss train:2.4024, loss val:2.4537\n",
      "step:8500, loss train:2.3926, loss val:2.4408\n",
      "step:8750, loss train:2.4289, loss val:2.4495\n",
      "step:9000, loss train:2.4257, loss val:2.4365\n",
      "step:9250, loss train:2.4111, loss val:2.4346\n",
      "step:9500, loss train:2.4334, loss val:2.4416\n",
      "step:9750, loss train:2.4166, loss val:2.4500\n",
      "batch:163\n",
      "step:0, loss train:2.3803, loss val:2.4373\n",
      "step:250, loss train:2.4296, loss val:2.4456\n",
      "step:500, loss train:2.4117, loss val:2.4481\n",
      "step:750, loss train:2.4187, loss val:2.4445\n",
      "step:1000, loss train:2.4108, loss val:2.4370\n",
      "step:1250, loss train:2.3842, loss val:2.4349\n",
      "step:1500, loss train:2.4113, loss val:2.4487\n",
      "step:1750, loss train:2.4141, loss val:2.4552\n",
      "step:2000, loss train:2.4282, loss val:2.4504\n",
      "step:2250, loss train:2.3999, loss val:2.4653\n",
      "step:2500, loss train:2.4044, loss val:2.4179\n",
      "step:2750, loss train:2.4046, loss val:2.4335\n",
      "step:3000, loss train:2.4164, loss val:2.4526\n",
      "step:3250, loss train:2.4042, loss val:2.4663\n",
      "step:3500, loss train:2.4189, loss val:2.4222\n",
      "step:3750, loss train:2.4176, loss val:2.4481\n",
      "step:4000, loss train:2.4218, loss val:2.4619\n",
      "step:4250, loss train:2.3974, loss val:2.4586\n",
      "step:4500, loss train:2.4082, loss val:2.4611\n",
      "step:4750, loss train:2.4221, loss val:2.4619\n",
      "step:5000, loss train:2.4061, loss val:2.4316\n",
      "step:5250, loss train:2.4152, loss val:2.4477\n",
      "step:5500, loss train:2.4254, loss val:2.4532\n",
      "step:5750, loss train:2.4109, loss val:2.4685\n",
      "step:6000, loss train:2.4125, loss val:2.4573\n",
      "step:6250, loss train:2.4075, loss val:2.4408\n",
      "step:6500, loss train:2.4030, loss val:2.4491\n",
      "step:6750, loss train:2.4090, loss val:2.4502\n",
      "step:7000, loss train:2.3809, loss val:2.4449\n",
      "step:7250, loss train:2.4102, loss val:2.4321\n",
      "step:7500, loss train:2.4260, loss val:2.4393\n",
      "step:7750, loss train:2.3900, loss val:2.4503\n",
      "step:8000, loss train:2.4084, loss val:2.4470\n",
      "step:8250, loss train:2.3943, loss val:2.4690\n",
      "step:8500, loss train:2.4218, loss val:2.4295\n",
      "step:8750, loss train:2.4291, loss val:2.4584\n",
      "step:9000, loss train:2.4052, loss val:2.4613\n",
      "step:9250, loss train:2.3938, loss val:2.4316\n",
      "step:9500, loss train:2.3921, loss val:2.4686\n",
      "step:9750, loss train:2.4198, loss val:2.4363\n",
      "batch:164\n",
      "step:0, loss train:2.4084, loss val:2.4557\n",
      "step:250, loss train:2.4298, loss val:2.4205\n",
      "step:500, loss train:2.4163, loss val:2.4595\n",
      "step:750, loss train:2.4364, loss val:2.4413\n",
      "step:1000, loss train:2.4048, loss val:2.4534\n",
      "step:1250, loss train:2.4093, loss val:2.4304\n",
      "step:1500, loss train:2.4181, loss val:2.4384\n",
      "step:1750, loss train:2.3971, loss val:2.4542\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:7\u001b[0m\n",
      "File \u001b[1;32md:\\dev\\LLM Tutorial\\cuda\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;32md:\\dev\\LLM Tutorial\\notebook.ipynb Cell 39\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/dev/LLM%20Tutorial/notebook.ipynb#X51sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m X, y \u001b[39m=\u001b[39m get_batch(split)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/dev/LLM%20Tutorial/notebook.ipynb#X51sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Extract logits and loss\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/dev/LLM%20Tutorial/notebook.ipynb#X51sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m logits, loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfoward(X, y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/dev/LLM%20Tutorial/notebook.ipynb#X51sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Stored loss on losses tensor\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/dev/LLM%20Tutorial/notebook.ipynb#X51sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m losses[k] \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "\u001b[1;32md:\\dev\\LLM Tutorial\\notebook.ipynb Cell 39\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/dev/LLM%20Tutorial/notebook.ipynb#X51sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     logits \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39mview(batch_dim\u001b[39m*\u001b[39mtime_dim,channel_dim)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/dev/LLM%20Tutorial/notebook.ipynb#X51sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mview(batch_dim\u001b[39m*\u001b[39mtime_dim)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/dev/LLM%20Tutorial/notebook.ipynb#X51sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mcross_entropy(logits,target)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/dev/LLM%20Tutorial/notebook.ipynb#X51sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mreturn\u001b[39;00m logits, loss\n",
      "File \u001b[1;32md:\\dev\\LLM Tutorial\\cuda\\lib\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate)\n",
    "\n",
    "# for x in range(10):\n",
    "#     print(f\"batch:{x}\")\n",
    "#     for iter in range(max_iters):\n",
    "#         if iter % eval_iters == 0:\n",
    "#             losses = estimate_loss(eval_iters)\n",
    "#             print(f\"\"\"step:{iter}, loss train:{losses[\"train\"]:.4f}, loss val:{losses[\"val\"]:.4f}\"\"\")\n",
    "            \n",
    "#         # sample a batch of data\n",
    "#         xb, yb = get_batch('train')\n",
    "        \n",
    "#         # Evaluate the loss\n",
    "#         logits, loss = model.foward(xb,yb)\n",
    "#         optimizer.zero_grad(set_to_none=True)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     if loss.item()<1:\n",
    "#         break\n",
    "# print(loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "whep, m foon d plino\n",
      "\n",
      "\n",
      "\n",
      "s  ssoteand thesthos. ughesealinolan PThestar an'vouienong.\n",
      "pounto\n",
      "\n",
      "\n",
      "th,\n",
      "thed oned beng bed  p bolierotleake I there.]\n",
      "ano we omo h bore s.]\n",
      "Thindint sat tit m.\" ndo thokinovelof thierey  thy'tizand co  s\n",
      "hasis.\n",
      "\n",
      "\n",
      "\n",
      "medey f g Jiee ted s, fiofomuthou, stl wethanbet ord APIlighy tr tranb  omeer wasthig gasoted akim ccallyoop\n",
      "\" rowhe Pirentr the wizingalyored \"Whain ndef-'r ase ZAProd wilusutourof tenso d armoond thr he t hry g. sereang s toy stooumony, fo e\n",
      "\n",
      "\"Bus oreve d BOh\n"
     ]
    }
   ],
   "source": [
    "# context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "# generated_chars = decode(m.generate(context,max_new_tokens=500)[0].tolist())\n",
    "# print(generated_chars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
